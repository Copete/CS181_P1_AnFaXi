\documentclass[11pt]{article}

\usepackage{common}
\usepackage{booktabs}
\title{Practical 1: Regression}
\author{Antonio (email, Camelot.ai username) \\
	Fangli (email, Camelot.ai username) \\
	Xihan (email, Camelot.ai username)}
\begin{document}


\maketitle{}


%\noindent This is the template you should use for submitting your practical assignments. A full-credit assignment will go through each of these points and provide a thoughtful clear answer.  Note that the limit for the report is 4 pages, please prioritize quality over quantity in your submission.

\section{Technical Approach}

We began by performing exploratory analysis on the sample dataset we were given, which consisted of a training set of 1 million molecules with 256 binary predictors, in addition to their SMILES string and the HOMO-LUMO gap we were seeking to predict. The test set consisted of 824,230 molecules with the same set of predictors as well as their SMILES string. The sample code we were given implemented a default Linear Regression model on the full set of 256 predictors, yielding an $R_\textrm{LR}^2 = 0.461 (\textrm{MSE}_\textrm{LR} = ?)$ on the full training set, and a default Random Forest regression with $R_\textrm{RF}^2 = 0.554 (\textrm{MSE}_\textrm{LR})$.

Initial inspection found that out of 256 molecular features, 221 of them were unexpressed (i.e. had $x_i = 0$) for \emph{all} molecules, both in the training set and the test set. Dropping unimportant features would normally call for $K$-fold cross-validation across the training set to ensure those features are consistently unimportant in all cases. However, in the case of null values of certain features for every element of the training set, it is not only legitimate but necessary to drop those features from all further analysis, as fitting along null dimensions would constitute a form of overfitting.

Having reduced the sample dataset to 31 expressed molecular features, we performed regularized and non-regularized linear regression with cross-validation\footnote{After initially trying 3-fold, 5-fold and 10-fold cross-validation, we settled on 5-fold cross-validation as the best compromise between accuracy and computational speed}, under the following methods:

\begin{description}

\item[Non-regularized linear regression] Yields $R^2 =  ...$
\item[Ridge Regression]
\item[Lasso]
\item[Elastic Net]

\end{description}

The results from linear regression on the sample dataset led us into pursuing 2 parallel tracks:

\begin{enumerate}

\item Feature engineering: (Fangli)

\item Non-linear methods: (Xihan, Antonio)

Among these we tried 2 broad categories:

\begin{enumerate}

\item Tree based methods / Ensemble methods (Xihan)

\item Deep learning (Antonio)

\end{enumerate}

\end{enumerate}


{\itshape
How did you tackle the problem? Credit will be given for:

  \begin{itemize}
  \item Diving deeply into a method (rather than just trying
    off-the-shelf tools with default settings). This can mean 
    providing mathematical descriptions or pseudo-code.
  \item Making tuning and configuration decisions using thoughtful experimentation.  
    This can mean carefully describing features added or hyperparameters tuned.
  \item Exploring several methods. This can contrasting two approaches
    or perhaps going beyond those we discussed in class.
  \end{itemize}

  \noindent Thoughtfully iterating on approaches is key.
  If you used existing packages or referred to papers or blogs for ideas,
  you should cite these in your report. 

  \begin{table}
    \centering
    \begin{tabular}{@{}lll@{}}
%      \toprule
      &\multicolumn{2}{c}{Mention Features  } \\
      & Feature & Value Set\\
      \midrule
      & Mention Head & $\mcV$ \\
      & Mention First Word & $\mcV$ \\
      & Mention Last Word & $\mcV$ \\
      & Word Preceding Mention & $\mcV$ \\
      & Word Following Mention & $\mcV$\\
      & \# Words in Mention & $\{1, 2, \ldots \}$ \\
      & Mention Type & $\mathcal{T}$ \\
      \bottomrule
      
    \end{tabular}
    \caption{Feature lists are a good way of illustrating problem specific tuning.}
  \end{table}

}

\section{Results}
This section should report on the following questions: 

\begin{itemize}
\item Did you create and submit a set of
  predictions? 
  

\item  Did your methods give reasonable performance?  
\end{itemize}

\noindent You must have \textit{at least one plot or table}
that details the performances of different methods tried. 
Credit will be given for quantitatively reporting (with clearly
labeled and captioned figures and/or tables) on the performance of the
methods you tried compared to your baselines.



\begin{table}
\centering
\begin{tabular}{llr}
 \toprule
 Model &  & Acc. \\
 \midrule
 \textsc{Baseline 1} & & 0.45\\
 \textsc{Baseline 2} & & 2.59 \\
 \textsc{Model 1} & & 10.59  \\
 \textsc{Model 2} & &13.42 \\
 \textsc{Model 3} & & 7.49\\
 \bottomrule
\end{tabular}
\caption{\label{tab:results} Result tables can compactly illustrate absolute performance, but a plot may be more effective at illustrating a trend.}
\end{table}




\section{Discussion} 


End your report by discussing the thought process behind your
analysis. This section does not need to be as technical as the others 
but should summarize why you took the approach that your did. Credit will be given for:

  \begin{itemize}
  \item Explaining the your reasoning for why you seqentially chose to
    try the approaches you did (i.e. what was it about your initial
    approach that made you try the next change?).  
  \item Explaining the results.  Did the adaptations you tried improve
    the results?  Why or why not?  Did you do additional tests to
    determine if your reasoning was correct?  
  \end{itemize}
 

\end{document}

