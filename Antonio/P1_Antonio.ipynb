{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in train and test as Pandas DataFrames\n",
    "\"\"\"\n",
    "df_train = pd.read_csv(\"../train.csv.gz\")\n",
    "df_test  = pd.read_csv(\"../test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  feat_001  feat_002  \\\n",
       "0  c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...       0.0       0.0   \n",
       "1  C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...       1.0       0.0   \n",
       "2  [nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...       1.0       0.0   \n",
       "3  [nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...       1.0       0.0   \n",
       "4     c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1       0.0       0.0   \n",
       "\n",
       "   feat_003  feat_004  feat_005  feat_006  feat_007  feat_008  feat_009  ...   \\\n",
       "0       0.0       0.0       1.0       0.0       1.0       0.0       0.0  ...    \n",
       "1       0.0       0.0       1.0       0.0       1.0       0.0       0.0  ...    \n",
       "2       0.0       0.0       1.0       1.0       1.0       0.0       0.0  ...    \n",
       "3       0.0       0.0       1.0       1.0       1.0       0.0       0.0  ...    \n",
       "4       0.0       0.0       1.0       0.0       1.0       0.0       0.0  ...    \n",
       "\n",
       "   feat_248  feat_249  feat_250  feat_251  feat_252  feat_253  feat_254  \\\n",
       "0       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       1.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "2       1.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "3       1.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "4       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   feat_255  feat_256   gap  \n",
       "0       0.0       0.0  1.19  \n",
       "1       0.0       0.0  1.60  \n",
       "2       0.0       0.0  1.49  \n",
       "3       0.0       0.0  1.36  \n",
       "4       0.0       0.0  1.98  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_247</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>c1sc(-c2cnc3c(c2)c2nsnc2c2cc4cccnc4cc32)c2cc[n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[nH]1cccc1-c1cc2c3nsnc3c3c4sccc4[nH]c3c2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[nH]1c2cc(-c3ccc[se]3)c3nsnc3c2c2c3cscc3c3ccc4...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[nH]1c(cc2cnc3c(c12)c1=C[SiH2]C=c1c1ccc2=CCC=c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>c1sc(-c2sc(-c3sc(-c4scc5[se]ccc45)c4ccoc34)c3c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                             smiles  feat_001  feat_002  \\\n",
       "0   1  c1sc(-c2cnc3c(c2)c2nsnc2c2cc4cccnc4cc32)c2cc[n...       0.0       0.0   \n",
       "1   2         [nH]1cccc1-c1cc2c3nsnc3c3c4sccc4[nH]c3c2s1       0.0       0.0   \n",
       "2   3  [nH]1c2cc(-c3ccc[se]3)c3nsnc3c2c2c3cscc3c3ccc4...       1.0       0.0   \n",
       "3   4  [nH]1c(cc2cnc3c(c12)c1=C[SiH2]C=c1c1ccc2=CCC=c...       1.0       0.0   \n",
       "4   5  c1sc(-c2sc(-c3sc(-c4scc5[se]ccc45)c4ccoc34)c3c...       0.0       0.0   \n",
       "\n",
       "   feat_003  feat_004  feat_005  feat_006  feat_007  feat_008    ...     \\\n",
       "0       0.0       0.0       1.0       1.0       1.0       0.0    ...      \n",
       "1       0.0       0.0       1.0       1.0       1.0       0.0    ...      \n",
       "2       0.0       0.0       1.0       1.0       1.0       0.0    ...      \n",
       "3       0.0       0.0       1.0       1.0       1.0       0.0    ...      \n",
       "4       0.0       0.0       1.0       0.0       1.0       0.0    ...      \n",
       "\n",
       "   feat_247  feat_248  feat_249  feat_250  feat_251  feat_252  feat_253  \\\n",
       "0       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4       0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   feat_254  feat_255  feat_256  \n",
       "0       0.0       0.0       0.0  \n",
       "1       0.0       0.0       0.0  \n",
       "2       0.0       0.0       0.0  \n",
       "3       0.0       0.0       0.0  \n",
       "4       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store gap values\n",
    "Y_train = df_train.gap.values\n",
    "#row where testing examples start\n",
    "test_idx = df_train.shape[0]\n",
    "#delete 'Id' column\n",
    "df_test = df_test.drop(['Id'], axis=1)\n",
    "#delete 'gap' column\n",
    "df_train = df_train.drop(['gap'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_247</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  feat_001  feat_002  \\\n",
       "0  c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...       0.0       0.0   \n",
       "1  C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...       1.0       0.0   \n",
       "2  [nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...       1.0       0.0   \n",
       "3  [nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...       1.0       0.0   \n",
       "4     c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1       0.0       0.0   \n",
       "\n",
       "   feat_003  feat_004  feat_005  feat_006  feat_007  feat_008  feat_009  \\\n",
       "0       0.0       0.0       1.0       0.0       1.0       0.0       0.0   \n",
       "1       0.0       0.0       1.0       0.0       1.0       0.0       0.0   \n",
       "2       0.0       0.0       1.0       1.0       1.0       0.0       0.0   \n",
       "3       0.0       0.0       1.0       1.0       1.0       0.0       0.0   \n",
       "4       0.0       0.0       1.0       0.0       1.0       0.0       0.0   \n",
       "\n",
       "     ...     feat_247  feat_248  feat_249  feat_250  feat_251  feat_252  \\\n",
       "0    ...          0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "1    ...          0.0       1.0       0.0       0.0       1.0       0.0   \n",
       "2    ...          0.0       1.0       0.0       0.0       0.0       1.0   \n",
       "3    ...          0.0       1.0       0.0       0.0       0.0       1.0   \n",
       "4    ...          0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   feat_253  feat_254  feat_255  feat_256  \n",
       "0       0.0       0.0       0.0       0.0  \n",
       "1       0.0       0.0       0.0       0.0  \n",
       "2       0.0       0.0       0.0       0.0  \n",
       "3       0.0       0.0       0.0       0.0  \n",
       "4       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame with all train and test examples so we can more easily apply feature engineering on\n",
    "df_all = pd.concat((df_train, df_test), axis=0)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample Feature Engineering\\n\\nthis calculates the length of each smile string and adds a feature column with those lengths\\nNote: this is NOT a good feature and will result in a lower score!\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Feature Engineering\n",
    "\n",
    "this calculates the length of each smile string and adds a feature column with those lengths\n",
    "Note: this is NOT a good feature and will result in a lower score!\n",
    "\"\"\"\n",
    "#smiles_len = np.vstack(df_all.smiles.astype(str).apply(lambda x: len(x)))\n",
    "#df_all['smiles_len'] = pd.DataFrame(smiles_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: (1000000, 256)\n",
      "Train gap: (1000000,)\n",
      "Test features: (824230, 256)\n"
     ]
    }
   ],
   "source": [
    "#Drop the 'smiles' column\n",
    "df_all = df_all.drop(['smiles'], axis=1)\n",
    "vals = df_all.values\n",
    "X_train = vals[:test_idx]\n",
    "X_test = vals[test_idx:]\n",
    "print(\"Train features:\", X_train.shape)\n",
    "print(\"Train gap:\", Y_train.shape)\n",
    "print(\"Test features:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(X_train, Y_train)\n",
    "LR_pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor()\n",
    "RF.fit(X_train, Y_train)\n",
    "RF_pred = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, predictions):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(predictions):\n",
    "            f.write(str(i+1) + \",\" + str(p) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Skip writing sample files\n",
    "#write_to_file(\"sample1.csv\", LR_pred)\n",
    "#write_to_file(\"sample2.csv\", RF_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression and Random Forest exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LinearRegression in module sklearn.linear_model.base object:\n",
      "\n",
      "class LinearRegression(LinearModel, sklearn.base.RegressorMixin)\n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : boolean, optional, default True\n",
      " |      whether to calculate the intercept for this model. If set\n",
      " |      to False, no intercept will be used in calculations\n",
      " |      (e.g. data is expected to be already centered).\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
      " |      an estimator with ``normalize=False``.\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, optional, default 1\n",
      " |      The number of jobs to use for the computation.\n",
      " |      If -1 all CPUs are used. This will only provide speedup for\n",
      " |      n_targets > 1 and sufficient large problems.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  intercept_ : array\n",
      " |      Independent term in the linear model.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
      " |          Training data\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples, n_targets]\n",
      " |          Target values. Will be cast to X's dtype if necessary\n",
      " |      \n",
      " |      sample_weight : numpy array of shape [n_samples]\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA30AAAJCCAYAAAB00a51AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAxN9AAAAAxN9AAAAA/sv+Y3VuzuMnhsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b726e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(range(len(LR.coef_)), LR.coef_)\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4610374302352153\n",
      "0.5541582497609592\n"
     ]
    }
   ],
   "source": [
    "# Calculate initial R^2\n",
    "print(LR.score(X_train, Y_train))\n",
    "print(RF.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestRegressor in module sklearn.ensemble.forest object:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and use averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a percentage and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a percentage and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a percentage and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_split : float,\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool, optional (default=False)\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=1)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      If -1, then the number of jobs is set to the number of cores.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_prediction_ : array of shape = [n_samples]\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>>\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      " |             max_features='auto', max_leaf_nodes=None,\n",
      " |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |             min_samples_leaf=1, min_samples_split=2,\n",
      " |             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      " |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(regr.feature_importances_)\n",
      " |  [ 0.17339552  0.81594114  0.          0.01066333]\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-2.50699856]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=10, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAD+CAYAAAC9QpvGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b5c6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(RF.feature_importances_)), RF.feature_importances_, 2)\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop predictors that are unimportant in both LR and RF\n",
    "LR_coef = LR.coef_\n",
    "RF_imp = RF.feature_importances_\n",
    "i0 = (LR_coef == 0) & (RF_imp == 0)\n",
    "X_train1 = X_train[:,i0 == False]\n",
    "X_test1 = X_test[:,i0 == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redo regressions\n",
    "LR1 = LinearRegression()\n",
    "LR1.fit(X_train1, Y_train)\n",
    "RF1 = RandomForestRegressor()\n",
    "RF1.fit(X_train1, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46103996777095446\n",
      "0.5541561274294204\n"
     ]
    }
   ],
   "source": [
    "# Recalculate R^2\n",
    "print(LR1.score(X_train1, Y_train))\n",
    "print(RF1.score(X_train1, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11897b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(LR1.coef_)), LR1.coef_)\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAD+CAYAAAC9QpvGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ9QAGEPUABhD1AAYQ9QAGON/AWwfMXW/ncTLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b3be7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(RF1.feature_importances_)), RF1.feature_importances_, 2)\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test only features with RF importance > 0\n",
    "RF_imp1 = RF1.feature_importances_\n",
    "LR_coef1 = LR.coef_\n",
    "i1 = (RF_imp1 > 0)\n",
    "X_train2 = X_train1[:,i1]\n",
    "X_test2 = X_test1[:,i1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46103751906354007\n",
      "0.5541627879401239\n"
     ]
    }
   ],
   "source": [
    "LR2 = LinearRegression()\n",
    "LR2.fit(X_train2, Y_train)\n",
    "RF2 = RandomForestRegressor()\n",
    "RF2.fit(X_train2, Y_train)\n",
    "print(LR2.score(X_train2, Y_train))\n",
    "print(RF2.score(X_train2, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 31)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14ed3be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(LR2.coef_)), LR2.coef_)\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAD+CAYAAAC9QpvGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZbobO8nCVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b01c9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(RF2.feature_importances_)), RF2.feature_importances_)\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_coef2 = LR2.coef_\n",
    "RF_imp2 = RF2.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR( 1 ):  0.461037535886\n",
      "RF( 1 ):  0.554151377388\n",
      "LR( 2 ):  0.461036270578\n",
      "RF( 2 ):  0.554157216\n",
      "LR( 3 ):  0.461036284923\n",
      "RF( 3 ):  0.554156637825\n",
      "LR( 4 ):  0.46094125547\n",
      "RF( 4 ):  0.553944778913\n",
      "LR( 5 ):  0.460901625442\n",
      "RF( 5 ):  0.553943517091\n",
      "LR( 6 ):  0.460179726007\n",
      "RF( 6 ):  0.553928203126\n",
      "LR( 7 ):  0.459706597906\n",
      "RF( 7 ):  0.553116761508\n",
      "LR( 8 ):  0.458371583134\n",
      "RF( 8 ):  0.552946118719\n",
      "LR( 9 ):  0.458331703566\n",
      "RF( 9 ):  0.551374648877\n",
      "LR( 10 ):  0.455302085306\n",
      "RF( 10 ):  0.549760051615\n",
      "LR( 11 ):  0.455087479995\n",
      "RF( 11 ):  0.549574017694\n",
      "LR( 12 ):  0.45459924113\n",
      "RF( 12 ):  0.547786784993\n",
      "LR( 13 ):  0.448234307771\n",
      "RF( 13 ):  0.544824645561\n",
      "LR( 14 ):  0.448150130583\n",
      "RF( 14 ):  0.541816138413\n",
      "LR( 15 ):  0.448150577671\n",
      "RF( 15 ):  0.541818000951\n",
      "LR( 16 ):  0.447436535208\n",
      "RF( 16 ):  0.535097785326\n",
      "LR( 17 ):  0.445835534228\n",
      "RF( 17 ):  0.528996194821\n",
      "LR( 18 ):  0.445836530438\n",
      "RF( 18 ):  0.52899627141\n",
      "LR( 19 ):  0.442223133773\n",
      "RF( 19 ):  0.521813806405\n",
      "LR( 20 ):  0.441944635433\n",
      "RF( 20 ):  0.509152899257\n",
      "LR( 21 ):  0.441502810961\n",
      "RF( 21 ):  0.497747510894\n",
      "LR( 22 ):  0.440549505451\n",
      "RF( 22 ):  0.492237657627\n",
      "LR( 23 ):  0.440503271316\n",
      "RF( 23 ):  0.481729981313\n",
      "LR( 24 ):  0.435481261964\n",
      "RF( 24 ):  0.471038742734\n",
      "LR( 25 ):  0.4266793153\n",
      "RF( 25 ):  0.460411705254\n",
      "LR( 26 ):  0.424991388824\n",
      "RF( 26 ):  0.454261656945\n",
      "LR( 27 ):  0.397500276671\n",
      "RF( 27 ):  0.430212159476\n",
      "LR( 28 ):  0.375542148082\n",
      "RF( 28 ):  0.391221080766\n",
      "LR( 29 ):  0.356274844166\n",
      "RF( 29 ):  0.368912158633\n",
      "LR( 30 ):  0.334196815245\n",
      "RF( 30 ):  0.34471571475\n"
     ]
    }
   ],
   "source": [
    "# Drop the least important RF feature in each iteration\n",
    "X_train0 = X_train2\n",
    "nx = np.array([])\n",
    "R2_LR = np.array([])\n",
    "R2_RF = np.array([])\n",
    "for i in range(30):\n",
    "    LR0 = LinearRegression()\n",
    "    LR0.fit(X_train0, Y_train)\n",
    "    RF0 = RandomForestRegressor()\n",
    "    RF0.fit(X_train0, Y_train)\n",
    "    print('LR(',i+1,'): ',LR0.score(X_train0, Y_train))\n",
    "    print('RF(',i+1,'): ',RF0.score(X_train0, Y_train))\n",
    "    n0 = X_train0.shape[1]\n",
    "    nx = np.append(nx,n0)\n",
    "    R2_LR = np.append(R2_LR,LR0.score(X_train0, Y_train))\n",
    "    R2_RF = np.append(R2_RF,RF0.score(X_train0, Y_train))\n",
    "    i0 = (range(n0) != RF0.feature_importances_.argmin())\n",
    "    X_train0 = X_train0[:,i0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f828f01c4214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR2_LR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR2_RF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(nx,R2_LR)\n",
    "plt.plot(nx,R2_RF,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression methods with Shrinkage and Cross-Validation:\n",
    "##    Ridge Regression, Lasso, Elastic Net\n",
    "##    Hyperparameter tuning by Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implement Ridge regression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45236625858580692"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.1, normalize=True) #Inialize alpha to 0.1\n",
    "ridge_coef = ridge.fit(X_train2, Y_train).coef_\n",
    "ridge.score(X_train2, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAD8VJREFUeJzt3X+s31V9x/Hny2qdkTlhFOgoWLY1\nmWwhOO9wm9OZgQZmYl0CDhJdSWg64khcTDabsTjHsqSK2/xD4+ymEXUOEX/QzG4InTqXTMatIloI\nUkmBrk17xZFJzDSM9/64nyY3l+/90fv53N77vef5SJrv58fp55yTz/2+enq+n++5qSokSW15zko3\nQJJ06hn+ktQgw1+SGmT4S1KDDH9JapDhL0kNMvwlqUGGvyQ1yPCXpAY9d6UbMJczzzyzNm/evNLN\nkKSxsn///u9V1YaFyq3a8N+8eTOTk5Mr3QxJGitJHl1MOad9JKlBhr8kNcjwl6QGGf6S1CDDX5Ia\nZPhLUoMMf0lqkOEvSQ1atV/y0uJt3vmFec8f2vX6U9QSSePCkb8kNcjwl6QGGf6S1CDDX5IaZPhL\nUoMMf0lqkOEvSQ0y/CWpQYa/JDVokPBPcnmSh5IcTLJzxPm3J3kgyf1J9iV5yRD1SpKWpnf4J1kH\nfAC4ArgQuCbJhbOKfQOYqKqLgNuB9/StV5K0dEOM/C8BDlbVI1X1Y+BWYOvMAlX1par6Ybf7NWDT\nAPVKkpZoiPA/F3h8xv7h7thcrgP+edSJJDuSTCaZnJqaGqBpkqRRhgj/jDhWIwsmbwYmgJtHna+q\n3VU1UVUTGzZsGKBpkqRRhljS+TBw3oz9TcCR2YWSXAbcCPxmVf1ogHolSUs0xMj/XmBLkguSrAeu\nBvbMLJDkZcCHgDdU1fEB6pQk9dA7/KvqaeAG4E7gQeC2qjqQ5KYkb+iK3QycBnw6yX1J9sxxOUnS\nKTDIb/Kqqr3A3lnH3jlj+7Ih6pEkDcNv+EpSgwx/SWqQ4S9JDTL8JalBhr8kNcjwl6QGGf6S1CDD\nX5IaZPhLUoMG+YavJM1l884vzHv+0K7Xn6KWaCZH/pLUoOZH/vONShyRSOPL/3HMz5G/JDXI8Jek\nBhn+ktQgw1+SGmT4S1KDDH9JapDhL0kNMvwlqUGGvyQ1yPCXpAYZ/pLUoEHCP8nlSR5KcjDJzhHn\nX53k60meTnLlEHVKkpaud/gnWQd8ALgCuBC4JsmFs4o9BlwLfLJvfZKk/oZY1fMS4GBVPQKQ5FZg\nK/DAiQJVdag798wA9UmSehpi2udc4PEZ+4e7YyctyY4kk0kmp6amBmiaJGmUIcI/I47VUi5UVbur\naqKqJjZs2NCzWZKkuQwR/oeB82bsbwKODHBdSdIyGSL87wW2JLkgyXrgamDPANeVJC2T3uFfVU8D\nNwB3Ag8Ct1XVgSQ3JXkDQJJfSXIYuAr4UJIDfeuVJC3dIL/Dt6r2AntnHXvnjO17mZ4OkrTKLfZ3\n3/o7cseb3/CVpAYZ/pLUIMNfkhpk+EtSgwx/SWqQ4S9JDTL8JalBhr8kNcjwl6QGGf6S1CDDX5Ia\nZPhLUoMMf0lqkOEvSQ0y/CWpQYOs5y+tda5dr7XG8B+QASFpXBj+0oAcAGhcOOcvSQ0y/CWpQYa/\nJDXI8JekBhn+ktSgQcI/yeVJHkpyMMnOEeefn+RT3fl7kmweol5J0tL0Dv8k64APAFcAFwLXJLlw\nVrHrgP+uqp8H/gZ4d996JUlLN8Rz/pcAB6vqEYAktwJbgQdmlNkKvKvbvh14f5JUVQ1QvzR2/D6A\nVlr65m+SK4HLq2p7t/8W4BVVdcOMMt/uyhzu9r/blfnerGvtAHYAnH/++S9/9NFHl9yuId9cQ79R\nF3u91V7vYsqtRJ3LUW6lzNe+lvoKy9OPtfhzl2R/VU0sVG6IkX9GHJv9L8piylBVu4HdABMTE83/\nr2Cl34yS1q4hwv8wcN6M/U3AkTnKHE7yXOCngO8PUPechgxOQ1ham1p+bw/xtM+9wJYkFyRZD1wN\n7JlVZg+wrdu+EvhX5/slaeX0HvlX1dNJbgDuBNYBH6mqA0luAiarag/wYeDjSQ4yPeK/um+9kqSl\nG2RVz6raC+yddeydM7b/F7hqiLokSf25pLOa1vKcr9pm+OtZDMTVw3uh5WL4a8kMJml8ubCbJDXI\n8JekBjntI0mn2GqYMjX8taxWww+5pGdz2keSGmT4S1KDDH9JapDhL0kNMvwlqUE+7SNJC1iLT605\n8pekBhn+ktQgp320KqzF/1ZLq5kjf0lqkOEvSQ0y/CWpQYa/JDXI8JekBvm0jzTmfFJKS+HIX5Ia\n1Cv8k5yR5K4kD3evp89R7l+SPJnkn/rUJ0kaRt+R/05gX1VtAfZ1+6PcDLylZ12SpIH0Df+twC3d\n9i3AG0cVqqp9wA961iVJGkjf8D+7qo4CdK9n9blYkh1JJpNMTk1N9WyaJGkuCz7tk+Ru4JwRp24c\nujFVtRvYDTAxMVFDX1+SNG3B8K+qy+Y6l+RYko1VdTTJRuD4oK2TpFl8tHUYfad99gDbuu1twB09\nrydJOgX6fslrF3BbkuuAx4CrAJJMANdX1fZu/6vALwCnJTkMXFdVd/ase2w5cpG00nqFf1U9AVw6\n4vgksH3G/qv61CNJGpbf8JWkBhn+ktQgw1+SGmT4S1KDDH9JapDhL0kNMvwlqUGGvyQ1yPCXpAYZ\n/pLUIMNfkhrUd2E36ZRyUby1y3t7ajnyl6QGGf6S1CDDX5IaZPhLUoMMf0lqkOEvSQ0y/CWpQT7n\n3xCfo5Z0giN/SWqQ4S9JDTL8JalBvcI/yRlJ7krycPd6+ogyFyf5jyQHktyf5Hf71ClJ6q/vyH8n\nsK+qtgD7uv3Zfgj8XlX9InA58L4kL+5ZrySph75P+2wFXtNt3wJ8GXjHzAJV9Z0Z20eSHAc2AE/2\nrFvSSfBpL83Ud+R/dlUdBehez5qvcJJLgPXAd+c4vyPJZJLJqampnk2TJM1lwZF/kruBc0acuvFk\nKkqyEfg4sK2qnhlVpqp2A7sBJiYm6mSuL0lavAXDv6oum+tckmNJNlbV0S7cj89R7kXAF4A/raqv\nLbm1kqRB9J322QNs67a3AXfMLpBkPfA54GNV9eme9UmSBtD3A99dwG1JrgMeA64CSDIBXF9V24E3\nAa8GfjrJtd3fu7aq7utZtyStKuP0oXqv8K+qJ4BLRxyfBLZ3258APtGnHknSsPyGryQ1yPCXpAYZ\n/pLUIMNfkhpk+EtSgwx/SWqQ4S9JDTL8JalBhr8kNcjwl6QGGf6S1CDDX5IaZPhLUoMMf0lqkOEv\nSQ0y/CWpQYa/JDXI8JekBhn+ktQgw1+SGmT4S1KDDH9JapDhL0kN6hX+Sc5IcleSh7vX00eUeUmS\n/UnuS3IgyfV96pQk9dd35L8T2FdVW4B93f5sR4Ffr6qLgVcAO5P8TM96JUk99A3/rcAt3fYtwBtn\nF6iqH1fVj7rd5w9QpySpp75BfHZVHQXoXs8aVSjJeUnuBx4H3l1VR+YotyPJZJLJqampnk2TJM3l\nuQsVSHI3cM6IUzcutpKqehy4qJvu+XyS26vq2Ihyu4HdABMTE7XY60uSTs6C4V9Vl811LsmxJBur\n6miSjcDxBa51JMkB4FXA7SfdWknSIPpO++wBtnXb24A7ZhdIsinJC7rt04FXAg/1rFeS1EPf8N8F\nvDbJw8Bru32STCT5+67MS4F7knwT+Arw3qr6Vs96JUk9LDjtM5+qegK4dMTxSWB7t30XcFGfeiRJ\nw/KxS0lqkOEvSQ0y/CWpQYa/JDXI8JekBhn+ktQgw1+SGmT4S1KDDH9JapDhL0kNMvwlqUGGvyQ1\nyPCXpAYZ/pLUIMNfkhpk+EtSgwx/SWqQ4S9JDTL8JalBvX6Hr7RaHdr1+pVugrSqOfKXpAYZ/pLU\nIMNfkhrUK/yTnJHkriQPd6+nz1P2RUn+K8n7+9QpSeqv78h/J7CvqrYA+7r9ufwF8JWe9UmSBtA3\n/LcCt3TbtwBvHFUoycuBs4Ev9qxPkjSAvuF/dlUdBehez5pdIMlzgL8C/mihiyXZkWQyyeTU1FTP\npkmS5rLgc/5J7gbOGXHqxkXW8VZgb1U9nmTeglW1G9gNMDExUYu8viTpJC0Y/lV12VznkhxLsrGq\njibZCBwfUezXgFcleStwGrA+yVNVNd/nA5KkZZSqpQ+wk9wMPFFVu5LsBM6oqj+ep/y1wERV3bCI\na08Bjy65cc92JvC9Aa+3EuzD6rEW+rEW+gBrox9D9uElVbVhoUJ9l3fYBdyW5DrgMeAqgCQTwPVV\ntX2pF15M409GksmqmhjymqeafVg91kI/1kIfYG30YyX60Cv8q+oJ4NIRxyeBZwV/VX0U+GifOiVJ\n/fkNX0lqUEvhv3ulGzAA+7B6rIV+rIU+wNroxynvQ68PfCVJ46mlkb8kqbPmwz/J5UkeSnKwexx1\nLCU5lORbSe5LMrnS7VmMJB9JcjzJt2ccW/RigKvFHP14V7dQ4X3dn99eyTYuJMl5Sb6U5MEkB5K8\nrTs+Nvdjnj6M2734iST/meSbXT/+vDt+QZJ7unvxqSTrl7Uda3naJ8k64DvAa4HDwL3ANVX1wIo2\nbAmSHGL6OxJj8zxzklcDTwEfq6pf6o69B/j+jO+GnF5V71jJdi5kjn68C3iqqt67km1brO5LmBur\n6utJfhLYz/RaXNcyJvdjnj68ifG6FwFeWFVPJXke8O/A24C3A5+tqluT/C3wzar64HK1Y62P/C8B\nDlbVI1X1Y+BWphej0ylQVf8GfH/W4UUtBriazNGPsVJVR6vq6932D4AHgXMZo/sxTx/GSk17qtt9\nXvengN8Cbu+OL/u9WOvhfy7w+Iz9w4zhD0ungC8m2Z9kx0o3pocFFwMcIzckub+bFlq10yWzJdkM\nvAy4hzG9H7P6AGN2L5KsS3If00vi3AV8F3iyqp7uiix7Vq318B+1kty4znO9sqp+GbgC+INuKkIr\n54PAzwEXA0eZXrl21UtyGvAZ4A+r6n9Wuj1LMaIPY3cvqur/qupiYBPTMxQvHVVsOduw1sP/MHDe\njP1NwJEVaksvVXWkez0OfI7pH5hxdKybuz0xhztqMcBVr6qOdW/gZ4C/YwzuRze//BngH6rqs93h\nsbofo/owjvfihKp6Evgy8KvAi5OcWHVh2bNqrYf/vcCW7lP09cDVwJ4VbtNJS/LC7gMukrwQeB3w\n7fn/1qq1B9jWbW8D7ljBtizZicDs/A6r/H50HzJ+GHiwqv56xqmxuR9z9WEM78WGJC/utl8AXMb0\n5xdfAq7sii37vVjTT/sAdI99vQ9YB3ykqv5yhZt00pL8LNOjfZhej+mT49CPJP8IvIbpFQuPAX8G\nfB64DTifbjHAqlrVH6bO0Y/XMD3NUMAh4PdPzJ2vRkl+A/gq8C3gme7wnzA9Zz4W92OePlzDeN2L\ni5j+QHcd0wPw26rqpu59fitwBvAN4M1V9aNla8daD39J0rOt9WkfSdIIhr8kNcjwl6QGGf6S1CDD\nX5IaZPhLUoMMf0lqkOEvSQ36f4BjtsGqQ3rPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1529f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(ridge_coef)), ridge_coef)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implement Lasso regression\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.001,normalize=True) #Inialize alpha to 0.1\n",
    "lasso_coef = lasso.fit(X_train2, Y_train).coef_\n",
    "lasso.score(X_train2, Y_train)\n",
    "# Why is Lasso fit not working??? Model not sensitive enough to the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADp1JREFUeJzt3H+s3fVdx/Hny3YwZQYKFFZbalGa\naKeG6QloNg0ZA4rJLCqaYow1wdQ/RrJJTMZcDD82E1g2WYyIqYOkEl0hbJMmi8GOH/FHDOspMKFD\n1g6Z3LWBkjK0WYR0vP3jfmvu53pu7+09h56e9vlIbu75fs/nnvP+5tvbZ8/33NtUFZIkHfED4x5A\nknRiMQySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktRYOu4BFuPcc8+tNWvWjHsMSZoo\nu3bterWqls+3biLDsGbNGvr9/rjHkKSJkuTbC1nnpSRJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlh\nGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktQw\nDJKkhmGQJDUMgySpYRgkSQ3DIElqjCQMSdYneT7J3iQ3Dbj/9CT3d/c/kWTNrPtXJzmU5A9HMY8k\nafGGDkOSJcBdwNXAOuC6JOtmLbseeK2qLgLuBO6Ydf+dwN8PO4skaXijeMVwCbC3ql6oqjeBbcCG\nWWs2AFu72w8ClycJQJJrgBeA3SOYRZI0pFGEYSXw0oztqW7fwDVVdRh4HTgnyRnAx4BbRzCHJGkE\nRhGGDNhXC1xzK3BnVR2a90mSzUn6SfoHDhxYxJiSpIVYOoLHmAIumLG9Ctg3x5qpJEuBM4GDwKXA\ntUk+DZwFvJXkf6rqz2c/SVVtAbYA9Hq92eGRJI3IKMKwE1ib5ELgO8BG4LdmrdkObAL+FbgWeLSq\nCvjFIwuS3AIcGhQFSdLxM3QYqupwkhuAh4ElwL1VtTvJbUC/qrYD9wD3JdnL9CuFjcM+ryTp7ZHp\nf7hPll6vV/1+f9xjSNJESbKrqnrzrfM3nyVJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMk\nqWEYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS\n1DAMkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1RhKGJOuTPJ9kb5Kb\nBtx/epL7u/ufSLKm239Fkl1Jnuk+f2AU80iSFm/oMCRZAtwFXA2sA65Lsm7WsuuB16rqIuBO4I5u\n/6vAh6rqp4FNwH3DziNJGs4oXjFcAuytqheq6k1gG7Bh1poNwNbu9oPA5UlSVU9V1b5u/27gnUlO\nH8FMkqRFGkUYVgIvzdie6vYNXFNVh4HXgXNmrfl14KmqemMEM0mSFmnpCB4jA/bVsaxJ8h6mLy9d\nOeeTJJuBzQCrV68+9iklSQsyilcMU8AFM7ZXAfvmWpNkKXAmcLDbXgV8GfidqvrWXE9SVVuqqldV\nveXLl49gbEnSIKMIw05gbZILk5wGbAS2z1qznek3lwGuBR6tqkpyFvAV4ONV9S8jmEWSNKShw9C9\nZ3AD8DDwHPBAVe1OcluSX+mW3QOck2QvcCNw5EdabwAuAv44ydPdx3nDziRJWrxUzX474MTX6/Wq\n3++PewxJmihJdlVVb751/uazJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEY\nJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAM\nkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUGEkYkqxP8nySvUluGnD/6Unu7+5/Isma\nGfd9vNv/fJKrRjGPJGnxhg5DkiXAXcDVwDrguiTrZi27Hnitqi4C7gTu6L52HbAReA+wHviL7vEk\nSWMyilcMlwB7q+qFqnoT2AZsmLVmA7C1u/0gcHmSdPu3VdUbVfUfwN7u8SRJYzKKMKwEXpqxPdXt\nG7imqg4DrwPnLPBrJUnH0SjCkAH7aoFrFvK10w+QbE7ST9I/cODAMY4oSVqoUYRhCrhgxvYqYN9c\na5IsBc4EDi7wawGoqi1V1auq3vLly0cwtiRpkFGEYSewNsmFSU5j+s3k7bPWbAc2dbevBR6tqur2\nb+x+aulCYC3wtRHMJElapKXDPkBVHU5yA/AwsAS4t6p2J7kN6FfVduAe4L4ke5l+pbCx+9rdSR4A\nvgEcBj5cVd8fdiZJ0uJl+h/uk6XX61W/3x/3GJI0UZLsqqrefOv8zWdJUsMwSJIahkGS1DAMkqSG\nYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLD\nMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlh\nGCRJjaHCkOTsJDuS7Ok+L5tj3aZuzZ4km7p9P5TkK0n+PcnuJLcPM4skaTSGfcVwE/BIVa0FHum2\nG0nOBm4GLgUuAW6eEZDPVNVPAO8F3pfk6iHnkSQNadgwbAC2dre3AtcMWHMVsKOqDlbVa8AOYH1V\nfa+qHgOoqjeBJ4FVQ84jSRrSsGE4v6r2A3SfzxuwZiXw0oztqW7f/0lyFvAhpl91SJLGaOl8C5J8\nFXj3gLs+scDnyIB9NePxlwJfAP6sql44yhybgc0Aq1evXuBTS5KO1bxhqKoPznVfkpeTrKiq/UlW\nAK8MWDYFXDZjexXw+IztLcCeqvrcPHNs6dbS6/XqaGslSYs37KWk7cCm7vYm4KEBax4GrkyyrHvT\n+cpuH0k+BZwJfHTIOSRJIzJsGG4HrkiyB7ii2yZJL8nnAarqIPBJYGf3cVtVHUyyiunLUeuAJ5M8\nneT3hpxHkjSkVE3eVZler1f9fn/cY0jSREmyq6p6863zN58lSQ3DIElqGAZJUsMwSJIahkGS1DAM\nkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgG\nSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAMkqTGUGFIcnaS\nHUn2dJ+XzbFuU7dmT5JNA+7fnuTZYWaRJI3GsK8YbgIeqaq1wCPddiPJ2cDNwKXAJcDNMwOS5NeA\nQ0POIUkakWHDsAHY2t3eClwzYM1VwI6qOlhVrwE7gPUASd4F3Ah8asg5JEkjMmwYzq+q/QDd5/MG\nrFkJvDRje6rbB/BJ4LPA94acQ5I0IkvnW5Dkq8C7B9z1iQU+RwbsqyQXAxdV1R8kWbOAOTYDmwFW\nr169wKeWJB2recNQVR+c674kLydZUVX7k6wAXhmwbAq4bMb2KuBx4BeAn0vyYjfHeUker6rLGKCq\ntgBbAHq9Xs03tyRpcYa9lLQdOPJTRpuAhwaseRi4Msmy7k3nK4GHq+ruqvqRqloDvB/45lxRkCQd\nP8OG4XbgiiR7gCu6bZL0knweoKoOMv1ews7u47ZunyTpBJSqybsq0+v1qt/vj3sMSZooSXZVVW++\ndf7msySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBI\nkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpkaoa\n9wzHLMkB4NsjerhzgVdH9FjjcjIcA5wcx3EyHAOcHMfhMfx/P1pVy+dbNJFhGKUk/arqjXuOYZwM\nxwAnx3GcDMcAJ8dxeAyL56UkSVLDMEiSGoYBtox7gBE4GY4BTo7jOBmOAU6O4/AYFumUf49BktTy\nFYMkqXHKhiHJ+iTPJ9mb5KZxz7NYSV5M8kySp5P0xz3PQiW5N8krSZ6dse/sJDuS7Ok+LxvnjPOZ\n4xhuSfKd7nw8neSXxznjfJJckOSxJM8l2Z3kI93+iTkXRzmGSTsX70zytSRf747j1m7/hUme6M7F\n/UlOe9tnORUvJSVZAnwTuAKYAnYC11XVN8Y62CIkeRHoVdVE/bx2kl8CDgF/XVU/1e37NHCwqm7v\nYr2sqj42zjmPZo5juAU4VFWfGedsC5VkBbCiqp5M8sPALuAa4HeZkHNxlGP4TSbrXAQ4o6oOJXkH\n8M/AR4AbgS9V1bYkfwl8varufjtnOVVfMVwC7K2qF6rqTWAbsGHMM51SquofgYOzdm8Atna3tzL9\nzX3CmuMYJkpV7a+qJ7vb/w08B6xkgs7FUY5hotS0Q93mO7qPAj4APNjtPy7n4lQNw0rgpRnbU0zg\nH6ROAf+QZFeSzeMeZkjnV9V+mP5mB84b8zyLdUOSf+suNZ2wl2BmS7IGeC/wBBN6LmYdA0zYuUiy\nJMnTwCvADuBbwHer6nC35Lj8XXWqhiED9k3qNbX3VdXPAlcDH+4ub2h87gZ+HLgY2A98drzjLEyS\ndwFfBD5aVf817nkWY8AxTNy5qKrvV9XFwCqmr2z85KBlb/ccp2oYpoALZmyvAvaNaZahVNW+7vMr\nwJeZ/sM0qV7urhcfuW78ypjnOWZV9XL3zf0W8FdMwPnormd/EfibqvpSt3uizsWgY5jEc3FEVX0X\neBz4eeCsJEu7u47L31Wnahh2Amu7d/tPAzYC28c80zFLckb3ZhtJzgCuBJ49+led0LYDm7rbm4CH\nxjjLohz5y7Tzq5zg56N7w/Me4Lmq+tMZd03MuZjrGCbwXCxPclZ3+weBDzL9fsljwLXdsuNyLk7J\nn0oC6H507XPAEuDeqvqTMY90zJL8GNOvEgCWAn87KceR5AvAZUz/75EvAzcDfwc8AKwG/hP4jao6\nYd/cneMYLmP60kUBLwK/f+Ra/YkoyfuBfwKeAd7qdv8R09foJ+JcHOUYrmOyzsXPMP3m8hKm/9H+\nQFXd1n2fbwPOBp4Cfruq3nhbZzlVwyBJGuxUvZQkSZqDYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBI\nkhqGQZLU+F+jfwjChVoPfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a11ed0f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(lasso_coef)), lasso_coef)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implement Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "LR_cv = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46264171  0.46145854  0.45888931]\n",
      "0.460996520416 0.00156636219245\n",
      "[ 0.46237257  0.46210402  0.4614502   0.45969942  0.45938669]\n",
      "0.461002579365 0.00123286166459\n",
      "[ 0.46632674  0.45837472  0.46581813  0.45841062  0.46077532  0.46214538\n",
      "  0.46222505  0.4571751   0.46342142  0.45535467]\n",
      "0.461002714493 0.00347003590046\n"
     ]
    }
   ],
   "source": [
    "#Try 3-, 5- and 10-fold Cross-validation\n",
    "cv3_results = cross_val_score(LR_cv, X_train2, Y_train, cv=3)\n",
    "print(cv3_results)\n",
    "print(np.mean(cv3_results), np.std(cv3_results))\n",
    "cv5_results = cross_val_score(LR_cv, X_train2, Y_train, cv=5)\n",
    "print(cv5_results)\n",
    "print(np.mean(cv5_results), np.std(cv5_results))\n",
    "cv10_results = cross_val_score(LR_cv, X_train2, Y_train, cv=10)\n",
    "print(cv10_results)\n",
    "print(np.mean(cv10_results), np.std(cv10_results))\n",
    "# 5-fold CV seems OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Ridge parameter: {'alpha': 0.0001}\n",
      "Best score:  0.461005562454\n"
     ]
    }
   ],
   "source": [
    "#Implement Grid search cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# with Ridge regression\n",
    "alpha_grid = {'alpha':np.logspace(-4,0,50)}\n",
    "ridge = Ridge(alpha=0.1, normalize=True)\n",
    "ridge_cv = GridSearchCV(ridge,alpha_grid,cv=5) #Instantiate Grid search CV regressor\n",
    "ridge_cv.fit(X_train2,Y_train) #Fit data and tune alpha to optimal value\n",
    "print('Optimal Ridge parameter:', ridge_cv.best_params_)\n",
    "print('Best score: ', ridge_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 31) (200000, 31) (800000,) (200000,)\n"
     ]
    }
   ],
   "source": [
    "# Create hold-out set (i.e. a test set from given data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train3, X_holdout3, Y_train3, Y_holdout3 = train_test_split(X_train2, Y_train, test_size=0.2, random_state=42) #How to set random_state?\n",
    "print(X_train3.shape, X_holdout3.shape, Y_train3.shape, Y_holdout3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try ElasticNet regression (combination of Ridge and Lasso)\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Create the hyperparameter grid (L1=1 for Lasso, <1 for Lasso/Ridge combination)\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "l1_grid = {'l1_ratio': l1_space}\n",
    "# Instantiate the ElasticNet regressor: EN\n",
    "EN = ElasticNet()\n",
    "# Setup the GridSearchCV object: EN_cv\n",
    "EN_cv = GridSearchCV(EN, l1_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned ElasticNet l1 ratio: {'l1_ratio': 0.0}\n",
      "Tuned ElasticNet R squared: 0.17387155110872932\n",
      "Tuned ElasticNet MSE: 0.13709885855522805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit it to the new training data\n",
    "EN_cv.fit(X_train3, Y_train3)\n",
    "# Predict on the test set and compute metrics\n",
    "Y_pred3 = EN_cv.predict(X_holdout3)\n",
    "r2_EN_cv = EN_cv.score(X_holdout3, Y_holdout3)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_EN_cv = mean_squared_error(Y_holdout3, Y_pred3)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(EN_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2_EN_cv))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse_EN_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Ridge parameter: {'alpha': 5.8570208180566735e-05}\n",
      "Best score:  0.461489960329\n",
      "Tuned Ridge R^2: 0.4590139793580381\n",
      "Tuned Ridge MSE: 0.0897784914971613\n"
     ]
    }
   ],
   "source": [
    "# Optimize Ridge alpha on new training data. Try lower range\n",
    "alpha_grid = {'alpha':np.logspace(-6,-1,100)}\n",
    "ridge = Ridge(normalize=True) # Initialize Ridge regressor\n",
    "ridge_cv = GridSearchCV(ridge,alpha_grid,cv=5) #Instantiate GridSearch CV regressor\n",
    "ridge_cv.fit(X_train3,Y_train3) #Fit data and tune alpha to optimal value\n",
    "print('Optimal Ridge parameter:', ridge_cv.best_params_)\n",
    "print('Best score: ', ridge_cv.best_score_)\n",
    "# Predict on the holdout set and compute metrics\n",
    "Y_pred3 = ridge_cv.predict(X_holdout3)\n",
    "r2_ridge_cv = ridge_cv.score(X_holdout3, Y_holdout3)\n",
    "mse_ridge_cv = mean_squared_error(Y_holdout3, Y_pred3)\n",
    "print(\"Tuned Ridge R^2: {}\".format(r2_ridge_cv))\n",
    "print(\"Tuned Ridge MSE: {}\".format(mse_ridge_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of predictors: 31\n",
      "   Optimal Ridge parameter: {'alpha': 5.4286754393238594e-05}\n",
      "   Best score:  0.461489958422\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4615412870213542, 0.45901409322897\n",
      "   Tuned Ridge MSE (holdout): 0.08977847259988837\n",
      "   Random Forest R^2 (train,holdout): 0.5544669644856763, 0.5516018947381855\n",
      "No. of predictors: 30\n",
      "   Optimal Ridge parameter: {'alpha': 5.4286754393238594e-05}\n",
      "   Best score:  0.461489676192\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4615369142822322, 0.4590204038087752\n",
      "   Tuned Ridge MSE (holdout): 0.08977742533745688\n",
      "   Random Forest R^2 (train,holdout): 0.5544565414451084, 0.551581353119931\n",
      "No. of predictors: 29\n",
      "   Optimal Ridge parameter: {'alpha': 5.4286754393238594e-05}\n",
      "   Best score:  0.461489676392\n",
      "   Tuned Ridge R^2 (train,holdout): 0.46153691430385313, 0.4590204026627419\n",
      "   Tuned Ridge MSE (holdout): 0.08977742552764505\n",
      "   Random Forest R^2 (train,holdout): 0.5544631765402315, 0.5516241924179079\n",
      "No. of predictors: 28\n",
      "   Optimal Ridge parameter: {'alpha': 9.9999999999999995e-07}\n",
      "   Best score:  0.46148771556\n",
      "   Tuned Ridge R^2 (train,holdout): 0.46153140017157035, 0.4590124299420607\n",
      "   Tuned Ridge MSE (holdout): 0.08977874862807385\n",
      "   Random Forest R^2 (train,holdout): 0.5544480245721923, 0.5516493302496425\n",
      "No. of predictors: 27\n",
      "   Optimal Ridge parameter: {'alpha': 9.9999999999999995e-07}\n",
      "   Best score:  0.461390419401\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4614278376280393, 0.45886182706971745\n",
      "   Tuned Ridge MSE (holdout): 0.0898037416929188\n",
      "   Random Forest R^2 (train,holdout): 0.5542282646857946, 0.5513637176124793\n",
      "No. of predictors: 26\n",
      "   Optimal Ridge parameter: {'alpha': 9.9999999999999995e-07}\n",
      "   Best score:  0.460652851064\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4606899946421871, 0.45812112408115335\n",
      "   Tuned Ridge MSE (holdout): 0.08992666390240918\n",
      "   Random Forest R^2 (train,holdout): 0.554228444012973, 0.5513951205332801\n",
      "No. of predictors: 25\n",
      "   Optimal Ridge parameter: {'alpha': 9.9999999999999995e-07}\n",
      "   Best score:  0.46018009613\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4602163659239036, 0.4576498985195644\n",
      "   Tuned Ridge MSE (holdout): 0.09000486540570155\n",
      "   Random Forest R^2 (train,holdout): 0.5534244735054722, 0.5506132806200825\n",
      "No. of predictors: 24\n",
      "   Optimal Ridge parameter: {'alpha': 9.9999999999999995e-07}\n",
      "   Best score:  0.460088057721\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4601234147675828, 0.4575651879871534\n",
      "   Tuned Ridge MSE (holdout): 0.09001892341001885\n",
      "   Random Forest R^2 (train,holdout): 0.5518543788421908, 0.5491254812078141\n",
      "No. of predictors: 23\n",
      "   Optimal Ridge parameter: {'alpha': 2.6826957952797274e-05}\n",
      "   Best score:  0.458842163935\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4588773333744268, 0.4561365288543455\n",
      "   Tuned Ridge MSE (holdout): 0.0902560143086985\n",
      "   Random Forest R^2 (train,holdout): 0.551657020073266, 0.5490040302290056\n",
      "No. of predictors: 22\n",
      "   Optimal Ridge parameter: {'alpha': 3.3932217718953299e-05}\n",
      "   Best score:  0.455800900489\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4558355644383161, 0.4531556908037767\n",
      "   Tuned Ridge MSE (holdout): 0.09075069463936554\n",
      "   Random Forest R^2 (train,holdout): 0.550046908987829, 0.547408502310814\n",
      "No. of predictors: 21\n",
      "   Optimal Ridge parameter: {'alpha': 2.6826957952797274e-05}\n",
      "   Best score:  0.455594463182\n",
      "   Tuned Ridge R^2 (train,holdout): 0.45562889877973756, 0.4529125731087841\n",
      "   Tuned Ridge MSE (holdout): 0.09079104085734509\n",
      "   Random Forest R^2 (train,holdout): 0.5498593610346312, 0.5472057480224819\n",
      "No. of predictors: 20\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.455094931986\n",
      "   Tuned Ridge R^2 (train,holdout): 0.45512743427202706, 0.45247872216914553\n",
      "   Tuned Ridge MSE (holdout): 0.09086303991352984\n",
      "   Random Forest R^2 (train,holdout): 0.5480820224802119, 0.5454817723269383\n",
      "No. of predictors: 19\n",
      "   Optimal Ridge parameter: {'alpha': 2.6826957952797274e-05}\n",
      "   Best score:  0.44871157109\n",
      "   Tuned Ridge R^2 (train,holdout): 0.44874185399478095, 0.44619591294958727\n",
      "   Tuned Ridge MSE (holdout): 0.09190569372078915\n",
      "   Random Forest R^2 (train,holdout): 0.5451105077456975, 0.5425920103691334\n",
      "No. of predictors: 18\n",
      "   Optimal Ridge parameter: {'alpha': 2.6826957952797274e-05}\n",
      "   Best score:  0.448640551056\n",
      "   Tuned Ridge R^2 (train,holdout): 0.44867062957876946, 0.44606285698294923\n",
      "   Tuned Ridge MSE (holdout): 0.09192777481625865\n",
      "   Random Forest R^2 (train,holdout): 0.5420810572686936, 0.5398404739830026\n",
      "No. of predictors: 17\n",
      "   Optimal Ridge parameter: {'alpha': 3.3932217718953299e-05}\n",
      "   Best score:  0.448640551185\n",
      "   Tuned Ridge R^2 (train,holdout): 0.44867062915312017, 0.44606284981534217\n",
      "   Tuned Ridge MSE (holdout): 0.0919277760057477\n",
      "   Random Forest R^2 (train,holdout): 0.5420775821023661, 0.539866509519379\n",
      "No. of predictors: 16\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.448640550811\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4486706298278015, 0.44606284321511736\n",
      "   Tuned Ridge MSE (holdout): 0.09192777710107772\n",
      "   Random Forest R^2 (train,holdout): 0.5420768610193794, 0.539797874789848\n",
      "No. of predictors: 15\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.447926671921\n",
      "   Tuned Ridge R^2 (train,holdout): 0.44795525107700307, 0.44535408955530054\n",
      "   Tuned Ridge MSE (holdout): 0.09204539720953439\n",
      "   Random Forest R^2 (train,holdout): 0.535321438902097, 0.5335388871851112\n",
      "No. of predictors: 14\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.446259758544\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4462879768786967, 0.4440162959155748\n",
      "   Tuned Ridge MSE (holdout): 0.09226740866699598\n",
      "   Random Forest R^2 (train,holdout): 0.5292105775960527, 0.5275827368166872\n",
      "No. of predictors: 13\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.442592592689\n",
      "   Tuned Ridge R^2 (train,holdout): 0.44262159796827505, 0.4406153094423777\n",
      "   Tuned Ridge MSE (holdout): 0.09283181407400365\n",
      "   Random Forest R^2 (train,holdout): 0.522104175040736, 0.5202349359644363\n",
      "No. of predictors: 12\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.442301436737\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4423292555555729, 0.44039293269647395\n",
      "   Tuned Ridge MSE (holdout): 0.09286871826011847\n",
      "   Random Forest R^2 (train,holdout): 0.5095085935453911, 0.5073914697754982\n",
      "No. of predictors: 11\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.441839115355\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4418665669283264, 0.44003593717968714\n",
      "   Tuned Ridge MSE (holdout): 0.0929279628944444\n",
      "   Random Forest R^2 (train,holdout): 0.498057243176618, 0.4963412596009963\n",
      "No. of predictors: 10\n",
      "   Optimal Ridge parameter: {'alpha': 1.3257113655901109e-05}\n",
      "   Best score:  0.44088638499\n",
      "   Tuned Ridge R^2 (train,holdout): 0.44091040987520946, 0.4390939609427039\n",
      "   Tuned Ridge MSE (holdout): 0.09308428709203119\n",
      "   Random Forest R^2 (train,holdout): 0.49262582545216316, 0.4905389564101825\n",
      "No. of predictors: 9\n",
      "   Optimal Ridge parameter: {'alpha': 1.3257113655901109e-05}\n",
      "   Best score:  0.440844298315\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4408653143842747, 0.4390432409203342\n",
      "   Tuned Ridge MSE (holdout): 0.09309270425425592\n",
      "   Random Forest R^2 (train,holdout): 0.48197805489028167, 0.4806596622862541\n",
      "No. of predictors: 8\n",
      "   Optimal Ridge parameter: {'alpha': 1.3257113655901109e-05}\n",
      "   Best score:  0.43592340318\n",
      "   Tuned Ridge R^2 (train,holdout): 0.4359440284622379, 0.4336217643466771\n",
      "   Tuned Ridge MSE (holdout): 0.09399241694533905\n",
      "   Random Forest R^2 (train,holdout): 0.4713666799945573, 0.4696685193049379\n",
      "No. of predictors: 7\n",
      "   Optimal Ridge parameter: {'alpha': 1.3257113655901109e-05}\n",
      "   Best score:  0.427107288815\n",
      "   Tuned Ridge R^2 (train,holdout): 0.42712664079269214, 0.4248813770825669\n",
      "   Tuned Ridge MSE (holdout): 0.09544291428488517\n",
      "   Random Forest R^2 (train,holdout): 0.4606791161261137, 0.45929482815439815\n",
      "No. of predictors: 6\n",
      "   Optimal Ridge parameter: {'alpha': 8.2864277285468425e-06}\n",
      "   Best score:  0.425410042517\n",
      "   Tuned Ridge R^2 (train,holdout): 0.42542851844605656, 0.42323399744377044\n",
      "   Tuned Ridge MSE (holdout): 0.0957163026040857\n",
      "   Random Forest R^2 (train,holdout): 0.4545526221088556, 0.45306298091861075\n",
      "No. of predictors: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.397946631086\n",
      "   Tuned Ridge R^2 (train,holdout): 0.3979655771529226, 0.395630765700685\n",
      "   Tuned Ridge MSE (holdout): 0.10029715388634257\n",
      "   Random Forest R^2 (train,holdout): 0.4306401295308582, 0.4284814033616793\n",
      "No. of predictors: 4\n",
      "   Optimal Ridge parameter: {'alpha': 1.3257113655901109e-05}\n",
      "   Best score:  0.375878839568\n",
      "   Tuned Ridge R^2 (train,holdout): 0.37589705088551306, 0.3741144608050647\n",
      "   Tuned Ridge MSE (holdout): 0.10386785871496182\n",
      "   Random Forest R^2 (train,holdout): 0.3914871059542564, 0.39014108756588084\n",
      "No. of predictors: 3\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.356524900459\n",
      "   Tuned Ridge R^2 (train,holdout): 0.3565409678307798, 0.3552027447921292\n",
      "   Tuned Ridge MSE (holdout): 0.10700632305688565\n",
      "   Random Forest R^2 (train,holdout): 0.36911270507587035, 0.36809809726937337\n",
      "No. of predictors: 2\n",
      "   Optimal Ridge parameter: {'alpha': 1.67683293681101e-05}\n",
      "   Best score:  0.334552416021\n",
      "   Tuned Ridge R^2 (train,holdout): 0.3345674750043567, 0.3327080738002659\n",
      "   Tuned Ridge MSE (holdout): 0.11073939110544254\n",
      "   Random Forest R^2 (train,holdout): 0.34501063553525346, 0.34353341028070017\n",
      "No. of predictors: 1\n",
      "   Optimal Ridge parameter: {'alpha': 6.5512855685955095e-06}\n",
      "   Best score:  0.176607770815\n",
      "   Tuned Ridge R^2 (train,holdout): 0.17661834699986645, 0.17405524633608005\n",
      "   Tuned Ridge MSE (holdout): 0.13706837370020866\n",
      "   Random Forest R^2 (train,holdout): 0.17661833660081416, 0.17405449977045262\n"
     ]
    }
   ],
   "source": [
    "# Drop features according to results from Ridge optimized by Grid Search Cross-Validation\n",
    "X_train0 = X_train3\n",
    "Y_train0 = Y_train3\n",
    "X_holdout0 = X_holdout3\n",
    "Y_holdout0 = Y_holdout3\n",
    "nx = np.array([])\n",
    "R2_train_rcv = np.array([])\n",
    "R2_holdout_rcv = np.array([])\n",
    "R2_train_RF = np.array([])\n",
    "R2_holdout_RF = np.array([])\n",
    "alpha_grid = {'alpha':np.logspace(-6,-1,50)}\n",
    "for i in range(31):\n",
    "    n0 = X_train0.shape[1]\n",
    "    nx = np.append(nx,n0)\n",
    "    print('No. of predictors:', n0)\n",
    "    ridge = Ridge(alpha=0.1,normalize=True) # Initialize Ridge regressor\n",
    "    rcv = GridSearchCV(ridge,alpha_grid,cv=5) # Initiate GridSearch CV regressor\n",
    "    rcv.fit(X_train0,Y_train0) #Fit data and tune alpha to optimal value\n",
    "    print('   Optimal Ridge parameter:', rcv.best_params_)\n",
    "    print('   Best score: ', rcv.best_score_)\n",
    "    # Predict on both training and holdout set and compute metrics\n",
    "    Y_pred0 = rcv.predict(X_holdout0)\n",
    "    R2_train_rcv0   = rcv.score(X_train0,   Y_train0)\n",
    "    R2_holdout_rcv0 = rcv.score(X_holdout0, Y_holdout0)\n",
    "    mse_rcv = mean_squared_error(Y_holdout0, Y_pred0)\n",
    "    print(\"   Tuned Ridge R^2 (train,holdout): {}, {}\".format(R2_train_rcv0,R2_holdout_rcv0))\n",
    "    print(\"   Tuned Ridge MSE (holdout): {}\".format(mse_rcv))\n",
    "    R2_train_rcv = np.append(R2_train_rcv, R2_train_rcv0)\n",
    "    R2_holdout_rcv = np.append(R2_holdout_rcv, R2_holdout_rcv0)\n",
    "    # Compare to Random Forest result\n",
    "    RF0 = RandomForestRegressor()\n",
    "    RF0.fit(X_train0, Y_train0)\n",
    "    R2_train_RF0 = RF0.score(X_train0, Y_train0)\n",
    "    R2_holdout_RF0 = RF0.score(X_holdout0, Y_holdout0)\n",
    "    print('   Random Forest R^2 (train,holdout): {}, {}'.format(R2_train_RF0,R2_holdout_RF0))\n",
    "    R2_train_RF = np.append(R2_train_RF, R2_train_RF0)\n",
    "    R2_holdout_RF = np.append(R2_holdout_RF, R2_holdout_RF0)\n",
    "    # Drop least important RF feature\n",
    "    i0 = (range(n0) != RF0.feature_importances_.argmin())\n",
    "    X_train0 = X_train0[:,i0]\n",
    "    X_holdout0 = X_holdout0[:,i0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of predictors: 31\n",
      "   NR Linear Regression R^2 (train,holdout): 0.46153993595628773, 0.45901235466094914\n",
      "   Random Forest R^2 (train,holdout): 0.5544584504489196, 0.5516010535979181\n",
      "No. of predictors: 30\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4615367590545181, 0.4590206517118671\n",
      "   Random Forest R^2 (train,holdout): 0.5544609406197744, 0.5515829832864672\n",
      "No. of predictors: 29\n",
      "   NR Linear Regression R^2 (train,holdout): 0.46153674331180705, 0.459020636537532\n",
      "   Random Forest R^2 (train,holdout): 0.5544575095420763, 0.5515595642838094\n",
      "No. of predictors: 28\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4614495578568437, 0.45889532630066887\n",
      "   Random Forest R^2 (train,holdout): 0.5542411371723615, 0.5513819878686399\n",
      "No. of predictors: 27\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4614275306066661, 0.45886074460023296\n",
      "   Random Forest R^2 (train,holdout): 0.5542263947767236, 0.5513230373829381\n",
      "No. of predictors: 26\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4606847117781765, 0.45813070500062214\n",
      "   Random Forest R^2 (train,holdout): 0.5542304889787444, 0.5513461041476271\n",
      "No. of predictors: 25\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4602113244537763, 0.4576593494969583\n",
      "   Random Forest R^2 (train,holdout): 0.5534262349892796, 0.5505477169597199\n",
      "No. of predictors: 24\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4601182745394211, 0.4575747598009162\n",
      "   Random Forest R^2 (train,holdout): 0.551844403042762, 0.5491731024311775\n",
      "No. of predictors: 23\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4599250785202367, 0.4573267371088544\n",
      "   Random Forest R^2 (train,holdout): 0.5516532237707059, 0.5488972476494961\n",
      "No. of predictors: 22\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4586998439732972, 0.4559214460041362\n",
      "   Random Forest R^2 (train,holdout): 0.5514803122050513, 0.5487750800007014\n",
      "No. of predictors: 21\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4556282866989604, 0.452913250794713\n",
      "   Random Forest R^2 (train,holdout): 0.5498568160383924, 0.5472048839204656\n",
      "No. of predictors: 20\n",
      "   NR Linear Regression R^2 (train,holdout): 0.45512672138361865, 0.4524735207792374\n",
      "   Random Forest R^2 (train,holdout): 0.5480794127093183, 0.5454994607233467\n",
      "No. of predictors: 19\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4487411641754321, 0.4461901998247644\n",
      "   Random Forest R^2 (train,holdout): 0.5451140978290681, 0.5425345688108316\n",
      "No. of predictors: 18\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4486699371786791, 0.4460571383815811\n",
      "   Random Forest R^2 (train,holdout): 0.5420859899839736, 0.5398011053309865\n",
      "No. of predictors: 17\n",
      "   NR Linear Regression R^2 (train,holdout): 0.44867062836129534, 0.4460625655356216\n",
      "   Random Forest R^2 (train,holdout): 0.5420791784286767, 0.5398262959665874\n",
      "No. of predictors: 16\n",
      "   NR Linear Regression R^2 (train,holdout): 0.44795525087570265, 0.4453539120519542\n",
      "   Random Forest R^2 (train,holdout): 0.5353200224828473, 0.5334693609870553\n",
      "No. of predictors: 15\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4462879303669004, 0.4440181066950075\n",
      "   Random Forest R^2 (train,holdout): 0.5292035129378121, 0.5276000238363446\n",
      "No. of predictors: 14\n",
      "   NR Linear Regression R^2 (train,holdout): 0.44628797728828096, 0.444016374147272\n",
      "   Random Forest R^2 (train,holdout): 0.5292074370434872, 0.527576894178322\n",
      "No. of predictors: 13\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4426215984133607, 0.44061538419657165\n",
      "   Random Forest R^2 (train,holdout): 0.5221011878273897, 0.5202339966697082\n",
      "No. of predictors: 12\n",
      "   NR Linear Regression R^2 (train,holdout): 0.44232925600096906, 0.44039300903160605\n",
      "   Random Forest R^2 (train,holdout): 0.5095079016751263, 0.5073633050407996\n",
      "No. of predictors: 11\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4418665673722417, 0.4400360038012956\n",
      "   Random Forest R^2 (train,holdout): 0.4980578241549026, 0.49631236296268266\n",
      "No. of predictors: 10\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4409104101488859, 0.4390940098021193\n",
      "   Random Forest R^2 (train,holdout): 0.4926255616222426, 0.4905330665715183\n",
      "No. of predictors: 9\n",
      "   NR Linear Regression R^2 (train,holdout): 0.44086531465638723, 0.4390432916249062\n",
      "   Random Forest R^2 (train,holdout): 0.48197634616380014, 0.4806507126831048\n",
      "No. of predictors: 8\n",
      "   NR Linear Regression R^2 (train,holdout): 0.4359440286380195, 0.4336217633703031\n",
      "   Random Forest R^2 (train,holdout): 0.47136739819989315, 0.4696577563243277\n",
      "No. of predictors: 7\n",
      "   NR Linear Regression R^2 (train,holdout): 0.427126640961873, 0.42488138250999175\n",
      "   Random Forest R^2 (train,holdout): 0.46067995561136976, 0.4592902815752852\n",
      "No. of predictors: 6\n",
      "   NR Linear Regression R^2 (train,holdout): 0.42542851851721875, 0.4232340010408314\n",
      "   Random Forest R^2 (train,holdout): 0.4545534844926272, 0.45306445463174616\n",
      "No. of predictors: 5\n",
      "   NR Linear Regression R^2 (train,holdout): 0.3979655773274646, 0.3956307384328478\n",
      "   Random Forest R^2 (train,holdout): 0.430639264257653, 0.4284776819682803\n",
      "No. of predictors: 4\n",
      "   NR Linear Regression R^2 (train,holdout): 0.3758970509919405, 0.3741144323155978\n",
      "   Random Forest R^2 (train,holdout): 0.39148695674711786, 0.3901432901280719\n",
      "No. of predictors: 3\n",
      "   NR Linear Regression R^2 (train,holdout): 0.35654096797333346, 0.3552027319505153\n",
      "   Random Forest R^2 (train,holdout): 0.3691122727658084, 0.3681063802985044\n",
      "No. of predictors: 2\n",
      "   NR Linear Regression R^2 (train,holdout): 0.334567475121445, 0.33270804541778676\n",
      "   Random Forest R^2 (train,holdout): 0.34501093250628123, 0.3435304396053823\n",
      "No. of predictors: 1\n",
      "   NR Linear Regression R^2 (train,holdout): 0.17661834700744672, 0.17405523079010599\n",
      "   Random Forest R^2 (train,holdout): 0.17661830467437656, 0.17405405548334962\n"
     ]
    }
   ],
   "source": [
    "# Try the same in non-regularized linear regression\n",
    "X_train0 = X_train3\n",
    "Y_train0 = Y_train3\n",
    "X_holdout0 = X_holdout3\n",
    "Y_holdout0 = Y_holdout3\n",
    "nx = np.array([])\n",
    "R2_train_LR = np.array([])\n",
    "R2_holdout_LR = np.array([])\n",
    "R2_train_RF = np.array([])\n",
    "R2_holdout_RF = np.array([])\n",
    "for i in range(31):\n",
    "    n0 = X_train0.shape[1]\n",
    "    nx = np.append(nx,n0)\n",
    "    print('No. of predictors:', n0)\n",
    "    LR0 = LinearRegression()\n",
    "    LR0.fit(X_train0, Y_train0)\n",
    "    R2_train_LR0 = LR0.score(X_train0, Y_train0)\n",
    "    R2_holdout_LR0 = LR0.score(X_holdout0, Y_holdout0) \n",
    "    print('   NR Linear Regression R^2 (train,holdout): {}, {}'.format(R2_train_LR0,R2_holdout_LR0))\n",
    "    R2_train_LR = np.append(R2_train_LR, R2_train_LR0)\n",
    "    R2_holdout_LR = np.append(R2_holdout_LR, R2_holdout_LR0)\n",
    "    # Compare to Random Forest result\n",
    "    RF0 = RandomForestRegressor()\n",
    "    RF0.fit(X_train0, Y_train0)\n",
    "    R2_train_RF0 = RF0.score(X_train0, Y_train0)\n",
    "    R2_holdout_RF0 = RF0.score(X_holdout0, Y_holdout0)\n",
    "    print('   Random Forest R^2 (train,holdout): {}, {}'.format(R2_train_RF0,R2_holdout_RF0))\n",
    "    R2_train_RF = np.append(R2_train_RF, R2_train_RF0)\n",
    "    R2_holdout_RF = np.append(R2_holdout_RF, R2_holdout_RF0)\n",
    "    # Drop least important RF feature\n",
    "    i0 = (range(n0) != RF0.feature_importances_.argmin())\n",
    "    X_train0 = X_train0[:,i0]\n",
    "    X_holdout0 = X_holdout0[:,i0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3X18VPWd9//X55y5Sya3QLi/SYCg\nICpIBG/qTVGUqgWtN1Xrrnbbtfaqa6tXt9V2V1u77c/a1l7tb71vXa3bVnu3Xuiq6BatqIgEUREQ\nCfch3OSOBHI3M+d8rj9mwBCCBEgySebzfDzGmXPO98x8jkPeOfme7zlHVBVjjDGZwUl3AcYYY3qP\nhb4xxmQQC31jjMkgFvrGGJNBLPSNMSaDWOgbY0wGsdA3xpgMYqFvjDEZxELfGGMySCDdBXQ0ZMgQ\nLS4uTncZxhjTryxfvrxGVYsO167PhX5xcTHl5eXpLsMYY/oVEdnclXbWvWOMMRnEQt8YYzKIhb4x\nxmQQC31jjMkgFvrGGJNBLPSNMSaDWOgbY0wG6dI4fRGZC/wCcIFfqeo9HZbfAPwE2Jaa9e+q+qvU\nMg9YmZq/RVXndUPdxpgBTlVJ+AmaE820JlppSbTQ6iWf9z1aE63EvFjX3g/FUw/f95PP6u9/7Jv2\n1ENVUfSgdXti+zoaFh3GlZOu7PbPau+woS8iLnA/MAeoBJaJyAJVXd2h6dOqenMnb9GiqtOOvVRj\nTF8V9+K0eW0Hhen+B/7+sG1ONLO7dTd1bXXJ59Y66tvqD3rdGGvEU693NkBBFBwFUTlwkRxihf3/\nBYQDpzvM72TV1OIDG0wtmpr+0AdmAhWqugFARJ4C5gMdQ98Y0w/EvBj1rfUkNIHvpwK5s6BO7fk2\nxhqpb61PBnJrPfVt9dS11FHXlppurWdvfO9R1SIKkUSAIgoYrHkU+FHG+EVEE6MIJ1wCnoPrgZMA\nx1Mk7kPcQ1MPPxbH9xLtorNdiIocMEcB9X3wk7+A1PPxfQ/1/eT8PmDExBFwSc9+RldCfxSwtd10\nJTCrk3aXi8jZwEfAraq6b52IiJQDCeAeVX3mWAo2xnRuT2wPW/ZsoballtqWWmpaaqhtPfh1Y6zx\nyN9cIRx3iMZDDCGfQZrDCC+bSfFhZMVHEYolw1lSbQVJPX+8fnKZQtxHW2J4TS3EmluS8wDwgMbU\nA3AcJBzBDYcJhMMEwxGCoQiB7DDBcJhAOEIwFMYNBhCRA7tLdN/TgfvejuMgjovjJh/iODiOi+Mm\nn8Vxko/UL4zOumD2z+uwrON8/biIg/b4O+7h7/sFlVM46KDP625dCf3D/IECwLPA71W1TURuAp4A\nZqeWjVXVKhEZDywSkZWquv6ADxC5EbgRYOzYsUe0AcZkkoSfoHJPJZsaN7GpYVPyOfW6trV2fzvH\ng0jMZbCXQxEFFHpRxiQGEY0NIxwTAq2K+CQDyldEk69VNfnT7ev+ZV5rG4mmlg57wwn2hXMoK4us\n3DwCoXAyMCEZ2CL7H+x7doRgbpiskXlEcvPIys0jKzc3+ZyTS1ZePpGcXLLy8giGI/vD13SfroR+\nJTCm3fRooKp9A1WtbTf5KPDjdsuqUs8bRORVYDqwvsP6jwCPAJSVlXX/ERNj+pmGtgY2NmxMPhqT\nz5saNlG5p5KEJkAh2uoyKlbIWK+ISa0lRPdOwG2I4e1tIdHS2uEdm4AmguEI2QUFZOfm40aCyVDe\nt3frODiOAyKpPWIHEYdINIesvDyycvPJzssjKy+frLx8svPyU2EfSsf/InOUuhL6y4BSESkhOTrn\nauDa9g1EZISqbk9NzgPWpOYXAs2pvwCGAGcC93ZX8cb0prgXpznR3K3v2djWuD/U9z02NW6irrUO\nFLJiDoUtWRR7Qzklls+5TUWEGuIk6vbgxxOpd2kmGPYpHDGKgvHDiRYOIju/YP8jml9Adn4+2XkF\nBCORbq3f9D+HDX1VTYjIzcBCkkM2H1PVVSJyN1CuqguAW0RkHsm/+eqAG1KrTwYeFhGf5DkB93Qy\n6seYPivmxXhj2xu8sOkFXt36Ki2Jlu79AIXsNpe8pgDDYvmMihcyuXUskT2j0fpm/Fg81dDHcfeQ\nP2wEhaOLKTx1FINGjKJwxEgKR4wiWjjIukJMl0hnByrSqaysTO16+iadPN/j7R1v8+KmF3l508u4\nda1MqinkuNrBBFs6HizUA54P9dMkh5jQWAKNfzws0XED5A8bTuHwERQMG0HB8BEUDB9JwfAR5BcN\nw3Hd7tpMM8CIyHJVLTtcuz53ExVj0kFVea/6PV7Y+AILNy6EnXuYuCufS6uH4zbEQISRpeMYdMLo\n1BqSHHAhkhyJIaT2tGX/SIx2b95+4oBFgXAkFfDJYM8dMgTHsWA3PcdC32QsX31W1qzkr5v/ysKN\nC/G21jJ+Zy6fqS4g0JSD47qMOeEESmeezoSy03plOJ0xPc1C32SUmBfj7R1vs2jLIv628RVClU0U\n74oyuzqXQOtw3GCQ4pNnUDrzdMbPmElWTm66SzamW1nomwFvb2wvi7ctZtGWRby9/nUGV0FxdQ5z\na3JwElGCkQjjT5lJ6cwzKJk+g1AkK90lG9NjLPTNgLS7dTcvbX6JRVv+yofr3mHkjhAl1Xl8tm4w\nAuQOGcKE805jQtksxkyZihsIprtkY3qFhb4ZMGJejNcqX+PZigWsXPMW4yojjK/OpXTvMACGjZ/I\nhPNnMWHGLIrGldgQR5ORLPRNv6aqvFv9Ls+uf5ZX175E0Saf46vyubhhKE7AZdyJ05kwYxbjZ5xK\n7qAh6S7XmLSz0Df90pbGLTy34TmeW/cssqGWSVX5XLSrAPFh2IRSpl4xh+PPOJtITk66SzWmT7HQ\nN/3Ksh3L+OU7v2Tz+lWUVuZwzo4CAq1Dyc4vYMrFsznhnPMYMmZcuss0ps+y0Df9QmOskfvK7+PN\nN/+b09cNZdrukTiuy8Sy0zjh3PMpPvkUO1vVmC6w0Dd93l+3/JUfv/ZDxq/wubByGPnDRzDji/M4\n/sxzyMrNS3d5xvQrFvqmz6ppqeFHS3/Emrdf59OrhxJqFWbOv5zTr7jWLudrzFGy0Dd9jqryTMUz\n/PLNn3HCeyHO3zaUwWPGMvertzJ8Qmm6yzOmX7PQN33K1j1buXvJ3VSteI8LVw8jFBNmXf55Tvvc\nVXYClTHdwELf9Ame7/HbNb/l0aX3M2NlHrOrhjK0uIQLv/oNhhaPT3d5xgwYFvom7d7Z+Q7/39If\n0bZqK59dM4yg53DG1ddS9tnP4Qbsn6gx3cl+okza7GzayX3L7+PN91/mUx8NZ8jOIoZPmMiFX/26\njbU3poc4XWkkInNFZK2IVIjI7Z0sv0FEqkXk3dTjy+2WXS8i61KP67uzeNM/xbwYv1r5K654eh4N\nz77Npa+PZOSeXD59/T9yzb/9xALfmB502D19EXGB+4E5QCWwTEQWdHKv26dV9eYO6w4C7gLKSN4y\naHlq3fpuqd70K6rKa5Wvce9bPyZvZQPzNg7DTcC0Cy/m9CuusTH3xvSCrnTvzAQqVHUDgIg8BcwH\nunKD8wuBl1W1LrXuy8Bc4PdHV67przY2bOTet3/M1uUrOP2jIrKaChl/ygzOvu4fGDxqTLrLMyZj\ndCX0RwFb201XArM6aXe5iJwNfATcqqpbD7HuqKOs1fRDDW0N/Hrlr/nvJU8xc3UhE+uKGDxmHOd+\n48sUnzQ93eUZk3G6EvqdXXRcO0w/C/xeVdtE5CbgCWB2F9dFRG4EbgQYO3ZsF0oyfZXne3xQ+wFv\nbnuTN7a9zubNHzK1IpeLtg0lkpfHWf/490ydPcdu/m1MmnQl9CuB9n9/jwaq2jdQ1dp2k48CP263\n7rkd1n214weo6iPAIwBlZWUH/VIwfdvOpp28WfUmr297naXb3iJ7R4wxu7KYXFvA9D0jcAIByuZf\nysxLryKcnZ3uco3JaF0J/WVAqYiUANuAq4Fr2zcQkRGquj01OQ9Yk3q9EPiRiBSmpi8A7jjmqk3a\nfVDzAS9ufJE3qt6gcucGRlVnMbG2gEt3DUbiPm4wyNgTTmL8KTOZcOosu4GJMX3EYUNfVRMicjPJ\nAHeBx1R1lYjcDZSr6gLgFhGZBySAOuCG1Lp1IvIDkr84AO7ed1DX9E+bGjbxi3d+wdI1rzJhRy4z\n6gbzqerkH4LRwkGMP/tUxp8yk3FTTyYYiaS5WmNMR6Lat3pTysrKtLy8PN1lmA5qWmp46L2HWLji\nGaZVFFC8LQIKwyeUMv6UmYw/5VSGlkyw+84akyYislxVyw7Xzs7INZ+oOd7ME6uf4I9Lf8PxayPM\n3zacQDDE9EsuYcZF88kZNDjdJRpjjoCFvulUwk/wl3V/4fE3H2LMBz4XbRuMGwgy/aKLOXXe5UQL\nCg//JsaYPsdC3xxAVXll6ys8uPjnFK5oZHZlLq4bYNrcizh1/hXkFA5Kd4nGmGNgoW/2e3fXu/z7\naz9Flm5lZmUurhRw0py5zLrsSht9Y8wAYaFv2NiwkV++9XNq/7aCqRvzcCWfE2dfwGmXfZ68IUXp\nLs8Y040s9DNYTUsND77zAO+9spBpH+Uzui2f0jM+xTnXfJH8ocPSXZ4xpgdY6GegpngTj696nBdf\n+R0nr8rh9D2FDC0t5fwbbmLExOPSXZ4xpgdZ6GeQuB/nTx/9if9c/Ail7wnnVhcSHTKY2V/+R0pn\nnWlj7I3JABb6GUBVeWnzSzy45JcMWbGHT2/JIxgOc8YXrmX63M8SCIXSXaIxppdY6A9wy3cu5+dL\nf0Zi+WZOX19IwMvn5Dmf4Ywrv0B2Xn66yzPG9DIL/QFqQ8MGfr7sPirfKmdGxSCyWgopnjaDc//u\nSwwebZevNiZTWegPMDUtNTyw4n6WLX6BUz4qoHjvEIZOmMg5136RsVNPTnd5xpg0s9AfIJrjzTy+\n6nGef/W3nLgmyrm7h5A/ciTnfOUGJp56uh2kNcYAFvr93r5r5Pz2lYeYsFL4dE0hWYMKOfumv2fK\n2bNxXLtDlclMiXiM1pYmWpv3EmtpIt7WjKriq6KqoOCnrjLsp6YV8NVHUFCFds+dzetu4ew8Sqac\n2u3v256Ffj+kqlTsrmDRlkX89d1nGfpOE2fviBKMZnPm31/LyXMushE5pmeoQqwJWhsOfnht4Hv4\nvkdrLE5rLEZrLEFbLE5rLEEsHqMtliDheQh+KjPbB6l/YKiqh3hxxI8hfhzxYjh+PPnQOK4fw/ET\nBDRG0G8jqG2EtY0wbUS0jZB45AA5af0fdmTWBo6DKW/36GdY6PcTnu/xfs37LNqyiDfXLCKwvp6x\nO7KZWR/BDRUw8/LLKbvkc3Y7QnP0VKGpGqrXQs1HULMOatdBU82B4a7eQav66uBpEMXBx0VwCKtD\nCAfFwfNdfM/Fi7l4nosigCSzHcFXQVVQnOQ+tQo+Lm0aJuGHiWuIhB8iRhRPQyQ0iK8hPA3gEySh\nYXxCeBrCJ4hPEI8gSgBPktO+uKCH7+bU/bf2FlRSdSKQ6iJVHBDpgf18yNM6evr0SAv9PizmxXhr\n+1ss2vxXyle/Rv6WOON2ZnNGQwgYROGYMRx/3llMu+AisvML0l2u6YPU90kk4sTaWoi3tZGItRKL\nteK1NeHu3kiwroJAfQXh3RWEdlcQiDUm11Nht4xmV/AU6nU8uxO5NMRyaIhl0RyPkPCzwQ/j+CGC\nGsDp6SgRkvft+wT7/gpwNYFLAhcfVzyC4uM6iut4pDI8+R/h42NdqefktKYW+/s/Gtn3q+CAXwnd\nuokAuYN6/u+SLn1TIjIX+AXJ/+2/UtV7DtHuCuCPwKmqWi4ixSTvl7s21eQtVb3pWIse6HY07eBn\ny37Kyg+WMKzKoXhnlE/vjQIwbGIpx332LEpPPZ2C4SPSXKnpC9pam1nzxgJi7z/DmIZyIrQR0Dgh\nEgRJEBQleIh1Y36EjYnjWBefzPbEXBq9EXheIWEvmwAH7hUripAg228jlGgi3LqdYEsDwXgTgUQz\nrhdDHMGNhHGzIjiRMG40CzcrghvNxs3Owo1mI6EQjsOBQSupEHZk/7Q4QiAcwA0FcENB3HDqORIk\nEA7hhoO4WSECkTDB3Czc0KG20rR32NAXERe4H5gDVALLRGSBqq7u0C4XuAVY2uEt1qvqtG6qd8Br\njjdzx+//FyWvtzCnpRAcYcyUE5k080wmnnqa3anKANDavJfVi5/BX/VfHNfwBif6bXxYXUZ569V4\nEsFXFyWQ6i7Z9+zi4ya7YCRIS6CQmJv78ZuqT0GikezYDrJitURjNWS31hDZtZlg2x4CiWYExS0s\nJFRSQqikmHBJCaGSUwmVlBAcNgzJzraRYn1cV/b0ZwIVqroBQESeAuYDqzu0+wFwL/DNbq0wg6gq\ndy28nQl/a6EgdzCf/uIXGT9jJlk5uYdf2Qx4TXt28+Hi/4LVzzB5zxJO9DxW7Tqdha23URc8nngw\nB8I+jnqIn8DBx1EfwUu+JjkqxcHHFZ+h/k5y/HXksJccZy9Rpxk3LEi2C66LOA4EhhIcdtIBIe8W\nWFdif9aV0B8FbG03XQnMat9ARKYDY1T1ORHpGPolIrICaAT+RVUXH0vBA9nD7zyI++wasohyzXd+\nZGfO9jBVpbE1Qe3eNppjHq1xj9a4T0t83+vUI+bhte7BaakhkGgm6LcS8NsI+G2HfO10crCzcz6S\nGjaI+vsqO3BIoCrhpiomN73N1ITD+zvP4rnYHdSFJuEFsnDDrYwI1TH+lAiTPnc64ULbSTCH1pXQ\n7+xvtf1HMETEAX4O3NBJu+3AWFWtFZEZwDMicoKqNh7wASI3AjcCjB2bmUG3aMsi3vn90xy3O5fP\n3vbPFvjHQH2fXVUb2VHxLo1NLTS2JtjbGqexJUFjW4I9LXEaW+PsaY2T8MHBJ58mBktj8kEjg2lg\nrDYyyN9DoddIyPdQLznapCNfXRKE8AiR0BBxDZM4ZC96h1ohNVIlOcpl3wgWXx1UU/OA+rYRPJO4\ni/pwKX4gRJBmRkXrmTgrmwnzZxOKRrr1/6EZuLoS+pXAmHbTo4GqdtO5wFTg1VRf3nBggYjMU9Vy\noA1AVZeLyHpgElDe/gNU9RHgEYCysrKeGAnVp62rX8ejT36fsq25zJj/OSbNOjPdJfUbiXiMreve\no7ainMS298jZvYZRbesp8vcQaRlEvC2IH3Pw4oIXd/Bjgh93Dnod0yzanBziEiXmDiHujGVrIJuN\ngSjxQDaJYDbxQDaeG8Z3gnhuCN8J4jsh1OmFE+BCEJY9FOfWUnpWMSUXnYUbtgOX5sh1JfSXAaUi\nUgJsA64Grt23UFUbgP03UBWRV4FvpkbvFAF1quqJyHigFNjQjfX3e7tbd/Mvf7yFspU5jJg6lbOv\nvj7dJXU73/OIxVqJx9pIxNpIxNuIt7WSiLfixdtIxGN48Tb8RBzfi6NeAt9LJJ99D/USqB9PPXv4\nrY04u1ZT2PghYxObGdoawW8cz87m8VT457NCrqcpNAwvkPVxEcHU4whOYxCUYMAnFFTCQcgNC4Gg\nEEh2eeO6pF57B8xzXXCO4Fim4wo4TnLkuiOpR3IsuOMAjkOkMMqos87BCdgZ1ubYHDb0VTUhIjcD\nC0kO2XxMVVeJyN1Auaou+ITVzwbuFpEE4AE3qWpddxQ+ECT8BN9+8Vamvg7RwYO57Nbv4vTGXmMn\n1PfZvPYddq5chN9cxwH9GPtfa7tphUQMJ74XJ96Em2gmmGgi6LUQ9psJawtZ2kI4EUM8F993UQ2A\n7yJ+kKA6OH4Qz3cJaABfXXx18XyX5AhpF9WPuzl8Pu7uiHsRdsWLqZAzeTU4glgodYnoLAgkWsiT\nBsbl1lM4Ik4wL4qTlZV8ZGchkSwkHDpohEkoEiCcHSASDRLODhCOBgmFXeRI0tuYfqBL4/RV9Xng\n+Q7z7jxE23Pbvf4z8OdjqG9A++nSe8l/cStRP4crv/X9Xh2lo77PlnXvs+O9lwhueZ2xje8SqC8i\n3nAKrZqbug7Jxyeu6AFnMibPmvSIEGM4cSIkJPnwJIznpB6prpCjtv9Emg4C4ATj5Hj1DMtuYvAw\njyGlQxl2ynjyJozEcZyj/0xjBjg7IzdN/rLuL1T814tMrs/joltupWhcSY9+nvo+letXUvXuywS2\nvMG4Pe8wqBnqa8rYGDuN8tA/EA/lQXbyzMbkSh/v7X989uHH8xz1CGgclwQB8Qg5PkFXCQTaCAbb\nCIZdgmGHQNDFCTi4AQdxHRzXwQ06OAEXx3Vwgi5uMDlEMDlPkNQycQRxk+uLm5wXyA5TOHksbtD+\n+RpzpOynJg1W7FrBf/7pp5y5eRDTL5rH8Wee02Of1VBfw5pnf8G4Db9nRKKeWO0JrNs7iw+cS9mb\nNQZCEHSaGBZpYMzkIBM/M5288SN7rB5jTHpZ6PeyHU07uOsvt/GplYWMnHIC5173pR75nO2b17L5\n+fs4oeoZhlRPZnHTzdRFJuG5ESTLo1CrOXF4DSVnT2LU2XaA0JhMYaHfi1oSLdz2/D9R9laEnPxB\nzL/1O91+vft1K16jcdHPOaHudaq2n8NfvJ+wN2s0wfBexubUMW7acMZfPJOsIXZ/XGMykYV+L3rg\nnX9n5Cv15MSjXH7nXd12Y3Lf83j/lT8QfPt+Sho3sq1qHk8GHiYWKiAar2Xm8Xs5+ctzCOVkHf7N\njDEDmoV+L6lpqeG9/36Ok2tzueCr/8Sw8ROP6n18z6OhbhcNNdvYW7edpspVjFj7G0bv9nlr1xUs\nzvouXnaYIf52Tj53EJOu+Jx13Rhj9rPQ7yVPLP8VkyuijDj5RKaee/4ntt3bWM8Hz/4SGrYRaK0l\n3FZLNF5Pnr+bAm2kUHzyPIfmtjx2Nhbz1t4vUx09Ccn2GR3exYyrTmbUWbN7acuMMf2JhX4vqG2p\n5YMXX2RqIofzr/3HT2yrvs/aB/6evPUJ6mPDaPDH0cJJtJFHXHKIOznE3eSlAZDkePRApJnjB1cz\n44tnU1B6QW9skjGmn7LQ7wWPlz/KpI3ZjDplOkOLx39i2/Jn/n9qP5jK+pw5kOqCDyaaCWkLYSdO\ndsAjEmkkK9pMVl6Y6JAcJl12jl1Z0RjTJRb6Pay2pZbVCxcyORHl/Gu//Iltt1asJPLii6zP+WfG\nZ1dx5s2ziY4qsgtrGWO6jYV+D3t82aOUbshizMwZDBkz7pDt4rE2dj/2Nd5K3EoO1Zz308tttI0x\nptvZRUp6UG1LLR++uJCA73D+1Z+8l7/sP77F2i2XkghkM+dLUy3wjTE9wkK/Bz3+9iNM3JjF2Fmn\nMnjUmEO2W73kBQJLatmZM42Tx+9l5Jkn9GKVxphMYqHfQ+pa6/joxZdx1WHONYcesdNQX4Pz9L+x\nPHgdQ7wqTvvmpb1YpTEm01iffg/5j7ceZsKmLIrPOI3C4Z1fwEx9n48euZEP676EG4xz4e2fthOp\njDE9yvb0e0Bdax3rXngZB2HOJ/TlL3/uYRo/GEtjdjFnnBWhoHRUL1ZpjMlEFvo94PElDzN+SxYl\nnzqT/KHDO22zbcMasp//Ex9mX8K48Dam3jCnl6s0xmSiLoW+iMwVkbUiUiEit39CuytEREWkrN28\nO1LrrRWRC7uj6L6svrWeihf+BweHOZ/v/LLJiXiM2sduYknbTWTFdzPnrvm9XKUxJlMdtk9fRFzg\nfmAOUAksE5EFqrq6Q7tc4BZgabt5U0jeSP0EYCTwPyIySVW97tuEvuXxNx6iZEuECed8irwhQztt\ns+zJ77Jr0wW0RQu4+PJCwoPyerlKY0ym6sqe/kygQlU3qGoMeArobNf0B8C9QGu7efOBp1S1TVU3\nAhWp9xuQ9u3li+Mw5/Od9+V/+PbLhF/fQGXO6UwZXse4C8o6bWeMMT2hK6E/CtjabroyNW8/EZkO\njFHV54503YHk8cUPUlwZYeK5Z5MzaPBBy/c01OE8fSdvO/9AQXwHZ33nc2mo0hiTyboS+tLJvP13\nxxYRB/g58L+PdN1273GjiJSLSHl1dXUXSup76lvr2fDiIsR1mHPVwXv5XiLBuge+wPJd1wPChd84\n3a6pY4zpdV0J/Uqg/emko4GqdtO5wFTgVRHZBJwGLEgdzD3cugCo6iOqWqaqZUVFRUe2BX3EE4sf\nZGxlmNLZnyZaUHjAMvV9yh/8EvWrj6MuOomZ05UhJ5akqVJjTCbrSugvA0pFpEREQiQPzC7Yt1BV\nG1R1iKoWq2ox8BYwT1XLU+2uFpGwiJQApcDb3b4Vaba7dTcbX1gEAYc5Vx48YuetJ/8VLVc+jM6j\nOFLFtK9+Jg1VGmNMF0bvqGpCRG4GFgIu8JiqrhKRu4FyVV3wCeuuEpE/AKuBBPC1gThy56FFP2PM\ntjCT5p530H1vyxc8SHTxByyO/BPDqWLujz+P49jpEcaY9OjSZRhU9Xng+Q7z7jxE23M7TP8Q+OFR\n1tfn/XHVU9QtWMLQUPSgvfwPFv9fov/9Z15xvkWht4vP3neZ9eMbY9LKdjmPweKti3nt0UcYVh/h\noptuJSvn47tXbfhgKeGnf8zfvNuIJuq59AcXEsqLprFaY4yx0D9qH9Z9yJMP3EXx9mxOv/Y6ppx5\n7v5lO7ZW4P3667zS/C2Cfivz7/gU2SMOHsJpjDG9za6yeRR2NO3gJw/ewpQN2Rw/53xOn/f5/csa\n6mvY/e9/x5v1t0LA5ZKvTLYLqRlj+gwL/SO0J7aHf/31V5jyfogR00/iM//wT4gkT0doa21m08+v\n5L0dNxEL5XHRVUMZdupxaa7YGGM+ZqF/BOJenO/8/muULkmQN34sV932PRwnef173/N4/+fXsX7j\nlezNGsl554UZO2dGmis2xpgDWZ9+F6kqP3judka8VE1kcCHXffdeAqHQ/uVLH7qZratOozZ6PGec\nEue4z5+dxmqNMaZzFvpd9MDr/wf3L6sIh7O54a77Dhips+TJ71O/ZDDbc8qYPraOaTddlMZKjTHm\n0Cz0u+DPH/yBqideIOqFuO5f7yWv6ONLJi958vu0vFTHxpzZTCrYwRnfuSKNlRpjzCez0D+MN7a8\nzpIHHqFwb4jLvvmvDCuZsH/Zkie/R9PCBtZE5zMuvI3zfnR1Gis1xpjDswO5n2Bt3Vqe/sX3GVcb\n4dyvfJUJ007dv2zJb+6k4aXKDdQPAAASO0lEQVQY63MuoSRrG3N/cq1dXsEY0+dZ6B9CbUstv/jF\nN5hQGebkyy5lxuyL9y978/F/Yff/CBtzLmBiThVz7v2CBb4xpl+w0O9E3I9z9+M3M+FDl9FnzuS8\ndve6ffOxO6hdFGFLzlkcV7iD2T+0PXxjTP9hod+Je5+/ixGLG4iMG8EV/+uO/Sdfvfnot9n1t3y2\n5ZzGlKKdnPP9qy3wjTH9ioV+B0+/91tif15OXiTK9Xf8BDeQvCrmGw//MzsXF7E9p4wTR9Zw9p3X\npLlSY4w5crab2s47O5az9NH/ILc1yFXfupucwkEAvPHg/2b74pHJcfjj6jn7zqvSXKkxxhwd29NP\n2dG0g4cfuINJ1RHOvOGLjDl+Kur7vPnQP1P5ZjE1OSdQNrGBWd+8PN2lGmPMUbPQB1oTrXzviZuZ\ntDbE2DNmMWvu5wB444FvsnVpKXXRUmZN2UvZLZeluVJjjDk2XereEZG5IrJWRCpE5PZOlt8kIitF\n5F0ReV1EpqTmF4tIS2r+uyLyUHdvwLFSVf7t+e8w7o1msscM47KvfhsRYclvfsTWpaXUZ0/kjGkx\nym6Zl+5SjTHmmB12T19EXOB+YA5QCSwTkQWqurpds9+p6kOp9vOA+4C5qWXrVXVa95bdfZ5Y8RjO\nM6sIhaJ84fZ7CIRCvPPCk+x8SajLOY4zTm5h+lcvPvwbGWNMP9CVPf2ZQIWqblDVGPAUML99A1Vt\nbDcZBbT7Suw5b1S+wYrHf0tec5Arv/k98oYU8eHbL1P7uxVsy5nF1OE7LfCNMQNKV0J/FLC13XRl\nat4BRORrIrIeuBe4pd2iEhFZISJ/E5GzOvsAEblRRMpFpLy6uvoIyj96Wxu38utf3cnYndmc+YXr\nGXvCSWz56F1qH3ySj6KXMDa4hbPu/Pzh38gYY/qRroS+dDLvoD15Vb1fVScA3wb+JTV7OzBWVacD\ntwG/E5G8TtZ9RFXLVLWsqKio69UfpaZ4E3f9581MWRNh7KyZnHbJFdTs2MLOn32P90J/x+DEVi76\nqV1awRgz8HQl1SqBMe2mRwNVn9D+KeBSAFVtU9Xa1OvlwHpg0tGV2n3+7YXvUvpGnJxRw7n0a9+i\nac9uttxzE8u9G4nGaph/z2W44WC6yzTGmG7XldBfBpSKSImIhICrgQXtG4hIabvJi4F1qflFqQPB\niMh4oBTY0B2FHy3P9+C51QQDIa65/Ucgwpp7/o53Gr9EwG9j/nfOImvIQX+MGGPMgHDY0TuqmhCR\nm4GFgAs8pqqrRORuoFxVFwA3i8j5QByoB65PrX42cLeIJAAPuElV63piQ7pqZ+N2hjSEyDt3CnlD\nilj642v5aNuVJIJZzLthLAWlo9NZnjHG9KgunZylqs8Dz3eYd2e7118/xHp/Bv58LAV2t81VHwEw\naNgoljz8DSpXnc7e6EjOmx1m5Kemprk6Y4zpWRl3pLJq+0YAWrZ+SP0bBVTnnMipk5vtRubGmIyQ\ncaFfs7MSgEHLmtmUcy7HFe7g1G/MP8xaxhgzMGRc6O+u2QnAhuhVjHQqmf0DG4tvjMkcGRf6TdU1\nQIT81u1c8pOrcAJuuksyxphek3GhT10L4uYxuLCeYDSS7mqMMaZXZVzoh/co4uSSO6Ig3aUYY0yv\ny6jQb0m0EIqBOLkMPb4k3eUYY0yvy6jQ31qzEUeToT9qxpR0l2OMMb0us0J/WwUArkbIHjkkzdUY\nY0zvy6jQ37FjEwAhX+0KmsaYjJRRyVe7K3lx0Cy8NFdijDHpkVGh37BrByBk2e3gjTEZKqNCP75z\nNzg5ZOV2dl8YY4wZ+DIq9IMNHiK55AzPT3cpxhiTFhkT+qpKqDU5XLPouHHpLscYY9IiY0K/vqUO\nN5E8G3fUdBujb4zJTBkT+pt3ViCAI9nkFg9PdznGGJMWXQp9EZkrImtFpEJEbu9k+U0islJE3hWR\n10VkSrtld6TWWysiF3Zn8Ueismo9ACFfbIy+MSZjHTb9Ujc2vx/4DDAFuKZ9qKf8TlVPVNVpwL3A\nfal1p5C8kfoJwFzggX03Su9tu3ZsASCiNkbfGJO5urLLOxOoUNUNqhoDngIOuNWUqja2m4wCmno9\nH3hKVdtUdSNQkXq/Xle7I3nHrKyA7eUbYzJXVxJwFLC13XRlat4BRORrIrKe5J7+LUe47o0iUi4i\n5dXV1V2t/Yg0bd8FBMjOsTOzjDGZqyuh39mZTHrQDNX7VXUC8G3gX45w3UdUtUxVy4qKirpQ0pFz\na9oQJ5ecobk98v7GGNMfdCX0K4Ex7aZHA1Wf0P4p4NKjXLfHBJtSY/RLxxy+sTHGDFBdCf1lQKmI\nlIhIiOSB2QXtG4hIabvJi4F1qdcLgKtFJCwiJUAp8Paxl31k4n6cQGqM/sjpk3v7440xps84bAe3\nqiZE5GZgIeACj6nqKhG5GyhX1QXAzSJyPhAH6oHrU+uuEpE/AKuBBPA11d4fPrOjcTuO7yMSpWDi\n6N7+eGOM6TO6dFRTVZ8Hnu8w7852r7/+Cev+EPjh0RbYHTZXfQRA0HdxAmkZMWqMMX1CRoxfrNq+\nAYDwwceQjTEmo2RE6O+q2gxAxHbyjTEZLiNCv25L8lSBnGg4zZUYY0x6ZUTos2MvSITcIhujb4zJ\nbBkR+sG9PuLkMshG7hhjMlxGhH6gDcTJY9Q0G6NvjMlsAz70m+JNiJccoz9o0th0l2OMMWk14EN/\na+0mBJ+gBnHDwXSXY4wxaTXgQ3/LtuQVIYI2RN8YYwZ+6FduTYZ+2Onsgp/GGJNZBnzo79qcPBs3\nmmVdO8YYM+BDP7G1DhAKigrSXYoxxqTdgA99d3ccJMqgCSPTXYoxxqTdwA/91uQY/ZEn2xh9Y4wZ\n0KHvq4/jKeLkMGRycbrLMcaYtBvQoV/bUov4CQKECGRH0l2OMcak3YAO/c07KgAloDZc0xhjoIuh\nLyJzRWStiFSIyO2dLL9NRFaLyPsi8lcRGddumSci76YeCzqu25O2bFkLQFgs9I0xBrpwu0QRcYH7\ngTlAJbBMRBao6up2zVYAZaraLCJfBe4FPp9a1qKq07q57i7Zun4VAFkRG6NvjDHQtT39mUCFqm5Q\n1RjwFDC/fQNVfUVVm1OTbwF94hrGzRu3A5A/yMboG2MMdC30RwFb201XpuYdypeAF9pNR0SkXETe\nEpFLO1tBRG5MtSmvrq7uQkldVNsGuBSNH9N972mMMf3YYbt3gM46xDu9fJmIXAeUAee0mz1WVatE\nZDywSERWqur6A95M9RHgEYCysrJuuzSa26ypMfrHd9dbGmNMv9aVPf1KoP2u8migqmMjETkf+C4w\nT1Xb9s1X1arU8wbgVWD6MdR7RNw4iJNL0QklvfWRxhjTp3Ul9JcBpSJSIiIh4GrggFE4IjIdeJhk\n4O9qN79QRMKp10OAM4H2B4B7TMyLge/hECGUF+2NjzTGmD7vsN07qpoQkZuBhYALPKaqq0TkbqBc\nVRcAPwFygD9KcnjkFlWdB0wGHhYRn+QvmHs6jPrpMVUN20DjBHRAn4pgjDFHpCt9+qjq88DzHebd\n2e71+YdY703gxGMp8GhtrEz+bgmJhb4xxuwzYBOx4sN3AQiH3DRXYowxfceADf2GjzYBkF+Ql95C\njDGmDxmwoe/t3APA0OKxaa7EGGP6jgEb+s5eHyTC6GlT0l2KMcb0GQM39GPJMfpDT5yQ7lKMMabP\nGJChr6qIpzhkER5kffrGGLPPgAz9xlgjqnEC2NU1jTGmvQEZ+lt2rQcSBGyMvjHGHGBApuLq1UsB\nCAVsjL4xxrQ3IEN/5+o1AOTkZqe5EmOM6VsGZOjHKusAKBrTJ+7lYowxfcaADH0aPEAoPuWkdFdi\njDF9yoAMfacVkCjDTj4u3aUYY0yf0qWrbPY34oFIlOyhhekuxRhj+pQBt6fv+R6qCVxC6S7FGGP6\nnAEX+jv37EC1jYDYcE1jjOlowIX+yg+XAD5B10LfGGM66lLoi8hcEVkrIhUicnsny28TkdUi8r6I\n/FVExrVbdr2IrEs9ru/O4juz9b0VAESikZ7+KGOM6XcOG/oi4gL3A58BpgDXiEjH6xWvAMpU9STg\nT8C9qXUHAXcBs4CZwF0i0qNHV5s27gBg8PDhPfkxxhjTL3VlT38mUKGqG1Q1BjwFzG/fQFVfUdXm\n1ORbwL6zoi4EXlbVOlWtB14G5nZP6Z3z6loBKJ42tSc/xhhj+qWuhP4oYGu76crUvEP5EvDCkawr\nIjeKSLmIlFdXV3ehpEOTFgCXMTNOPqb3McaYgagroS+dzNNOG4pcB5QBPzmSdVX1EVUtU9WyoqKi\nLpR0aJIQxMkhOmrIMb2PMcYMRF0J/UpgTLvp0UBVx0Yicj7wXWCeqrYdybrdSX0flzCOM+AGJhlj\nzDHrSjIuA0pFpEREQsDVwIL2DURkOvAwycDf1W7RQuACESlMHcC9IDWvRzTHmlCN44rdPMUYYzpz\n2MswqGpCRG4mGdYu8JiqrhKRu4FyVV1AsjsnB/ijiABsUdV5qlonIj8g+YsD4G5VreuRLQFWb1qB\n0kLAsTH6xhjTmS5de0dVnwee7zDvznavz/+EdR8DHjvaAo/ER+VvABCKhHvj44wxpt8ZUB3fuz/a\nDEDeELvQmjHGdGZAhX5iVxMA4yZ3PHfMGGMMDLDQ1+bkaNDxp81IcyXGGNM3DajQJ+4AYQpLxx22\nqTHGZKKBFfq+4kiWjdE3xphDGDDp6HseviZwxW6eYowxhzJgQn/z9o/wtRXXxugbY8whDZjQD7Y4\nQBs50fx0l2KMMX3WgAn9cCJATngsE048Kd2lGGNMn9WlM3L7g6LJE/jKbx5IdxnGGNOnDZg9fWOM\nMYdnoW+MMRnEQt8YYzKIhb4xxmQQC31jjMkgFvrGGJNBLPSNMSaDWOgbY0wGEVVNdw0HEJFqYHMn\ni4YANb1cTk8YKNsBti190UDZDrBtOVLjVLXocI36XOgfioiUq2pZuus4VgNlO8C2pS8aKNsBti09\nxbp3jDEmg1joG2NMBulPof9IugvoJgNlO8C2pS8aKNsBti09ot/06RtjjDl2/WlP3xhjzDHq86Ev\nInNFZK2IVIjI7emu51iIyCYRWSki74pIebrrORIi8piI7BKRD9rNGyQiL4vIutRzYTpr7IpDbMf3\nRGRb6nt5V0QuSmeNXSUiY0TkFRFZIyKrROTrqfn98Xs51Lb0q+9GRCIi8raIvJfaju+n5peIyNLU\nd/K0SPpu5t2nu3dExAU+AuYAlcAy4BpVXZ3Wwo6SiGwCylS13409FpGzgb3Ab1R1amrevUCdqt6T\n+oVcqKrfTmedh3OI7fgesFdVf5rO2o6UiIwARqjqOyKSCywHLgVuoP99L4falqvoR9+NiAgQVdW9\nIhIEXge+DtwG/EVVnxKRh4D3VPXBdNTY1/f0ZwIVqrpBVWPAU8D8NNeUkVT1NaCuw+z5wBOp10+Q\n/CHt0w6xHf2Sqm5X1XdSr/cAa4BR9M/v5VDb0q9o0t7UZDD1UGA28KfU/LR+J3099EcBW9tNV9IP\n/yG0o8BLIrJcRG5MdzHdYJiqbofkDy0wNM31HIubReT9VPdPn+8O6UhEioHpwFL6+ffSYVugn303\nIuKKyLvALuBlYD2wW1UTqSZpzbG+HvrSyby+2x91eGeq6inAZ4CvpboaTPo9CEwApgHbgZ+lt5wj\nIyI5wJ+Bb6hqY7rrORadbEu/+25U1VPVacBokr0Vkztr1rtVfayvh34lMKbd9GigKk21HDNVrUo9\n7wL+i+Q/iP5sZ6ovdl+f7K4013NUVHVn6gfVBx6lH30vqX7jPwO/VdW/pGb3y++ls23pz9+Nqu4G\nXgVOAwpEJJBalNYc6+uhvwwoTR35DgFXAwvSXNNREZFo6gAVIhIFLgA++OS1+rwFwPWp19cD/zeN\ntRy1fQGZchn95HtJHTT8NbBGVe9rt6jffS+H2pb+9t2ISJGIFKReZwHnkzw+8QpwRapZWr+TPj16\nByA1ROv/AC7wmKr+MM0lHRURGU9y7x4gAPyuP22LiPweOJfk1QJ3AncBzwB/AMYCW4ArVbVPHyQ9\nxHacS7L7QIFNwFf29Yn3ZSLyKWAxsBLwU7O/Q7IvvL99L4falmvoR9+NiJxE8kCtS3Kn+g+qenfq\n5/8pYBCwArhOVdvSUmNfD31jjDHdp6937xhjjOlGFvrGGJNBLPSNMSaDWOgbY0wGsdA3xpgMYqFv\njDEZxELfGGMyiIW+McZkkP8HqtVyN6L2gB4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1285c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nx,R2_train_rcv)\n",
    "plt.plot(nx,R2_train_LR)\n",
    "plt.plot(nx,R2_train_RF)\n",
    "plt.plot(nx,R2_holdout_rcv)\n",
    "plt.plot(nx,R2_holdout_LR)\n",
    "plt.plot(nx,R2_holdout_RF)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 256) (1000000,) (256,)\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Create array of feature indices ordered by importance\n",
    "print(X_train.shape, Y_train.shape, RF.feature_importances_.shape)\n",
    "imp = np.flip(np.argsort(RF.feature_importances_),0)\n",
    "print(imp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.05105743288323 0.4031551393097869\n",
      "1.6999799208247506 0.30861511660139984\n",
      "-0.35107751205847926\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+YAAAEyCAYAAAB3UMo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEb0/wE/G58137KCewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1253d0d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h0 = np.logical_not(X_train[:,imp[0]])\n",
    "h1 = np.logical_and(X_train[:,imp[0]],1)\n",
    "print(Y_train[h0].mean(), Y_train[h0].std())\n",
    "print(Y_train[h1].mean(), Y_train[h1].std())\n",
    "print(Y_train[h1].mean()-Y_train[h0].mean())\n",
    "plt.figure(figsize=(17,5))\n",
    "plt.hist(Y_train[h0],1000)\n",
    "plt.hist(Y_train[h1],1000)\n",
    "#plt.yscale('symlog')\n",
    "plt.legend(['0','1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Feature  24 : train  385474 38.5474 %, test  318217 38.6077915145 %\n",
      "2 ) Feature  118 : train  292201 29.2201 %, test  239947 29.1116557272 %\n",
      "3 ) Feature  67 : train  621252 62.1252 %, test  512035 62.1228297927 %\n",
      "4 ) Feature  224 : train  334526 33.4526 %, test  275771 33.4580153598 %\n",
      "5 ) Feature  250 : train  218787 21.8787 %, test  180673 21.9202164444 %\n",
      "6 ) Feature  207 : train  40061 4.0061 %, test  32517 3.94513667302 %\n",
      "7 ) Feature  71 : train  770810 77.081 %, test  634196 76.9440568773 %\n",
      "8 ) Feature  89 : train  800323 80.0323 %, test  659667 80.0343350764 %\n",
      "9 ) Feature  172 : train  436831 43.6831 %, test  359844 43.6582022008 %\n",
      "10 ) Feature  125 : train  933919 93.3919 %, test  769664 93.3797605037 %\n",
      "11 ) Feature  122 : train  301730 30.173 %, test  248277 30.1222959611 %\n",
      "12 ) Feature  101 : train  290511 29.0511 %, test  239588 29.0680999236 %\n",
      "13 ) Feature  186 : train  155527 15.5527 %, test  128523 15.5930990136 %\n",
      "14 ) Feature  217 : train  290511 29.0511 %, test  239588 29.0680999236 %\n",
      "15 ) Feature  86 : train  139328 13.9328 %, test  114735 13.9202649746 %\n",
      "16 ) Feature  247 : train  917407 91.7407 %, test  756183 91.7441733497 %\n",
      "17 ) Feature  5 : train  477650 47.765 %, test  393188 47.7036749451 %\n",
      "18 ) Feature  251 : train  37309 3.7309 %, test  30919 3.75125875059 %\n",
      "19 ) Feature  242 : train  477650 47.765 %, test  393188 47.7036749451 %\n",
      "20 ) Feature  195 : train  15337 1.5337 %, test  12412 1.50589034614 %\n",
      "21 ) Feature  0 : train  642013 64.2013 %, test  528668 64.1408344758 %\n",
      "22 ) Feature  36 : train  358453 35.8453 %, test  294903 35.7792121131 %\n",
      "23 ) Feature  43 : train  5031 0.5031 %, test  4084 0.495492762942 %\n",
      "24 ) Feature  175 : train  34947 3.4947 %, test  28771 3.49065188115 %\n",
      "25 ) Feature  68 : train  355291 35.5291 %, test  292360 35.4706817272 %\n",
      "26 ) Feature  225 : train  996001 99.6001 %, test  820880 99.59356005 %\n",
      "27 ) Feature  131 : train  975816 97.5816 %, test  804131 97.5614816253 %\n",
      "28 ) Feature  6 : train  975860 97.586 %, test  804170 97.5662133142 %\n",
      "29 ) Feature  199 : train  791 0.0791 %, test  618 0.0749790713757 %\n",
      "30 ) Feature  198 : train  791 0.0791 %, test  618 0.0749790713757 %\n",
      "31 ) Feature  4 : train  999987 99.9987 %, test  824215 99.9981801196 %\n",
      "32 ) Feature  80 : train  0 0.0 %, test  0 0.0 %\n",
      "33 ) Feature  77 : train  0 0.0 %, test  0 0.0 %\n",
      "34 ) Feature  81 : train  0 0.0 %, test  0 0.0 %\n",
      "35 ) Feature  82 : train  0 0.0 %, test  0 0.0 %\n",
      "36 ) Feature  79 : train  0 0.0 %, test  0 0.0 %\n",
      "37 ) Feature  78 : train  0 0.0 %, test  0 0.0 %\n",
      "38 ) Feature  113 : train  0 0.0 %, test  0 0.0 %\n",
      "39 ) Feature  107 : train  0 0.0 %, test  0 0.0 %\n",
      "40 ) Feature  76 : train  0 0.0 %, test  0 0.0 %\n",
      "41 ) Feature  75 : train  0 0.0 %, test  0 0.0 %\n",
      "42 ) Feature  74 : train  0 0.0 %, test  0 0.0 %\n",
      "43 ) Feature  83 : train  0 0.0 %, test  0 0.0 %\n",
      "44 ) Feature  72 : train  0 0.0 %, test  0 0.0 %\n",
      "45 ) Feature  114 : train  0 0.0 %, test  0 0.0 %\n",
      "46 ) Feature  70 : train  0 0.0 %, test  0 0.0 %\n",
      "47 ) Feature  69 : train  0 0.0 %, test  0 0.0 %\n",
      "48 ) Feature  115 : train  0 0.0 %, test  0 0.0 %\n",
      "49 ) Feature  116 : train  0 0.0 %, test  0 0.0 %\n",
      "50 ) Feature  73 : train  0 0.0 %, test  0 0.0 %\n",
      "51 ) Feature  85 : train  0 0.0 %, test  0 0.0 %\n",
      "52 ) Feature  84 : train  0 0.0 %, test  0 0.0 %\n",
      "53 ) Feature  106 : train  0 0.0 %, test  0 0.0 %\n",
      "54 ) Feature  105 : train  0 0.0 %, test  0 0.0 %\n",
      "55 ) Feature  104 : train  0 0.0 %, test  0 0.0 %\n",
      "56 ) Feature  103 : train  0 0.0 %, test  0 0.0 %\n",
      "57 ) Feature  102 : train  0 0.0 %, test  0 0.0 %\n",
      "58 ) Feature  108 : train  0 0.0 %, test  0 0.0 %\n",
      "59 ) Feature  100 : train  0 0.0 %, test  0 0.0 %\n",
      "60 ) Feature  109 : train  0 0.0 %, test  0 0.0 %\n",
      "61 ) Feature  99 : train  0 0.0 %, test  0 0.0 %\n",
      "62 ) Feature  98 : train  0 0.0 %, test  0 0.0 %\n",
      "63 ) Feature  97 : train  0 0.0 %, test  0 0.0 %\n",
      "64 ) Feature  96 : train  0 0.0 %, test  0 0.0 %\n",
      "65 ) Feature  95 : train  0 0.0 %, test  0 0.0 %\n",
      "66 ) Feature  94 : train  0 0.0 %, test  0 0.0 %\n",
      "67 ) Feature  65 : train  0 0.0 %, test  0 0.0 %\n",
      "68 ) Feature  93 : train  0 0.0 %, test  0 0.0 %\n",
      "69 ) Feature  92 : train  0 0.0 %, test  0 0.0 %\n",
      "70 ) Feature  91 : train  0 0.0 %, test  0 0.0 %\n",
      "71 ) Feature  90 : train  0 0.0 %, test  0 0.0 %\n",
      "72 ) Feature  110 : train  0 0.0 %, test  0 0.0 %\n",
      "73 ) Feature  88 : train  0 0.0 %, test  0 0.0 %\n",
      "74 ) Feature  87 : train  0 0.0 %, test  0 0.0 %\n",
      "75 ) Feature  111 : train  0 0.0 %, test  0 0.0 %\n",
      "76 ) Feature  112 : train  0 0.0 %, test  0 0.0 %\n",
      "77 ) Feature  66 : train  0 0.0 %, test  0 0.0 %\n",
      "78 ) Feature  56 : train  0 0.0 %, test  0 0.0 %\n",
      "79 ) Feature  64 : train  0 0.0 %, test  0 0.0 %\n",
      "80 ) Feature  17 : train  0 0.0 %, test  0 0.0 %\n",
      "81 ) Feature  30 : train  0 0.0 %, test  0 0.0 %\n",
      "82 ) Feature  29 : train  0 0.0 %, test  0 0.0 %\n",
      "83 ) Feature  28 : train  0 0.0 %, test  0 0.0 %\n",
      "84 ) Feature  27 : train  0 0.0 %, test  0 0.0 %\n",
      "85 ) Feature  26 : train  0 0.0 %, test  0 0.0 %\n",
      "86 ) Feature  25 : train  0 0.0 %, test  0 0.0 %\n",
      "87 ) Feature  23 : train  0 0.0 %, test  0 0.0 %\n",
      "88 ) Feature  22 : train  0 0.0 %, test  0 0.0 %\n",
      "89 ) Feature  21 : train  0 0.0 %, test  0 0.0 %\n",
      "90 ) Feature  20 : train  0 0.0 %, test  0 0.0 %\n",
      "91 ) Feature  19 : train  0 0.0 %, test  0 0.0 %\n",
      "92 ) Feature  18 : train  0 0.0 %, test  0 0.0 %\n",
      "93 ) Feature  16 : train  0 0.0 %, test  0 0.0 %\n",
      "94 ) Feature  63 : train  0 0.0 %, test  0 0.0 %\n",
      "95 ) Feature  15 : train  0 0.0 %, test  0 0.0 %\n",
      "96 ) Feature  14 : train  0 0.0 %, test  0 0.0 %\n",
      "97 ) Feature  13 : train  0 0.0 %, test  0 0.0 %\n",
      "98 ) Feature  12 : train  0 0.0 %, test  0 0.0 %\n",
      "99 ) Feature  11 : train  0 0.0 %, test  0 0.0 %\n",
      "100 ) Feature  10 : train  0 0.0 %, test  0 0.0 %\n",
      "101 ) Feature  9 : train  0 0.0 %, test  0 0.0 %\n",
      "102 ) Feature  8 : train  0 0.0 %, test  0 0.0 %\n",
      "103 ) Feature  7 : train  0 0.0 %, test  0 0.0 %\n",
      "104 ) Feature  3 : train  0 0.0 %, test  0 0.0 %\n",
      "105 ) Feature  2 : train  0 0.0 %, test  0 0.0 %\n",
      "106 ) Feature  1 : train  0 0.0 %, test  0 0.0 %\n",
      "107 ) Feature  31 : train  0 0.0 %, test  0 0.0 %\n",
      "108 ) Feature  32 : train  0 0.0 %, test  0 0.0 %\n",
      "109 ) Feature  33 : train  0 0.0 %, test  0 0.0 %\n",
      "110 ) Feature  34 : train  0 0.0 %, test  0 0.0 %\n",
      "111 ) Feature  62 : train  0 0.0 %, test  0 0.0 %\n",
      "112 ) Feature  61 : train  0 0.0 %, test  0 0.0 %\n",
      "113 ) Feature  60 : train  0 0.0 %, test  0 0.0 %\n",
      "114 ) Feature  59 : train  0 0.0 %, test  0 0.0 %\n",
      "115 ) Feature  58 : train  0 0.0 %, test  0 0.0 %\n",
      "116 ) Feature  57 : train  0 0.0 %, test  0 0.0 %\n",
      "117 ) Feature  55 : train  0 0.0 %, test  0 0.0 %\n",
      "118 ) Feature  54 : train  0 0.0 %, test  0 0.0 %\n",
      "119 ) Feature  53 : train  0 0.0 %, test  0 0.0 %\n",
      "120 ) Feature  52 : train  0 0.0 %, test  0 0.0 %\n",
      "121 ) Feature  51 : train  0 0.0 %, test  0 0.0 %\n",
      "122 ) Feature  50 : train  0 0.0 %, test  0 0.0 %\n",
      "123 ) Feature  49 : train  0 0.0 %, test  0 0.0 %\n",
      "124 ) Feature  48 : train  0 0.0 %, test  0 0.0 %\n",
      "125 ) Feature  47 : train  0 0.0 %, test  0 0.0 %\n",
      "126 ) Feature  46 : train  0 0.0 %, test  0 0.0 %\n",
      "127 ) Feature  45 : train  0 0.0 %, test  0 0.0 %\n",
      "128 ) Feature  44 : train  0 0.0 %, test  0 0.0 %\n",
      "129 ) Feature  42 : train  0 0.0 %, test  0 0.0 %\n",
      "130 ) Feature  41 : train  0 0.0 %, test  0 0.0 %\n",
      "131 ) Feature  40 : train  0 0.0 %, test  0 0.0 %\n",
      "132 ) Feature  39 : train  0 0.0 %, test  0 0.0 %\n",
      "133 ) Feature  38 : train  0 0.0 %, test  0 0.0 %\n",
      "134 ) Feature  37 : train  0 0.0 %, test  0 0.0 %\n",
      "135 ) Feature  35 : train  0 0.0 %, test  0 0.0 %\n",
      "136 ) Feature  117 : train  0 0.0 %, test  0 0.0 %\n",
      "137 ) Feature  255 : train  0 0.0 %, test  0 0.0 %\n",
      "138 ) Feature  119 : train  0 0.0 %, test  0 0.0 %\n",
      "139 ) Feature  203 : train  0 0.0 %, test  0 0.0 %\n",
      "140 ) Feature  216 : train  0 0.0 %, test  0 0.0 %\n",
      "141 ) Feature  215 : train  0 0.0 %, test  0 0.0 %\n",
      "142 ) Feature  214 : train  0 0.0 %, test  0 0.0 %\n",
      "143 ) Feature  213 : train  0 0.0 %, test  0 0.0 %\n",
      "144 ) Feature  212 : train  0 0.0 %, test  0 0.0 %\n",
      "145 ) Feature  211 : train  0 0.0 %, test  0 0.0 %\n",
      "146 ) Feature  210 : train  0 0.0 %, test  0 0.0 %\n",
      "147 ) Feature  209 : train  0 0.0 %, test  0 0.0 %\n",
      "148 ) Feature  208 : train  0 0.0 %, test  0 0.0 %\n",
      "149 ) Feature  206 : train  0 0.0 %, test  0 0.0 %\n",
      "150 ) Feature  205 : train  0 0.0 %, test  0 0.0 %\n",
      "151 ) Feature  204 : train  0 0.0 %, test  0 0.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 ) Feature  202 : train  0 0.0 %, test  0 0.0 %\n",
      "153 ) Feature  219 : train  0 0.0 %, test  0 0.0 %\n",
      "154 ) Feature  201 : train  0 0.0 %, test  0 0.0 %\n",
      "155 ) Feature  200 : train  0 0.0 %, test  0 0.0 %\n",
      "156 ) Feature  197 : train  0 0.0 %, test  0 0.0 %\n",
      "157 ) Feature  196 : train  0 0.0 %, test  0 0.0 %\n",
      "158 ) Feature  194 : train  0 0.0 %, test  0 0.0 %\n",
      "159 ) Feature  193 : train  0 0.0 %, test  0 0.0 %\n",
      "160 ) Feature  192 : train  0 0.0 %, test  0 0.0 %\n",
      "161 ) Feature  191 : train  0 0.0 %, test  0 0.0 %\n",
      "162 ) Feature  190 : train  0 0.0 %, test  0 0.0 %\n",
      "163 ) Feature  189 : train  0 0.0 %, test  0 0.0 %\n",
      "164 ) Feature  188 : train  0 0.0 %, test  0 0.0 %\n",
      "165 ) Feature  187 : train  0 0.0 %, test  0 0.0 %\n",
      "166 ) Feature  218 : train  0 0.0 %, test  0 0.0 %\n",
      "167 ) Feature  220 : train  0 0.0 %, test  0 0.0 %\n",
      "168 ) Feature  184 : train  0 0.0 %, test  0 0.0 %\n",
      "169 ) Feature  237 : train  0 0.0 %, test  0 0.0 %\n",
      "170 ) Feature  253 : train  0 0.0 %, test  0 0.0 %\n",
      "171 ) Feature  252 : train  0 0.0 %, test  0 0.0 %\n",
      "172 ) Feature  249 : train  0 0.0 %, test  0 0.0 %\n",
      "173 ) Feature  248 : train  0 0.0 %, test  0 0.0 %\n",
      "174 ) Feature  246 : train  0 0.0 %, test  0 0.0 %\n",
      "175 ) Feature  245 : train  0 0.0 %, test  0 0.0 %\n",
      "176 ) Feature  244 : train  0 0.0 %, test  0 0.0 %\n",
      "177 ) Feature  243 : train  0 0.0 %, test  0 0.0 %\n",
      "178 ) Feature  241 : train  0 0.0 %, test  0 0.0 %\n",
      "179 ) Feature  240 : train  0 0.0 %, test  0 0.0 %\n",
      "180 ) Feature  239 : train  0 0.0 %, test  0 0.0 %\n",
      "181 ) Feature  238 : train  0 0.0 %, test  0 0.0 %\n",
      "182 ) Feature  236 : train  0 0.0 %, test  0 0.0 %\n",
      "183 ) Feature  221 : train  0 0.0 %, test  0 0.0 %\n",
      "184 ) Feature  235 : train  0 0.0 %, test  0 0.0 %\n",
      "185 ) Feature  234 : train  0 0.0 %, test  0 0.0 %\n",
      "186 ) Feature  233 : train  0 0.0 %, test  0 0.0 %\n",
      "187 ) Feature  232 : train  0 0.0 %, test  0 0.0 %\n",
      "188 ) Feature  231 : train  0 0.0 %, test  0 0.0 %\n",
      "189 ) Feature  230 : train  0 0.0 %, test  0 0.0 %\n",
      "190 ) Feature  229 : train  0 0.0 %, test  0 0.0 %\n",
      "191 ) Feature  228 : train  0 0.0 %, test  0 0.0 %\n",
      "192 ) Feature  227 : train  0 0.0 %, test  0 0.0 %\n",
      "193 ) Feature  226 : train  0 0.0 %, test  0 0.0 %\n",
      "194 ) Feature  223 : train  0 0.0 %, test  0 0.0 %\n",
      "195 ) Feature  222 : train  0 0.0 %, test  0 0.0 %\n",
      "196 ) Feature  185 : train  0 0.0 %, test  0 0.0 %\n",
      "197 ) Feature  183 : train  0 0.0 %, test  0 0.0 %\n",
      "198 ) Feature  120 : train  0 0.0 %, test  0 0.0 %\n",
      "199 ) Feature  137 : train  0 0.0 %, test  0 0.0 %\n",
      "200 ) Feature  149 : train  0 0.0 %, test  0 0.0 %\n",
      "201 ) Feature  148 : train  0 0.0 %, test  0 0.0 %\n",
      "202 ) Feature  147 : train  0 0.0 %, test  0 0.0 %\n",
      "203 ) Feature  146 : train  0 0.0 %, test  0 0.0 %\n",
      "204 ) Feature  145 : train  0 0.0 %, test  0 0.0 %\n",
      "205 ) Feature  144 : train  0 0.0 %, test  0 0.0 %\n",
      "206 ) Feature  143 : train  0 0.0 %, test  0 0.0 %\n",
      "207 ) Feature  142 : train  0 0.0 %, test  0 0.0 %\n",
      "208 ) Feature  141 : train  0 0.0 %, test  0 0.0 %\n",
      "209 ) Feature  140 : train  0 0.0 %, test  0 0.0 %\n",
      "210 ) Feature  139 : train  0 0.0 %, test  0 0.0 %\n",
      "211 ) Feature  138 : train  0 0.0 %, test  0 0.0 %\n",
      "212 ) Feature  136 : train  0 0.0 %, test  0 0.0 %\n",
      "213 ) Feature  151 : train  0 0.0 %, test  0 0.0 %\n",
      "214 ) Feature  135 : train  0 0.0 %, test  0 0.0 %\n",
      "215 ) Feature  134 : train  0 0.0 %, test  0 0.0 %\n",
      "216 ) Feature  133 : train  0 0.0 %, test  0 0.0 %\n",
      "217 ) Feature  132 : train  0 0.0 %, test  0 0.0 %\n",
      "218 ) Feature  130 : train  0 0.0 %, test  0 0.0 %\n",
      "219 ) Feature  129 : train  0 0.0 %, test  0 0.0 %\n",
      "220 ) Feature  128 : train  0 0.0 %, test  0 0.0 %\n",
      "221 ) Feature  254 : train  0 0.0 %, test  0 0.0 %\n",
      "222 ) Feature  126 : train  0 0.0 %, test  0 0.0 %\n",
      "223 ) Feature  124 : train  0 0.0 %, test  0 0.0 %\n",
      "224 ) Feature  123 : train  0 0.0 %, test  0 0.0 %\n",
      "225 ) Feature  121 : train  0 0.0 %, test  0 0.0 %\n",
      "226 ) Feature  150 : train  0 0.0 %, test  0 0.0 %\n",
      "227 ) Feature  152 : train  0 0.0 %, test  0 0.0 %\n",
      "228 ) Feature  182 : train  0 0.0 %, test  0 0.0 %\n",
      "229 ) Feature  167 : train  0 0.0 %, test  0 0.0 %\n",
      "230 ) Feature  181 : train  0 0.0 %, test  0 0.0 %\n",
      "231 ) Feature  180 : train  0 0.0 %, test  0 0.0 %\n",
      "232 ) Feature  179 : train  0 0.0 %, test  0 0.0 %\n",
      "233 ) Feature  178 : train  0 0.0 %, test  0 0.0 %\n",
      "234 ) Feature  177 : train  0 0.0 %, test  0 0.0 %\n",
      "235 ) Feature  176 : train  0 0.0 %, test  0 0.0 %\n",
      "236 ) Feature  174 : train  0 0.0 %, test  0 0.0 %\n",
      "237 ) Feature  173 : train  0 0.0 %, test  0 0.0 %\n",
      "238 ) Feature  171 : train  0 0.0 %, test  0 0.0 %\n",
      "239 ) Feature  170 : train  0 0.0 %, test  0 0.0 %\n",
      "240 ) Feature  169 : train  0 0.0 %, test  0 0.0 %\n",
      "241 ) Feature  168 : train  0 0.0 %, test  0 0.0 %\n",
      "242 ) Feature  166 : train  0 0.0 %, test  0 0.0 %\n",
      "243 ) Feature  153 : train  0 0.0 %, test  0 0.0 %\n",
      "244 ) Feature  165 : train  0 0.0 %, test  0 0.0 %\n",
      "245 ) Feature  164 : train  0 0.0 %, test  0 0.0 %\n",
      "246 ) Feature  163 : train  0 0.0 %, test  0 0.0 %\n",
      "247 ) Feature  162 : train  0 0.0 %, test  0 0.0 %\n",
      "248 ) Feature  161 : train  0 0.0 %, test  0 0.0 %\n",
      "249 ) Feature  160 : train  0 0.0 %, test  0 0.0 %\n",
      "250 ) Feature  159 : train  0 0.0 %, test  0 0.0 %\n",
      "251 ) Feature  158 : train  0 0.0 %, test  0 0.0 %\n",
      "252 ) Feature  157 : train  0 0.0 %, test  0 0.0 %\n",
      "253 ) Feature  156 : train  0 0.0 %, test  0 0.0 %\n",
      "254 ) Feature  155 : train  0 0.0 %, test  0 0.0 %\n",
      "255 ) Feature  154 : train  0 0.0 %, test  0 0.0 %\n",
      "256 ) Feature  127 : train  0 0.0 %, test  0 0.0 %\n"
     ]
    }
   ],
   "source": [
    "n_train = X_train.shape[0]\n",
    "n_test  = X_test.shape[0]\n",
    "for i in range(256):\n",
    "    h1 = (X_train[:,imp[i]] == 1)\n",
    "    t1 = (X_test[:,imp[i]] == 1)\n",
    "    print(i+1, ') Feature ', imp[i],': train ',sum(h1), 100*sum(h1)/n_train,'%, test ',sum(t1), 100*sum(t1)/n_test,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: Mean of empty slice.\n",
      "  \"\"\"\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Flip X such that Y(1) > Y(0) for all features\n",
    "n2 = X_train2.shape[1]\n",
    "X_trainb = (X_train == 1)\n",
    "for i in range(n2): \n",
    "    b0 = X_trainb[:,imp[i]]\n",
    "    if Y_train[np.logical_not(b0)].mean() > Y_train[b0].mean():\n",
    "        X_trainb[b0] = np.logical_not(X_trainb[b0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: Mean of empty slice.\n",
      "  after removing the cwd from sys.path.\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "'mean1'\n",
    "Y_train2_mean1 = np.array([[Y_train[np.logical_and(X_trainb[:,imp[i]],X_trainb[:,imp[j]])].mean() for i in range(n2)] for j in range(n2)])\n",
    "'mean0'\n",
    "Y_train2_mean0 = np.array([[Y_train[np.logical_and(np.logical_not(X_trainb[:,imp[i]]),np.logical_not(X_trainb[:,imp[j]]))].mean() for i in range(n2)] for j in range(n2)])\n",
    "'std1'\n",
    "Y_train2_std1  = np.array([[Y_train[np.logical_and(X_trainb[:,imp[i]],X_trainb[:,imp[j]])].std() for i in range(n2)] for j in range(n2)])\n",
    "'std0'\n",
    "Y_train2_std0  = np.array([[Y_train[np.logical_and(np.logical_not(X_trainb[:,imp[i]]),np.logical_not(X_trainb[:,imp[j]]))].std() for i in range(n2)] for j in range(n2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.54915793715 1.37092391304\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAF4xJREFUeJzt3XuMnGd1BvDnzGXX9u76suvEXnxp\niBWgAXLTKkKFIgpN5CKkEAkqUIpSFGGoiAQqIEJ6IeWvUBUQqtpUpolIqxRICRGRiAImUAIqCqyd\n4DgxaRzIxfbGXt93197duZz+MeOycuc8M/7m5uh9ftJqd+fd9/ve75s5883OmfO+5u4QkfTk+j0A\nEekPBb9IohT8IolS8IskSsEvkigFv0iiFPwiiVLwiyRKwS+SqEI7nc1sK4CvAsgD+Fd3v5P9fX5k\nyAtr1zRsGxwohf1KlXzY5m6kjY0m7lcslMO2Mh0L2V2JPM8Wq3EbOT4Y2SHrR+QLFdpeqZDj6MZY\nWT+GbpN2DFvyM/Gxj68/1nxMDeyfbRwPzbx5bH3D23fu3HnE3S9qZRuZg9/M8gD+CcB1APYD+KWZ\nPeTuz4Q7W7sG41/4eMO2LRunw30dmhkO2+bni2FbpUwCtRo/AjauOx62vXJsZdhWrZLAmBqMx7Ju\nIW4j47Rc/EBl/Zg1o7O0/dTMirCtQp7g8uQJjvWzfHyMRg6xWs4W/Dmyv1U/WRa23f7p++KNEp/5\n+fsy9Zv80Oca3m5mL7a6jXZe9l8LYJ+7/8bdFwF8E8ANbWxPRHqoneDfAODlJb/vr98mIq8C7QR/\noxdP/+81k5ltM7NJM5uszMy1sTsR6aR2gn8/gE1Lft8I4OC5f+Tu2919wt0n8iNDbexORDqpneD/\nJYDLzOy1ZjYA4AMAHurMsESk2zK/2+/uZTO7FcD3UUv13ePuT7M+gwOl8F39fc+Nk1FmS4Xllscp\nOz8dH/qBZ9aFbdWV8TbZO9MF8u5z6Uw8ltHxk2HbsSMjcb+1M3G/A6vDtpk9Y2EbAJTH4uMHyTCU\nF1iqL2665vLfhm07n70k7lgm1zWSPmSZzOGpOA36uQduijsSvoanVruprTy/uz8M4OEOjUVEekif\n8BNJlIJfJFEKfpFEKfhFEqXgF0lUW+/2n69SJR8X6ZB0Xv4EGSYp0qiQwpf8bLbqtOJ0XEhUHo8L\ndFYciLc5MxAXIM2ejotJbC4+L7Mr4n75ufjYi6d4QVBlWTxWYxlZdpkhqb5leZJaJOk8doys4K+6\nIj6IXKnza1zQx2GX6covkigFv0iiFPwiiVLwiyRKwS+SKAW/SKJ6mupzt3jOvYwTLlqJzHE3R9JS\npMquMhKne6oknWVHB8K2wjxJO87HY1k8HacW8wvZ+hXYOWuSzWL7ZF1ppo/cv7sObgzbCqfi+xfs\nfmJz+J2Mt1mYPRP3W4zPN5NxntWO0JVfJFEKfpFEKfhFEqXgF0mUgl8kUQp+kUT1ONUXL6HFJttk\n1XlZ03nVQbLU1WA8qaINkBzSkTjVt7iSpBZZJRlZ5oqliVg/duyleGU0AEB5ObkvMlb1sX4LZDk2\nNmGqZ3xks1TnmfXxkmuLa7NNxGkkddptuvKLJErBL5IoBb9IohT8IolS8IskSsEvkqi2Un1m9gKA\nGQAVAGV3n2jSAx6s58bWzmOTHGZN5+XPsBRLtglDc2QsA6fiscyTsZTIJJ35Ckkfkn5Fsr/CHE89\n0RQaSdnxsj6ySXa+F0kjKzHMmF1b/sp82FY4tTzbRjs/J2jLOpHn/yN3P9KB7YhID+llv0ii2g1+\nB/ADM9tpZts6MSAR6Y12X/a/1d0PmtnFAHaY2a/d/bGlf1B/UtgGAPmxeF14Eemttq787n6w/v0w\ngAcBXNvgb7a7+4S7T+RXDrWzOxHpoMzBb2ZDZjZy9mcA1wPY06mBiUh3tfOyfx2AB602G2IBwH+4\n+yOsQ7FQxsZ1xxu2HXhmXdyRlK+xyTZZdR49dJJ+8WLcWB6Mx3LiDfHzLEtJLh+LJ42cL8WvpFi/\nxdm4dK9ClsYDgDI536xCzckxsklYX/+aQ2Hbs9Obw7bMqT7Sb3ZzvP5hVpVl/cv1ZQ5+d/8NgCs7\nOBYR6SGl+kQSpeAXSZSCXyRRCn6RRCn4RRLV0wk8y5U8Xjm2smFbdWWcYypOx5M40rXz2GSbrCCM\npPNAKulsZSnutoz0m4nvhnIpnqDUySSdtB879mYVbyx7yi4lpB/b5/H5uFrORxdJx3gyVTpL5+r4\nPhx+KT7fR67MVtXnZLnBbtOVXyRRCn6RRCn4RRKl4BdJlIJfJFEKfpFE9Xytvmq18fMNW3etPL4Q\nttlRktIha+exyTZZdR5L5+VfiddyW703bMLxN5K18+bjuyhH1iksDcT9CuTYm01umSMVeEbSoCyl\nxfodPto4NQzw882RgyTbLDyxO2wr3/KGTCMpTJPHb5fpyi+SKAW/SKIU/CKJUvCLJErBL5IoBb9I\nonqa6kMpB0w1TqWw9NOKA2Rtufk4Tba4MtvaeWyyTVadx9J5VZLRGd0Tb3N2Q5x6GjwRb3Nhddxv\nxSGyhiEplAMAz5G1EcmjiaZWybyYc6W4cc0zJEU6lG1BvuJcvE2//NKwbfz7ceUpmxR0YVUro+oO\nXflFEqXgF0mUgl8kUQp+kUQp+EUSpeAXSVTTVJ+Z3QPgPQAOu/ub6reNAvgWgEsAvADgT9298SJ8\nSxWr8HWNK/RKZ+KhzAzEJWH5ebKO34q4Om/+DElZsXXlyGSbrDqPpfNOvD5sQmksriJcXB2PpbIq\nnhC1PBL3K57kKbLSKraQIemYLfMG3xyvOXgsTybNzLoEnsUDHXkpPm9HriDVjuyU9fHy28quvw5g\n6zm33QbgUXe/DMCj9d9F5FWkafC7+2MAjp1z8w0A7q3/fC+A93Z4XCLSZVlfdKxz9ykAqH+/uHND\nEpFe6Pp/HGa2zcwmzWyycmqu27sTkRZlDf5DZjYOAPXvh6M/dPft7j7h7hP5lUMZdycinZY1+B8C\ncHP955sBfLczwxGRXmkl1fcNAO8AsNbM9gP4PIA7AdxvZrcAeAnA+1vamxu82jglMjp+Muw2ezqu\n7Fo8HVdT5chadqW5+NCXj8XpJbYGHptsk1XnsXQegvMFANWLSAnePBnn6njhvOpmXtZXOU0eMmVy\nLSmQdRPJBJ45dvxjcToTZHcUOYTScHzsW6+fjIdC8nnf23llS8PqhqbB7+4fDJre1eGxiEgP6RN+\nIolS8IskSsEvkigFv0iiFPwiiertBJ7msFzjEqdjR0bibiQtl18g68ORSrI8SS/Nl+IPIzlJH7K1\n89hkm6w6j6Xz8ofjWUErF8f9cmR9uIGXV4RtALA4SnJoJC2HHLnOkH4+FKfz8ifi80Yr6TJWGA4e\nnQ3bHtkxkWmbBbKGYbfpyi+SKAW/SKIU/CKJUvCLJErBL5IoBb9Ionqb6mNVfWtnwm6zKzpf1Vfp\nRlXfQLxNtnYem2yTVeexdJ6zfiNxVd/ihgusqo+NZXVvq/oWxuLH4dbrXn1VfbryiyRKwS+SKAW/\nSKIU/CKJUvCLJErBL5Ko3qb6iGMHVodt+bn4OapQyrbmXpGs1bc4Oxy2sYqwQjluXHEoHgtbO49N\ntsmq81g6r3gsTgMWX+RVfXStPpZey5HyNbbJ4TidVzxCHr6Z1+qLm4qz82HbIz/MVtVHstFdpyu/\nSKIU/CKJUvCLJErBL5IoBb9IohT8IolqZa2+ewC8B8Bhd39T/bY7AHwEwHT9z25394ebbStfqGDN\naONJEGf2jIX9iqfi/AubqLEUZ+xQmIu3WSHFYnTyRzZhKCmWK54k6Uqydh6bbJNV5zVL5zFsrOz4\nWeqNFL1h0/rpsO3lX2+Oh0LuQzZONpbcAntgkP0RxZmMs4l2QCtX/q8D2Nrg9q+4+1X1r6aBLyIX\nlqbB7+6PATjWg7GISA+18z//rWa228zuMbM1HRuRiPRE1uC/C8AWAFcBmALwpegPzWybmU2a2WT5\n5OmMuxORTssU/O5+yN0r7l4F8DUA15K/3e7uE+4+UViV/Y0mEemsTMFvZuNLfr0RwJ7ODEdEeqWV\nVN83ALwDwFoz2w/g8wDeYWZXoZbgeAHAR7s4RhHpgqbB7+4fbHDz3Vl2VqnkcGqm8Uv/8licQ60s\ni8tB2UKd5eVx8tXZJLQjbKbZuClHyos9F7exMlk2Wy5bNJP1Y/ujefwmfWmuO2M6+8Wj8XvJlaF4\nh1mz5+wQvNj5VTVLK7PWHrdPn/ATSZSCXyRRCn6RRCn4RRKl4BdJlIJfJFE9X6izUgqeb4IFPAHA\nSOaNZpfYzKikzUj6kD1dGllwssrONDsItvglOWe8H9lfsxwZzYU16ZtBaTE+cfkujMVouS9L5Wbc\nH0kdd5uu/CKJUvCLJErBL5IoBb9IohT8IolS8IskqrepPnPkg5UJywukAo/NqEp2R9MvpM3JAp+s\nqs9J0VeOLOJJ02sFkpfLkYOg/bItmgkge7lcxn7FAVLtyU5pxv3x1DGpIsy64Gb/Ju/VlV8kVQp+\nkUQp+EUSpeAXSZSCXyRRCn6RRF04VX0ZK7TYwpk0/UK2aWwiTrY/knsqLyNjYVg+i1X1sX4ZF83s\nGrbYKqvqy7hNhqUIK4PxHmnVJpFjC4p2ma78IolS8IskSsEvkigFv0iiFPwiiWoa/Ga2ycx+bGZ7\nzexpM/tE/fZRM9thZs/Vv8frKonIBaeVBEUZwKfcfZeZjQDYaWY7APw5gEfd/U4zuw3AbQA+S7dk\nDgtmXbzm8t+G3Zbl43zIroMbw7aF+WLYxrJkr3/NobDt+PzysO3w0ZVh21wpzvX55jNhW44M1Ifi\n88Ke1avDcb9N66dJT752HkvLseo81o+pbJzP1C+ro2+M7/t9N/1zpm1u+dGHsw6nbU2v/O4+5e67\n6j/PANgLYAOAGwDcW/+zewG8t1uDFJHOO6//+c3sEgBXA3gcwDp3nwJqTxAALu704ESke1oOfjMb\nBvAAgE+6+6nz6LfNzCbNbLIyM5dljCLSBS0Fv5kVUQv8+9z9O/WbD5nZeL19HMDhRn3dfbu7T7j7\nRH5kqBNjFpEOaOXdfgNwN4C97v7lJU0PAbi5/vPNAL7b+eGJSLe08jbrWwF8CMBTZvZk/bbbAdwJ\n4H4zuwXASwDe350hikg3NA1+d/8Z4mkG33W+O4yqpnY+e0nciaw7VzgVV1pFaUUAyC3GKbRnpzeH\nbT66GLblXxkM29Y8E4/lWD5OIVXH4hRZ/kR891VWx/2KR+J+L/86PnYAqAzFx8HWzmNFhqw6j6Xz\n8i9nLZXMZt0//nfYtuXSj2XaZn6+fzN46hN+IolS8IskSsEvkigFv0iiFPwiiVLwiySq5xN4VqM1\n60g6Lz9HnqPIJJ3Oji7jhKE4PkAaY6WhbBNqsuOzjP3o5KVNJpSkiSm2XdYx42SbFF10L+Mm/+DK\nsC1zyq4bx94iXflFEqXgF0mUgl8kUQp+kUQp+EUSpeAXSVRvU32GOM1C8lZ0fbysRVGsH23rQg4p\nI3ZeMutfkVlnZU0tsoxsPr5WOl04kOxOa/WJSK8p+EUSpeAXSZSCXyRRCn6RRCn4RRLV21QfHLlg\nlscKK9xbEZeo5U6SCTyzVkyxfqtLcRuZwLM4l7HkLevTM+vH0llN9kezZBkLFzOna7PKuL/cT58I\n26o3viXeHbvr2cymXaYrv0iiFPwiiVLwiyRKwS+SKAW/SKIU/CKJaprqM7NNAP4NwHrUpoXc7u5f\nNbM7AHwEwHT9T29394fZtvIzOaz6SeP11YanKmG/XCnOlRRmz4RtZ9bHqbflr8RrwM1ujteAG34p\nTjsWntgdtvnll4ZtIy/Fd0NpOG4bPDobti2MxcdQnI2PPbfAy8y8GKdWPRenrawa34eVwXibR98Y\nr2PI1s5jk22y6jyazvvDq8O21/3NU2EbU77mdZn64bPZui3VSp6/DOBT7r7LzEYA7DSzHfW2r7j7\nP7Q/DBHptVZW6Z0CMFX/ecbM9gLY0O2BiUh3ndf//GZ2CYCrATxev+lWM9ttZveY2ZqgzzYzmzSz\nyfL8XFuDFZHOaTn4zWwYwAMAPunupwDcBWALgKtQe2XwpUb93H27u0+4+0Rh2VAHhiwindBS8JtZ\nEbXAv8/dvwMA7n7I3SvuXgXwNQDXdm+YItJpTYPfzAzA3QD2uvuXl9w+vuTPbgSwp/PDE5FuMXde\n+mZmbwPwUwBP4XcrwN0O4IOoveR3AC8A+Gj9zcHQljcP+Z0PvqFh2+ceuOl8xv1/cotxemlxbZw+\nLJzK9hEHVvVWvngxbBv/fjFsO3JFfAxbr58M2x7ZMRH3u470+2Hcr52149i5MbJ2YJW87bzvprvC\nti3f+ljYxtbOY5NtVgvxCWDpvOf/+op4o12w77N/2fB2M9vp7uQO/p1W3u3/GRoXQdKcvohc2PQJ\nP5FEKfhFEqXgF0mUgl8kUQp+kUT1eALPhLFJHElbtdmMmh3ul7rMk76+CukRIpIoBb9IohT8IolS\n8IskSsEvkigFv0iieprq2z+7Bp/5+fsatvmauAIvPxs/RzlZ6swWsi0eV1kWN7KKsML0QNi2sIoM\nhTwFf29nPBFlgYyF9SuSCrviDF87rrQyPjcW34V0fbwcmTN0y48+HLaxyj2aWiX7Y2vnZZ5sk+lj\nalFXfpFEKfhFEqXgF0mUgl8kUQp+kUQp+EUSpeAXSZSCXyRRCn6RRCn4RRKl4BdJlIJfJFEKfpFE\ntbJW3zIAjwEYRK0K8Nvu/nkzey2AbwIYBbALwIfcPV6sDsDExIRPTsZryIlIe85nrb5WrvwLAN7p\n7leitjDnVjN7C4AvAviKu18G4DiAW7IOWER6r2nwe81s/ddi/csBvBPAt+u33wvgvV0ZoYh0RUv/\n85tZ3syeBHAYwA4AzwM44e5np0XYD2BD0HebmU2a2eT09HQnxiwiHdBS8Lt7xd2vArARwLUAfr/R\nnwV9t7v7hLtPXHTRRdlHKiIddV7v9rv7CQD/BeAtAFab2dlpwDYCONjZoYlINzUNfjO7yMxW139e\nDuCPAewF8GMAZyfkuxnAd7s1SBHpvFZSfVeg9oZeHrUni/vd/Qtmdil+l+p7AsCfuftCk21NA3ix\n/utaAEfaG35HXUjj0Vga01gaWzqW33P3lv6/bhr83WJmk63mI3vhQhqPxtKYxtJY1rHoE34iiVLw\niySqn8G/vY/7buRCGo/G0pjG0limsfTtf34R6S+97BdJVF+C38y2mtmzZrbPzG7rxxiWjOUFM3vK\nzJ40s56WHJrZPWZ22Mz2LLlt1Mx2mNlz9e9r+jiWO8zsQP3cPGlm7+7RWDaZ2Y/NbK+ZPW1mn6jf\n3vNzQ8bS83NjZsvM7Bdm9qv6WP6ufvtrzezx+nn5lpnFi0Yu5e49/ULt8wLPA7gUwACAXwG4vNfj\nWDKeFwCs7dO+3w7gGgB7ltz29wBuq/98G4Av9nEsdwD4dB/OyziAa+o/jwD4HwCX9+PckLH0/Nyg\nttzpcP3nIoDHUfu07f0APlC//V8A/EUr2+vHlf9aAPvc/Tdeq///JoAb+jCOvnP3xwAcO+fmG1D7\nUBXQw2rJYCx94e5T7r6r/vMMap8o3YA+nBsylp7zmo5V2PYj+DcAeHnJ72FFYI84gB+Y2U4z29bH\ncZy1zt2ngNoDD8DFfR7PrWa2u/5vQU/+BVnKzC4BcDVqV7m+nptzxgL04dy0U2F7rn4Ef6MF0PuZ\ncniru18D4E8AfNzM3t7HsVxo7gKwBbVJXKYAfKmXOzezYQAPAPiku5/q5b5bGEtfzo23UWF7rn4E\n/34Am5b83teKQHc/WP9+GMCDqJ3QfjpkZuMAUP9+uF8DcfdD9QdbFcDX0MNzY2ZF1ILtPnf/Tv3m\nvpybRmPp57mp77/tCtt+BP8vAVxWf4dyAMAHADzUh3HAzIbMbOTszwCuB7CH9+q6h1CrkgT6XC15\nNtDqbkSPzo2ZGYC7Aex19y8vaer5uYnG0o9z0/EK216+W7nkXct3o/au6fMA/qofY6iP41LUsg2/\nAvB0r8cC4BuovWQsofaK6BYAYwAeBfBc/ftoH8fy7wCeArAbtcAb79FY3obaS9fdAJ6sf727H+eG\njKXn5wbAFahV0O5G7cnmb5c8jn8BYB+A/wQw2Mr29Ak/kUTpE34iiVLwiyRKwS+SKAW/SKIU/CKJ\nUvCLJErBL5IoBb9Iov4XJRTxxwPp5rUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a13342940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.nanmax(Y_train2_mean1), np.nanmin(Y_train2_mean1))\n",
    "plt.imshow(Y_train2_mean1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37326633166 1.49595212187\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGaZJREFUeJzt3XuMXHd1B/Dvmeeu1961nbXXm10n\ncYid2knARksaFAiPlBAeUoJaCkFFqRRhhBogiCKiIJWUvwI0RFSqKE7zakWBFAhEbZpiQtwAigK2\n4yR2XGrHcZONN7t2dm3va96nf8xYrJx7zozvzsPR7/uRrF3f3/zu/c2dOXNn58z5/URVQUThSXR6\nAETUGQx+okAx+IkCxeAnChSDnyhQDH6iQDH4iQLF4CcKFIOfKFCpxXQWkWsBfBtAEsA/qeod3u3T\nmR7tWrIisi0xnTP7aXfWGYQ7QHufTjd/n05bxW7SpN2WKDv9nOMlivYBK2n7dV3K9r3XlHcH4d7H\nRNG+I5W0cwJinm9xxoKy3agp59w4TwxtwaWy1F3nfBtS89EDzc1NoViYbWinsYNfRJIA/gHA+wCM\nAvidiDysqs9bfbqWrMCWd34usq17h9kN5TdfZA8kYd/PSsZ+tNR5YXD7OU+ARMF+wpV67Cd/5mTJ\nbCs7Y+kamzHbcoNL7eOdKNj9+rvMNgBIzdkBnn3lhNmWH+oz28pZ76TGG0vqeN4ey8ASsy2Zs/fp\nPYZxvXZJOla/c/YVI7c//au/b3gfi3ktuxzAQVU9pKoFAD8AcN0i9kdEbbSY4B8C8PKC/4/WthHR\nG8Bigj/qffPr/hARka0islNEdhYLs4s4HBE102KCfxTA2gX/HwZw5PQbqeo2VR1R1ZF0pmcRhyOi\nZlpM8P8OwHoRWSciGQAfB/Bwc4ZFRK0W+9N+VS2JyM0A/gvVVN+9qrrP65OYzpmf6s9ftcns13XM\nTgMmpuxPvKVof4p+4NPDZtvwjuhPUgEgmbc/0fc+7S93258UJ5xPmDO7Dppt0tdrtnUfft2bsD8Y\nXmOPpS9j9wOQPuGkZNP208nrlz3wstmm6+yPkQr99qf2o++3swvnPTRhtuWH7X5emjOu3hedPK/H\nGssZzM2zqDy/qj4C4JHF7IOIOoPf8CMKFIOfKFAMfqJAMfiJAsXgJwrUoj7tP1PanTWLdLx03ovX\n2UUq6ZllZtt5391vtnW9Zhf2jF1hp7u8qq9St9PoVecV7C8/DXfZRU1earGSWW22vXSNXSUpdTJP\nUrELUQor7fFkJu3rzIV32WnQsXdFV4ECwPQ6+3gb7p8y2168wT43hT57n6nGiuXOyPUfejJWv5/+\nx9sjtxf3ND5GXvmJAsXgJwoUg58oUAx+okAx+IkCxeAnClRbU30QmHPuedV5XjqvtMROr02/Z4PZ\nlrSneEMx5rQD6Wk7zVJ20oCpeXufXsXf/Bp7vr3uV53qO29SzEWs2J6aiXctKWxeZ7Yl8/aAsk76\ncOoyuzovZT/VIGV7n5W0N7tnvDTgsbydxva4k5c2iFd+okAx+IkCxeAnChSDnyhQDH6iQDH4iQLV\n5lSfmEtheZNtetV5XjqvsNR+bRt40l5a6uR6O7WYLNjpntxy57XUyRJlp+10XvrwuNk2O3x+rH7n\nP3qu2TZ9nrMuIoBE0b4jXvrJW+YsPWnnOlPz9ni6jtpjmRm2D5iddNYqdNKA6Tm7DRovR7qj79JY\n/fqNeV29FPbpeOUnChSDnyhQDH6iQDH4iQLF4CcKFIOfKFCLSvWJyGEA0wDKAEqqOuLdXgGoRFc/\neWvneZNteqkNL5136E/tde68yrbUbLx0XqHPbkwWnLTUlL1WXb7P7pfbZPc79An7fKbtZewAAAln\nrPkBO12bHbefassO2xOmlp2lA+dX2/djybh9vk9ssNsyJ+z7N+NUkJ7JGnkLrdpwLFa/yblVkdvL\nv2h8H83I879HVePdAyLqGL7tJwrUYoNfAfxcRHaJyNZmDIiI2mOxb/uvVNUjIrIawHYR+R9VfWLh\nDWovClsBINu1fJGHI6JmWdSVX1WP1H5OAHgIwOURt9mmqiOqOpJOx5wfi4iaLnbwi0iPiCw79TuA\nawDsbdbAiKi1FvO2fwDAQ1JN3aUA/KuqPur2EJhVfcM7imY3b+08b7JNrzrPS+d5czGWnaI3t6LK\n2afaS9WhkrI7JuxT5vbLjtrns5L1c1aatNuXHrKfToVeu1+px+5X7rLvh3fe5gbtfj2jdlveXhqw\nJSYO9Mfq14w0Xex9qOohAG9pwhiIqAOY6iMKFIOfKFAMfqJAMfiJAsXgJwpU29fqsyZyTObt2R/j\nrh/nTbbpVed56TxvvTYv9eSRkp16knK8CTO9foviVS466TyvXyUd77Eo9dgn4Nxf2W1TF9tPe2+f\niWK89fg873v7M7H6Pf6LzYs+Nq/8RIFi8BMFisFPFCgGP1GgGPxEgWLwEwWqvam+CpAoRKdSrO0A\nUOq280TpaTv9EnftPK86z0vnSdmpwHPylWIv1YfMVM5sm9xo58FWPG/3q2Ttqj6x5+AEACSctGS5\ny5mkNGf3Ky2xH6eCPc+qm3o7usV+aiftpQHdfXpVlG4pqOMra7bH6vffxS3GOBrfB6/8RIFi8BMF\nisFPFCgGP1GgGPxEgWLwEwWqrak+TQKlnuhcWbnby6HZTWUnDRh37TzveB4vnVd2Jsb0skS5gSVm\nm1e16PXzxgJnbbzqMf1285jO45RwKhDTM/Y+c87cl4kWTKZacio64y7W9+jshlj9rOpDq2o2Cq/8\nRIFi8BMFisFPFCgGP1GgGPxEgWLwEwWqbqpPRO4F8GEAE6p6aW3bSgA/BHABgMMA/lxVp+rtK1EG\nMiejy8YSObu0LVGwF+RLORVa2Wl7n8mC/brnVu65k23a/dyiL6ctfdIuJVsyYd8Hr1/FzgIiPVXn\nKeFktNy1A9POMU/aJy41Z4/HO573vPDWd/T2qQkvR2g3eX76aryJOK3qwzOZ7LaRK//9AK49bdut\nAB5T1fUAHqv9n4jeQOoGv6o+AWDytM3XAXig9vsDAK5v8riIqMXi/s0/oKpjAFD7ubp5QyKidmj5\n13tFZCuArQCQzS5v9eGIqEFxr/zjIjIIALWfE9YNVXWbqo6o6kgm43zSQkRtFTf4HwZwY+33GwH8\nrDnDIaJ2aSTV930A7wbQLyKjAL4K4A4AD4rITQBeAvDRRg6mApQz0a83mV0HzX7DXReZbV6KMH14\n3GzLTg2ZbZVUvLXzvMk2vSo7Ly1X6rEfot5n7Ps3t94uebt425zZllvt5AEBZJyxpg4eMdtKF51r\ntslv9phty9MjZtuq3c5YJmft452YNtuK69bYbb1OvjKmk7vWxuo3NBN938enG8/11Q1+Vb3BaLq6\n4aMQ0VmH3/AjChSDnyhQDH6iQDH4iQLF4CcKVFsn8EwUK+gai56RUfrsRdm8dfzm13SZbbPD55tt\n+T77dc+r7BJ7KO7aed5km151npfOG3/voNk28Msxs+2VD9tpt9ScnypKrbRLHvOb7ZRs9rh94lbM\nbjLbptfa6bXcOc6ag2X7C2XdR1eZbXNr7DRv12S8yWI9k5fF67jyuejzUs40Pvssr/xEgWLwEwWK\nwU8UKAY/UaAY/ESBYvATBaqtqb5KOoHc4NLItu7DdkVYJWNPFNT9ql1J51X15TY1v6pvxfPNr+rz\nqvO8dJ7Xb81vTphtnajqq+x53mxbNmBX9a3c98av6uuacmaLdaRnoifC9VLKp+OVnyhQDH6iQDH4\niQLF4CcKFIOfKFAMfqJAtTXVJ2VF5kQhunHYTrG8dI1dLeetgXf+o3Z66dAn7I7ZUbtazFPJ2v3K\nWTsF462d50226VXneem8Fz5mV1Bq0k8VSdl+yiTn7aq+cre936Het5ltr11ip9dm19pjWfNkt9mW\nW25X9R3faI8zmW+8Yq5RA085ZaKO0aujz0thL6v6iKgOBj9RoBj8RIFi8BMFisFPFCgGP1GgGlmr\n714AHwYwoaqX1rbdDuBTAI7Wbnabqj5Sb1+aEuT6oyfcTPR5kzE643MyU9Pn2SnCtLmuMFBx0nIe\niS60qnKyh+kp+2Hwquy8yTa9fm46r1InVeR0LS2101ZScvYb8xKUnLf3ObXBbvMmaE3ahZnQFlwq\nR27bFavfv//STo82qpG7cz+AayO236Wqm2v/6gY+EZ1d6ga/qj4BYLINYyGiNlrMG5mbReRZEblX\nRFY0bURE1BZxg/87AN4EYDOAMQB3WjcUka0islNEdhYL9uwqRNResYJfVcdVtayqFQB3A7jcue02\nVR1R1ZF0xl5FhYjaK1bwi8jCdaI+AmBvc4ZDRO3SSKrv+wDeDaBfREYBfBXAu0VkM6qJn8MAPt3C\nMRJRC9QNflW9IWLzPbGOVgFSc9FJ+/QJO8EqlXizpiaKdlI6UbDf9Lh5cKcp4eSyk0Ylc719urPl\nOotmev28stx6C05636tIzNn3X52HUEpOGa1z3hLO9yq6J+x9Fnud7wA4+1Rvot2YC3VOF+2FZj0J\n47sv3uPzun3EOjIRveEx+IkCxeAnChSDnyhQDH6iQDH4iQLV1tl7E8Uysq9EzyqraXsohZV2qWhq\nxn79Emdi1PyAndNZesgZS6+dSyl3xcv3eCWm3uKX+c32bLleP2+WXa8sF6iTznOeTeLcR2/x07lB\nL13rlAk7Oa+uo04asMe5fwm7nzeLtOf337wkXkd7/dKG8cpPFCgGP1GgGPxEgWLwEwWKwU8UKAY/\nUaDamuqrpJPID/VFtnlVfZnJeK9R3myr2fF46TyveiuZs/M93kKVXtFi6SJ7Mc7scTst5/XzxuLO\nsos61XlOOs/rp0n7mEvG7Lb51fEep9wqL13pdIyXWXRd/KV9sfpN7Lgs3gEX4JWfKFAMfqJAMfiJ\nAsXgJwoUg58oUAx+okC1NdUHAcrZ6Neb7IGXzW4X3mXPnFjYvM5sS0/Om23LDtsrZ5Z67NNSSduv\nl6UldlvCqVxLn7RXIpXf7DHbVsxuMtsqe54324Z6nUUe61wOvMk2veo8L53ntS0/aOcPu4/Zz4ve\ng/YCMflz7EkzC332PtOzdmrVmyzWc9+X74vV76qvfSNy+/hM4+PglZ8oUAx+okAx+IkCxeAnChSD\nnyhQdYNfRNaKyOMisl9E9onI52vbV4rIdhE5UPu5ovXDJaJmaSTVVwLwRVXdLSLLAOwSke0A/hLA\nY6p6h4jcCuBWAF929yQwX2503ZDZbexd9utKMm+nNlLzWbOtbGf6UO5yqvPsXaLQa7elZ+y21Jz9\nMCxP2zM1Tq+1S+WWDdj9Xrsk3tqHgL92njfZpled56XzJjfaY83128ebPm+Z2eZO7LrCmfjzWPw1\nDi1fePpjsfpNbrwicntpZ+Mzida98qvqmKrurv0+DWA/gCEA1wF4oHazBwBc3/BRiajjzuhvfhG5\nAMAWAE8BGFDVMaD6AgFgdbMHR0St03Dwi8hSAD8GcIuqnjyDfltFZKeI7CwW7G9dEVF7NRT8IpJG\nNfC/p6o/qW0eF5HBWvsggImovqq6TVVHVHUknelpxpiJqAka+bRfANwDYL+qfmtB08MAbqz9fiOA\nnzV/eETUKo182n8lgE8CeE5ETlWZ3AbgDgAPishNAF4C8NHWDJGIWqFu8Kvqr2FPXXj1mRxMKkBq\nLrqCrdC/xOw3vc7OzWSdyT29NdnmV3tVZmYTSj1eZZe9z1y/vU9vrb5Vu+3G3Dl2vnLlPrvf7Fr7\nYU/O+6mihL3Eobt2njfZpled56XzPN4krOnpeAvr5VbFmzDUc+fg7lj9NvT/cfQwzqBOl9/wIwoU\ng58oUAx+okAx+IkCxeAnChSDnyhQ7Z3As1xB6ng+smn0/dFr+AHAhvunzLapy+x+M8P2a9uScTs3\nMzdop4LO/ZWd6ju6xT6diei7DQBI2fOMIjVpfyVayvY3Jr1+a57sNtumNvhpsO4Jby27eKkwb7JN\nrzrPS+dlTjqVmfb8ne44084+467V947PfjpWv8z66LGIPRfs6/DKTxQoBj9RoBj8RIFi8BMFisFP\nFCgGP1Gg2prq01QC+YHo6r3zHoqcCwQA8OIN9gxhKWdizOyknX85scFu6xm1UzpTF3sVcfZYzLpI\nAEVnjhM5MW22dR9dFatfbrndz6swBIBir31HvCrK3Cq7n7d2njfZpled56XzUnN2m5SddJ4zlrip\nvri8sTSKV36iQDH4iQLF4CcKFIOfKFAMfqJAMfiJAtXWVJ8okMxFlx3lh+3qvEKfndeQsv36pU4a\nMHPC7pd3lhyNO4GnNymol14rrltjts2tsY/n9Tu+0c5LJXP2WAB/As9ij3P/U/YxC332yfHWznN5\nxYdOOq+0xOnYgmUnfn3Ld2P12/DAZyK3e8+z0/HKTxQoBj9RoBj8RIFi8BMFisFPFCgGP1Gg6qb6\nRGQtgH8GsAZABcA2Vf22iNwO4FMAjtZuepuqPuLtSxNAqcfIRThVSqlZOzVTSTtrsjnVWzNeSsfh\npfO8lF3JGacmnJRdb9ps63KqFr1+ybyTkqtzOfBSSZrw8mt2U3rWfvC7jtlPUW/tPHeyTa8izknn\nabwl/lyfO/K2WP2sx+lMntWN5PlLAL6oqrtFZBmAXSKyvdZ2l6r+3Rkcj4jOEo2s0jsGYKz2+7SI\n7Acw1OqBEVFrndHf/CJyAYAtAJ6qbbpZRJ4VkXtFJPJ7cSKyVUR2isjOYsGen52I2qvh4BeRpQB+\nDOAWVT0J4DsA3gRgM6rvDO6M6qeq21R1RFVH0pkWfD+SiGJpKPhFJI1q4H9PVX8CAKo6rqplVa0A\nuBvA5a0bJhE1W93gFxEBcA+A/ar6rQXbBxfc7CMA9jZ/eETUKo182n8lgE8CeE5E9tS23QbgBhHZ\njGp24TCAeIuOLZaXf9F4a8fF5uaCWjCWNk8aWe+Y3t33JrhMFGOem5jLBrZ7ss2zVSOf9v8a0Vla\nN6dPRGc3fsOPKFAMfqJAMfiJAsXgJwoUg58oUAx+okAx+IkCxeAnChSDnyhQDH6iQDH4iQLF4CcK\nVFvX6it1C167JHpiyd4Xo9fwA4DrP/Sk2XYsv9Rs29F3qdm2asMxs23iQL/Z9r63P2O2fWXNdrPt\n0dkNZttPX91stp3ctdZsm7zMLk/rmrJn2hx4yp7BcuS2XWYbAEwXu8y233/zErPt4i/tM9vu+/J9\nZtsXnv6Y2Xbn4G6z7R2fjVdk6q2dF3eyzVb4wNU7I7c/eJ8za+1peOUnChSDnyhQDH6iQDH4iQLF\n4CcKFIOfKFAMfqJAMfiJAsXgJwoUg58oUAx+okAx+IkCxeAnCpSot54dABHpAvAEgCyqVYA/UtWv\nisg6AD8AsBLAbgCfVNWCt69ly4d1yzs/F91oF5ph7Eq7+FCcfn0H7bapP7Lb4koU7cXqSj32QL1+\nQzuKZtv02ugKSQBY9rLdb/Rqu5+73CCAhF186a+r5+z33CdKZtvkRnusuX77gJkT9gG950x+hb1P\nbcGl0qrOq+c/HxuJ3D767buQH325zqNY1cjdyQN4r6q+BcBmANeKyBUAvg7gLlVdD2AKwE0NjZqI\nzgp1g1+rZmr/Tdf+KYD3AvhRbfsDAK5vyQiJqCUaeiMjIsna8twTALYDeAHAcVU99X5tFMCQ0Xer\niOwUkZ3FwmwzxkxETdBQ8KtqWVU3AxgGcDmAjVE3M/puU9URVR1JZ3rij5SImuqMPsJQ1eMAdgC4\nAsByETn1SdwwgCPNHRoRtVLd4BeRVSKyvPZ7N4A/AbAfwOMA/qx2sxsB/KxVgySi5msk1fdmVD/Q\nS6L6YvGgqn5NRC7EH1J9TwP4C1XN19nXUQD/V/tvPwB7Fs32O5vGw7FE41iiLRzL+aq6qpFOdYO/\nVURkp6pGJys74GwaD8cSjWOJFncs/IYfUaAY/ESB6mTwb+vgsaOcTePhWKJxLNFijaVjf/MTUWfx\nbT9RoDoS/CJyrYj8XkQOisitnRjDgrEcFpHnRGSPiMQrsYp/7HtFZEJE9i7YtlJEtovIgdrPFR0c\ny+0i8krt3OwRkQ+2aSxrReRxEdkvIvtE5PO17W0/N85Y2n5uRKRLRH4rIs/UxvK3te3rROSp2nn5\noYhkGtqhqrb1H6rfF3gBwIUAMgCeAbCp3eNYMJ7DAPo7dOyrALwVwN4F274B4Nba77cC+HoHx3I7\ngL/uwHkZBPDW2u/LAPwvgE2dODfOWNp+blAtjF5a+z0N4ClUv237IICP17b/I4DPNLK/Tlz5Lwdw\nUFUPabX+/wcAruvAODpOVZ8AMHna5utQ/VIV0MZqSWMsHaGqY6q6u/b7NKrfKB1CB86NM5a206qm\nVdh2IviHALy84P9mRWCbKICfi8guEdnawXGcMqCqY0D1iQdgdYfHc7OIPFv7s6Atf4IsJCIXANiC\n6lWuo+fmtLEAHTg3i6mwPV0ngj9qlpFOphyuVNW3AvgAgL8Skas6OJazzXcAvAnVSVzGANzZzoOL\nyFIAPwZwi6qebOexGxhLR86NLqLC9nSdCP5RAGsX/L+jFYGqeqT2cwLAQ6ie0E4aF5FBAKj9nOjU\nQFR1vPZkqwC4G208NyKSRjXYvqeqP6lt7si5iRpLJ89N7fiLrrDtRPD/DsD62ieUGQAfB/BwB8YB\nEekRkWWnfgdwDYC9fq+WexjVKkmgw9WSpwKt5iNo07kREQFwD4D9qvqtBU1tPzfWWDpxbppeYdvO\nTysXfGr5QVQ/NX0BwFc6MYbaOC5ENdvwDIB97R4LgO+j+paxiOo7opsAnAPgMQAHaj9XdnAs/wLg\nOQDPohp4g20ayztQfev6LIA9tX8f7MS5ccbS9nMD4M2oVtA+i+qLzd8seB7/FsBBAP8GINvI/vgN\nP6JA8Rt+RIFi8BMFisFPFCgGP1GgGPxEgWLwEwWKwU8UKAY/UaD+Hz2VcuAfiJjyAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a150cec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.nanmax(Y_train2_mean0), np.nanmin(Y_train2_mean0))\n",
    "plt.imshow(Y_train2_mean0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717762314629 -0.585553192392\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106509be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.nanmax(Y_train2_mean1-Y_train2_mean0), np.nanmin(Y_train2_mean1-Y_train2_mean0))\n",
    "plt.imshow(Y_train2_mean1-Y_train2_mean0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try 3D plota\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7aAhTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a147f2550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.resize(np.array(range(1,32)).reshape(-1,1),(31,31))\n",
    "y = x.T\n",
    "z = Y_train2_mean1\n",
    "ax = plt.gca(projection='3d')\n",
    "ax.plot_surface(x,y,z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Learning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPLEMENT DEEP LEARNING\n",
    "# Import deep learning modules from keras library\n",
    "from keras.layers import Dense # For dense layers\n",
    "from keras.models import Sequential # For sequential layering\n",
    "from keras.callbacks import EarlyStopping # For stopping execution\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 1,024\n",
      "Trainable params: 1,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_shape = (X_train2.shape[1],) # Shape of input data\n",
    "model_DL = Sequential()\n",
    "# Start with 1 hidden layer with same nodes as input data\n",
    "model_DL.add(Dense(input_shape[0],activation='relu',input_shape=input_shape))\n",
    "# Output layer\n",
    "model_DL.add(Dense(1))\n",
    "# Compile model\n",
    "model_DL.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model_DL.summary()\n",
    "# Early stopping monitor w/ patience=3 (stop after 3 runs without improvements)\n",
    "early_stopping_monitor = EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0858 - val_loss: 0.0794\n",
      "Epoch 2/20\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0789 - val_loss: 0.0788\n",
      "Epoch 3/20\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0782 - val_loss: 0.0778\n",
      "Epoch 4/20\n",
      "800000/800000 [==============================] - 39s 48us/step - loss: 0.0777 - val_loss: 0.0774\n",
      "Epoch 5/20\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0773 - val_loss: 0.0771\n",
      "Epoch 6/20\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0771 - val_loss: 0.0773\n",
      "Epoch 7/20\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0769 - val_loss: 0.0770\n",
      "Epoch 8/20\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0767 - val_loss: 0.0788\n",
      "Epoch 9/20\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0767 - val_loss: 0.0766\n",
      "Epoch 10/20\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0765 - val_loss: 0.0768\n",
      "Epoch 11/20\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0764 - val_loss: 0.0768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x166c65390>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model using 20% of data for validation\n",
    "model_DL.fit(X_train2, Y_train, validation_split=0.2, epochs=20, callbacks=[early_stopping_monitor])\n",
    "# To save model: model_DL.save('file.h5')\n",
    "# To predict: model_DL.predict(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_DLpred = model_DL.predict(X_train2)\n",
    "Y_test_DLpred  = model_DL.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning MSE: 0.07626104374885893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_DL = mean_squared_error(Y_train, Y_train_DLpred)\n",
    "print(\"Deep Learning MSE: {}\".format(mse_DL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Sequential in module keras.models object:\n",
      "\n",
      "class Sequential(keras.engine.training.Model)\n",
      " |  Linear stack of layers.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      layers: list of layers to add to the model.\n",
      " |  \n",
      " |  # Note\n",
      " |      The first layer passed to a Sequential model\n",
      " |      should have a defined input shape. What that\n",
      " |      means is that it should have received an `input_shape`\n",
      " |      or `batch_input_shape` argument,\n",
      " |      or for some type of layers (recurrent, Dense...)\n",
      " |      an `input_dim` argument.\n",
      " |  \n",
      " |  # Example\n",
      " |  \n",
      " |      ```python\n",
      " |          model = Sequential()\n",
      " |          # first layer must have a defined input shape\n",
      " |          model.add(Dense(32, input_dim=500))\n",
      " |          # afterwards, Keras does automatic shape inference\n",
      " |          model.add(Dense(32))\n",
      " |  \n",
      " |          # also possible (equivalent to the above):\n",
      " |          model = Sequential()\n",
      " |          model.add(Dense(32, input_shape=(500,)))\n",
      " |          model.add(Dense(32))\n",
      " |  \n",
      " |          # also possible (equivalent to the above):\n",
      " |          model = Sequential()\n",
      " |          # here the batch dimension is None,\n",
      " |          # which means any batch size will be accepted by the model.\n",
      " |          model.add(Dense(32, batch_input_shape=(None, 500)))\n",
      " |          model.add(Dense(32))\n",
      " |      ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      keras.engine.training.Model\n",
      " |      keras.engine.topology.Container\n",
      " |      keras.engine.topology.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, name=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add(self, layer)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          layer: layer instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          TypeError: If `layer` is not a layer instance.\n",
      " |          ValueError: In case the `layer` argument does not\n",
      " |              know its input shape.\n",
      " |          ValueError: In case the `layer` argument has\n",
      " |              multiple output tensors, or is already connected\n",
      " |              somewhere else (forbidden in `Sequential` models).\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  call(self, inputs, mask=None)\n",
      " |      Call the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      A model is callable on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compile(self, optimizer, loss, metrics=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          optimizer: String (name of optimizer) or optimizer object.\n",
      " |              See [optimizers](/optimizers).\n",
      " |          loss: String (name of objective function) or objective function.\n",
      " |              See [losses](/losses).\n",
      " |              If the model has multiple outputs, you can use a different loss\n",
      " |              on each output by passing a dictionary or a list of losses.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the sum of all individual losses.\n",
      " |          metrics: List of metrics to be evaluated by the model\n",
      " |              during training and testing.\n",
      " |              Typically you will use `metrics=['accuracy']`.\n",
      " |              To specify different metrics for different outputs of a\n",
      " |              multi-output model, you could also pass a dictionary,\n",
      " |              such as `metrics={'output_a': 'accuracy'}`.\n",
      " |          sample_weight_mode: If you need to do timestep-wise\n",
      " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
      " |              `None` defaults to sample-wise weights (1D).\n",
      " |              If the model has multiple outputs, you can use a different\n",
      " |              `sample_weight_mode` on each output by passing a\n",
      " |              dictionary or a list of modes.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
      " |              by sample_weight or class_weight during training and testing.\n",
      " |          target_tensors: By default, Keras will create a placeholder for the\n",
      " |              model's target, which will be fed with the target data during\n",
      " |              training. If instead you would like to use your own\n",
      " |              target tensor (in turn, Keras will not expect external\n",
      " |              Numpy data for these targets at training time), you\n",
      " |              can specify them via the `target_tensors` argument.\n",
      " |              It should be a single tensor\n",
      " |              (for a single-output `Sequential` model).\n",
      " |          **kwargs: When using the Theano/CNTK backends, these arguments\n",
      " |              are passed into `K.function`.\n",
      " |              When using the TensorFlow backend,\n",
      " |              these arguments are passed into `tf.Session.run`.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      " |      \n",
      " |      # Example\n",
      " |          ```python\n",
      " |              model = Sequential()\n",
      " |              model.add(Dense(32, input_shape=(500,)))\n",
      " |              model.add(Dense(10, activation='softmax'))\n",
      " |              model.compile(optimizer='rmsprop',\n",
      " |                            loss='categorical_crossentropy',\n",
      " |                            metrics=['accuracy'])\n",
      " |          ```\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
      " |      Computes the loss on some input data, batch by batch.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: labels, as a Numpy array.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |          sample_weight: sample weights, as a Numpy array.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has no metrics)\n",
      " |          or list of scalars (if the model computes other metrics).\n",
      " |          The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the model was never compiled.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Evaluates the model on a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data\n",
      " |      as accepted by `test_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: Generator yielding tuples (inputs, targets)\n",
      " |              or (inputs, targets, sample_weights)\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: maximum size for the generator queue\n",
      " |          workers: maximum number of processes to spin up\n",
      " |          use_multiprocessing: if True, use process based threading.\n",
      " |              Note that because this implementation\n",
      " |              relies on multiprocessing, you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has no metrics)\n",
      " |          or list of scalars (if the model computes other metrics).\n",
      " |          The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the model was never compiled.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
      " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of training data.\n",
      " |              If the input layer in the model is named, you can also pass a\n",
      " |              dictionary mapping the input name to a Numpy array.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: Numpy array of target (label) data.\n",
      " |              If the output layer in the model is named, you can also pass a\n",
      " |              dictionary mapping the output name to a Numpy array.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, it will default to 32.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling.\n",
      " |          validation_data: tuple `(x_val, y_val)` or tuple\n",
      " |              `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |              This will override `validation_split`.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch').\n",
      " |              'batch' is a special option for dealing with the\n",
      " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          initial_epoch: Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined.\n",
      " |          validation_steps: Only relevant if `steps_per_epoch`\n",
      " |              is specified. Total number of steps (batches of samples)\n",
      " |              to validate before stopping.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: If the model was never compiled.\n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Fits the model on data generated batch-by-batch by a Python generator.\n",
      " |      \n",
      " |      The generator is run in parallel to the model, for efficiency.\n",
      " |      For instance, this allows you to do real-time data augmentation\n",
      " |      on images on CPU in parallel to training your model on GPU.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: A generator.\n",
      " |              The output of the generator must be either\n",
      " |              - a tuple (inputs, targets)\n",
      " |              - a tuple (inputs, targets, sample_weights).\n",
      " |              All arrays should contain the same number of samples.\n",
      " |              The generator is expected to loop over its data\n",
      " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
      " |              batches have been seen by the model.\n",
      " |          steps_per_epoch: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before declaring one epoch\n",
      " |              finished and starting the next epoch. It should typically\n",
      " |              be equal to the number of samples of your dataset\n",
      " |              divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          epochs: Integer, total number of iterations on the data.\n",
      " |              Note that in conjunction with initial_epoch, the parameter\n",
      " |              epochs is to be understood as \"final epoch\". The model is\n",
      " |              not trained for n steps given by epochs, but until the\n",
      " |              epoch epochs is reached.\n",
      " |          verbose: Verbosity mode, 0, 1, or 2.\n",
      " |          callbacks: List of callbacks to be called during training.\n",
      " |          validation_data: This can be either\n",
      " |              - A generator for the validation data\n",
      " |              - A tuple (inputs, targets)\n",
      " |              - A tuple (inputs, targets, sample_weights).\n",
      " |          validation_steps: Only relevant if `validation_data`\n",
      " |              is a generator.\n",
      " |              Number of steps to yield from validation generator\n",
      " |              at the end of every epoch. It should typically\n",
      " |              be equal to the number of samples of your\n",
      " |              validation dataset divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(validation_data)` as a number of steps.\n",
      " |          class_weight: Dictionary mapping class indices to a weight\n",
      " |              for the class.\n",
      " |          max_queue_size: Maximum size for the generator queue\n",
      " |          workers: Maximum number of processes to spin up\n",
      " |          use_multiprocessing: if True, use process based threading.\n",
      " |              Note that because\n",
      " |              this implementation relies on multiprocessing,\n",
      " |              you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed\n",
      " |              easily to children processes.\n",
      " |          shuffle: Whether to shuffle the order of the batches at\n",
      " |              the beginning of each epoch. Only used with instances\n",
      " |              of `Sequence` (keras.utils.Sequence).\n",
      " |          initial_epoch: Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the model was never compiled.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |          def generate_arrays_from_file(path):\n",
      " |              while 1:\n",
      " |                  f = open(path)\n",
      " |                  for line in f:\n",
      " |                      # create Numpy arrays of input data\n",
      " |                      # and labels, from each line in the file\n",
      " |                      x, y = process_line(line)\n",
      " |                      yield (x, y)\n",
      " |                  f.close()\n",
      " |      \n",
      " |          model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      " |                              steps_per_epoch=1000, epochs=10)\n",
      " |      ```\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Container` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieve a layer that is part of the model.\n",
      " |      \n",
      " |      Returns a layer based on either its name (unique)\n",
      " |      or its index in the graph. Indices are based on\n",
      " |      order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: string, name of layer.\n",
      " |          index: integer, index of layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A flat list of Numpy arrays\n",
      " |          (one array per model weight).\n",
      " |  \n",
      " |  legacy_get_config(self)\n",
      " |      Retrieves the model configuration as a Python list.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of dicts (each dict is a layer config).\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False, skip_mismatch=False)\n",
      " |      Loads all layer weights from a HDF5 save file.\n",
      " |      \n",
      " |      If `by_name` is False (default) weights are loaded\n",
      " |      based on the network's topology, meaning the architecture\n",
      " |      should be the same as when the weights were saved.\n",
      " |      Note that layers that don't have weights are not taken\n",
      " |      into account in the topological ordering, so adding or\n",
      " |      removing layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      If `by_name` is True, weights are loaded into layers\n",
      " |      only if they share the same name. This is useful\n",
      " |      for fine-tuning or transfer-learning models where\n",
      " |      some of the layers have changed.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the weights file to load.\n",
      " |          by_name: Boolean, whether to load weights by name\n",
      " |              or by topological order.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers\n",
      " |              where there is a mismatch in the number of weights,\n",
      " |              or a mismatch in the shape of the weight\n",
      " |              (only valid when `by_name`=True).\n",
      " |      \n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  pop(self)\n",
      " |      Removes the last layer in the model.\n",
      " |      \n",
      " |      # Raises\n",
      " |          TypeError: if there are no layers in the model.\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: the input data, as a Numpy array.\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A Numpy array of predictions.\n",
      " |  \n",
      " |  predict_classes(self, x, batch_size=None, verbose=0, steps=None)\n",
      " |      Generate class predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A numpy array of class predictions.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data as accepted by\n",
      " |      `predict_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: generator yielding batches of input samples.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: maximum size for the generator queue\n",
      " |          workers: maximum number of processes to spin up\n",
      " |          use_multiprocessing: if True, use process based threading.\n",
      " |              Note that because this implementation\n",
      " |              relies on multiprocessing, you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A Numpy array of predictions.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A Numpy array of predictions.\n",
      " |  \n",
      " |  predict_proba(self, x, batch_size=None, verbose=0, steps=None)\n",
      " |      Generates class probability predictions for the input samples.\n",
      " |      \n",
      " |      The input samples are processed batch by batch.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      \n",
      " |      # Returns\n",
      " |          A Numpy array of probability predictions.\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True)\n",
      " |      Dumps all layer weights to a HDF5 file.\n",
      " |      \n",
      " |      The weight file has:\n",
      " |          - `layer_names` (attribute), a list of strings\n",
      " |              (ordered names of model layers).\n",
      " |          - For every layer, a `group` named `layer.name`\n",
      " |              - For every such layer group, a group attribute `weight_names`,\n",
      " |                  a list of strings\n",
      " |                  (ordered names of weights tensor of the layer).\n",
      " |              - For every weight in the layer, a dataset\n",
      " |                  storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the model.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: Should be a list\n",
      " |              of Numpy arrays with shapes and types matching\n",
      " |              the output of `model.get_weights()`.\n",
      " |  \n",
      " |  test_on_batch(self, x, y, sample_weight=None)\n",
      " |      Evaluates the model over a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          y: labels, as a Numpy array.\n",
      " |          sample_weight: sample weights, as a Numpy array.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has no metrics)\n",
      " |          or list of scalars (if the model computes other metrics).\n",
      " |          The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the model was never compiled.\n",
      " |  \n",
      " |  train_on_batch(self, x, y, class_weight=None, sample_weight=None)\n",
      " |      Single gradient update over one batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: input data, as a Numpy array or list of Numpy arrays\n",
      " |              (if the model has multiple inputs).\n",
      " |          y: labels, as a Numpy array.\n",
      " |          class_weight: dictionary mapping classes to a weight value,\n",
      " |              used for scaling the loss function (during training only).\n",
      " |          sample_weight: sample weights, as a Numpy array.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar training loss (if the model has no metrics)\n",
      " |          or list of scalars (if the model computes other metrics).\n",
      " |          The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the model was never compiled.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A model instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  legacy_from_config(config, layer_cache=None) from builtins.type\n",
      " |      Load a model from a legacy configuration.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: dictionary with configuration.\n",
      " |          layer_cache: cache to draw pre-existing layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The loaded Model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  losses\n",
      " |      Retrieve the model's losses.\n",
      " |      \n",
      " |      Will only include losses that are either\n",
      " |      inconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include losses that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of loss tensors.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  regularizers\n",
      " |  \n",
      " |  state_updates\n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |      Retrieve the model's updates.\n",
      " |      \n",
      " |      Will only include updates that are either\n",
      " |      inconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include updates that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  uses_learning_phase\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.topology.Container:\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  run_internal_graph(self, inputs, masks=None)\n",
      " |      Computes output tensors for new inputs.\n",
      " |      \n",
      " |      # Note:\n",
      " |          - Expects `inputs` to be a list (potentially with 1 element).\n",
      " |          - Can be run on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: List of tensors\n",
      " |          masks: List of masks (tensors or None).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Three lists: output_tensors, output_masks, output_shapes\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True)\n",
      " |      Save the model to a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |          - The model architecture, allowing to re-instantiate the model.\n",
      " |          - The model weights.\n",
      " |          - The state of the optimizer, allowing to resume training\n",
      " |              exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model`\n",
      " |      is a compiled model ready to be used (unless the saved model\n",
      " |      was never compiled in the first place).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |              It defaults to `print` (prints to stdout).\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A YAML string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.topology.Container:\n",
      " |  \n",
      " |  input_spec\n",
      " |      Gets the model's input specs.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of `InputSpec` instances (one per input to the model)\n",
      " |              or a single instance if the model has only one input.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write results to file\n",
    "write_to_file(\"sample_Copete_DL_v1.csv\", Y_test_DLpred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to train 1-layer neural network of a given number of nodes\n",
    "def train_model_DL1(X_train,Y_train,n_nodes):\n",
    "    input_shape = (X_train.shape[1],) # Shape of input data\n",
    "    # Initialize model\n",
    "    model_DL = Sequential()\n",
    "    # First layer\n",
    "    model_DL.add(Dense(n_nodes,activation='relu',input_shape=input_shape))\n",
    "    # Output layer\n",
    "    model_DL.add(Dense(1))\n",
    "    # Compile model\n",
    "    model_DL.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    model_DL.summary()\n",
    "    # Early stopping monitor w/ patience=3 (stop after 3 runs without improvements)\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    # Fit model using 20% of data for validation\n",
    "    model_DL.fit(X_train, Y_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    Y_train_DLpred = model_DL.predict(X_train)\n",
    "    mse_DL = mean_squared_error(Y_train, Y_train_DLpred)\n",
    "    print('DONE')\n",
    "    return mse_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 20)                640       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 661\n",
      "Trainable params: 661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0871 - val_loss: 0.0795\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0795 - val_loss: 0.0789\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0788 - val_loss: 0.0783\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0784 - val_loss: 0.0783\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0781 - val_loss: 0.0786\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0779 - val_loss: 0.0790\n",
      "DONE\n",
      "Deep Learning MSE: 0.07853868946269231\n"
     ]
    }
   ],
   "source": [
    "### Optimize number of nodes for 1-layer deep learning model\n",
    "n_nodes = 20\n",
    "mse_DL1 = train_model_DL1(X_train2,Y_train,n_nodes)\n",
    "print(\"Deep Learning MSE ({} nodes): {}\".format(n_nodes,mse_DL1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                320       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0887 - val_loss: 0.0843\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0818 - val_loss: 0.0820\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0813 - val_loss: 0.0818\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0807 - val_loss: 0.0801\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0799 - val_loss: 0.0799\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0796 - val_loss: 0.0796\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0794 - val_loss: 0.0798\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0793 - val_loss: 0.0791\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0790 - val_loss: 0.0793\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0788 - val_loss: 0.0803\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0787 - val_loss: 0.0789\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0787 - val_loss: 0.0787\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0786 - val_loss: 0.0786\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0786 - val_loss: 0.0790\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 48s 59us/step - loss: 0.0786 - val_loss: 0.0790\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0785 - val_loss: 0.0792\n",
      "DONE\n",
      "Deep Learning MSE (10 nodes): 0.07884292022486443\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 10\n",
    "mse_DL1 = train_model_DL1(X_train2,Y_train,n_nodes)\n",
    "print(\"Deep Learning MSE ({} nodes): {}\".format(n_nodes,mse_DL1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 5)                 160       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 166\n",
      "Trainable params: 166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0976 - val_loss: 0.0850\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0836 - val_loss: 0.0830\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 45s 57us/step - loss: 0.0825 - val_loss: 0.0821\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0821 - val_loss: 0.0824\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0820 - val_loss: 0.0825\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0820 - val_loss: 0.0826\n",
      "DONE\n",
      "Deep Learning MSE (5 nodes): 0.082229926046723\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 5\n",
    "mse_DL1 = train_model_DL1(X_train2,Y_train,n_nodes)\n",
    "print(\"Deep Learning MSE ({} nodes): {}\".format(n_nodes,mse_DL1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 40)                1280      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 1,321\n",
      "Trainable params: 1,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0831 - val_loss: 0.0812\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0782 - val_loss: 0.0777\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0774 - val_loss: 0.0775\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 45s 57us/step - loss: 0.0769 - val_loss: 0.0779\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0767 - val_loss: 0.0767\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 47s 58us/step - loss: 0.0766 - val_loss: 0.0792\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0764 - val_loss: 0.0766\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0763 - val_loss: 0.0765\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0762 - val_loss: 0.0764\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0761 - val_loss: 0.0769\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0761 - val_loss: 0.0769\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0760 - val_loss: 0.0761\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0760 - val_loss: 0.0762\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0760 - val_loss: 0.0761\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0759 - val_loss: 0.0762\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 48s 61us/step - loss: 0.0759 - val_loss: 0.0761\n",
      "Epoch 17/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0759 - val_loss: 0.0781\n",
      "DONE\n",
      "Deep Learning MSE (40 nodes): 0.07767538151992882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 50)                1600      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,651\n",
      "Trainable params: 1,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0852 - val_loss: 0.0794\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0786 - val_loss: 0.0775\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0776 - val_loss: 0.0780\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0770 - val_loss: 0.0770\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0767 - val_loss: 0.0773\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0764 - val_loss: 0.0773\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0763 - val_loss: 0.0769\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0762 - val_loss: 0.0786\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0761 - val_loss: 0.0763\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0760 - val_loss: 0.0763\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0759 - val_loss: 0.0763\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0758 - val_loss: 0.0789\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0758 - val_loss: 0.0761\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 48s 61us/step - loss: 0.0758 - val_loss: 0.0776\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 49s 62us/step - loss: 0.0758 - val_loss: 0.0770\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 48s 59us/step - loss: 0.0757 - val_loss: 0.0771\n",
      "DONE\n",
      "Deep Learning MSE (50 nodes): 0.07661487229092204\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 60)                1920      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 1,981\n",
      "Trainable params: 1,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0839 - val_loss: 0.0781\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0776 - val_loss: 0.0795\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 48s 59us/step - loss: 0.0769 - val_loss: 0.0779\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0765 - val_loss: 0.0782\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0763 - val_loss: 0.0781\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0762 - val_loss: 0.0764\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 48s 59us/step - loss: 0.0761 - val_loss: 0.0761\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 53s 67us/step - loss: 0.0760 - val_loss: 0.0768\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 54s 67us/step - loss: 0.0759 - val_loss: 0.0787\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0758 - val_loss: 0.0760\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0757 - val_loss: 0.0781\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0757 - val_loss: 0.0762\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0757 - val_loss: 0.0758\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0756 - val_loss: 0.0763\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0756 - val_loss: 0.0759\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 47s 58us/step - loss: 0.0756 - val_loss: 0.0763\n",
      "DONE\n",
      "Deep Learning MSE (60 nodes): 0.07567453047443741\n"
     ]
    }
   ],
   "source": [
    "n_nodes = [40,50,60]\n",
    "for n in n_nodes:\n",
    "    mse_DL1 = train_model_DL1(X_train2,Y_train,n)\n",
    "    print(\"Deep Learning MSE ({} nodes): {}\".format(n,mse_DL1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 25)                800       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 826\n",
      "Trainable params: 826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0844 - val_loss: 0.0792\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0787 - val_loss: 0.0783\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0780 - val_loss: 0.0778\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0777 - val_loss: 0.0780\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0774 - val_loss: 0.0775\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 48s 59us/step - loss: 0.0773 - val_loss: 0.0775\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0772 - val_loss: 0.0779\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 48s 59us/step - loss: 0.0771 - val_loss: 0.0774\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 48s 61us/step - loss: 0.0770 - val_loss: 0.0777\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0769 - val_loss: 0.0772\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 48s 61us/step - loss: 0.0769 - val_loss: 0.0775\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0769 - val_loss: 0.0773\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 54s 67us/step - loss: 0.0768 - val_loss: 0.0769\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0768 - val_loss: 0.0771\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 49s 62us/step - loss: 0.0768 - val_loss: 0.0770\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0768 - val_loss: 0.0774\n",
      "DONE\n",
      "Deep Learning MSE (25 nodes): 0.07691540432198211\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 35)                1120      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 36        \n",
      "=================================================================\n",
      "Total params: 1,156\n",
      "Trainable params: 1,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0878 - val_loss: 0.0808\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0788 - val_loss: 0.0807\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0780 - val_loss: 0.0777\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0775 - val_loss: 0.0779\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0772 - val_loss: 0.0776\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0769 - val_loss: 0.0776\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0767 - val_loss: 0.0786\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0766 - val_loss: 0.0770\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 49s 62us/step - loss: 0.0765 - val_loss: 0.0779\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0764 - val_loss: 0.0778\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0764 - val_loss: 0.0775\n",
      "DONE\n",
      "Deep Learning MSE (35 nodes): 0.07700472074158336\n"
     ]
    }
   ],
   "source": [
    "n_nodes = [25,35]\n",
    "for n in n_nodes:\n",
    "    mse_DL1 = train_model_DL1(X_train2,Y_train,n)\n",
    "    print(\"Deep Learning MSE ({} nodes): {}\".format(n,mse_DL1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to train multi-layered neural network of a given number of nodes\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def train_model_DL(X_train,Y_train,n_nodes):\n",
    "    input_shape = (X_train.shape[1],) # Shape of input data\n",
    "    # Initialize model\n",
    "    model_DL = Sequential()\n",
    "    for i in range(len(n_nodes)):\n",
    "        if i == 0:\n",
    "            # First layer\n",
    "            model_DL.add(Dense(n_nodes[i],activation='relu',input_shape=input_shape))\n",
    "        else:\n",
    "            # Subsequent layers\n",
    "            model_DL.add(Dense(n_nodes[i],activation='relu'))    \n",
    "    # Output layer\n",
    "    model_DL.add(Dense(1))\n",
    "    # Compile model\n",
    "    model_DL.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    model_DL.summary()\n",
    "    # Early stopping monitor w/ patience=3 (stop after 3 runs without improvements)\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    # Fit model using 20% of data for validation\n",
    "    model_DL.fit(X_train, Y_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    Y_train_DLpred = model_DL.predict(X_train)\n",
    "    mse_DL = mean_squared_error(Y_train, Y_train_DLpred)\n",
    "    print('DONE')\n",
    "    return mse_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 160       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,158\n",
      "Trainable params: 1,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0846 - val_loss: 0.0789\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0784 - val_loss: 0.0777\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0776 - val_loss: 0.0773\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0772 - val_loss: 0.0770\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0769 - val_loss: 0.0769\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0767 - val_loss: 0.0792\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0765 - val_loss: 0.0767\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0763 - val_loss: 0.0765\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0763 - val_loss: 0.0765\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 53s 67us/step - loss: 0.0762 - val_loss: 0.0776\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0761 - val_loss: 0.0763\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 55s 69us/step - loss: 0.0760 - val_loss: 0.0762\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 55s 69us/step - loss: 0.0760 - val_loss: 0.0763\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 56s 70us/step - loss: 0.0760 - val_loss: 0.0761\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 55s 69us/step - loss: 0.0759 - val_loss: 0.0766\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0759 - val_loss: 0.0764\n",
      "Epoch 17/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0758 - val_loss: 0.0762\n",
      "DONE\n",
      "Deep Learning MSE ([31, 5] nodes): 0.07559711001798362\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                320       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,323\n",
      "Trainable params: 1,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 56s 70us/step - loss: 0.0852 - val_loss: 0.0787\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 55s 69us/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 55s 69us/step - loss: 0.0770 - val_loss: 0.0782\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 57s 71us/step - loss: 0.0766 - val_loss: 0.0767\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 59s 74us/step - loss: 0.0764 - val_loss: 0.0772\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 60s 75us/step - loss: 0.0762 - val_loss: 0.0770\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 59s 73us/step - loss: 0.0761 - val_loss: 0.0768\n",
      "DONE\n",
      "Deep Learning MSE ([31, 10] nodes): 0.07630886532455498\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 20)                640       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,653\n",
      "Trainable params: 1,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 55s 68us/step - loss: 0.0836 - val_loss: 0.0788\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 53s 67us/step - loss: 0.0779 - val_loss: 0.0772\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 54s 67us/step - loss: 0.0771 - val_loss: 0.0790\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 54s 68us/step - loss: 0.0767 - val_loss: 0.0767\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0764 - val_loss: 0.0767\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0762 - val_loss: 0.0772\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 58s 73us/step - loss: 0.0761 - val_loss: 0.0767\n",
      "DONE\n",
      "Deep Learning MSE ([31, 20] nodes): 0.0761112202831061\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 2,016\n",
      "Trainable params: 2,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 61s 76us/step - loss: 0.0876 - val_loss: 0.0790\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 59s 74us/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 58s 73us/step - loss: 0.0770 - val_loss: 0.0767\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 58s 73us/step - loss: 0.0766 - val_loss: 0.0773\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0764 - val_loss: 0.0785\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 39s 48us/step - loss: 0.0761 - val_loss: 0.0764\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 38s 47us/step - loss: 0.0760 - val_loss: 0.0766\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0758 - val_loss: 0.0763\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0757 - val_loss: 0.0768\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0756 - val_loss: 0.0761\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0756 - val_loss: 0.0759\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0755 - val_loss: 0.0758\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0755 - val_loss: 0.0763\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0755 - val_loss: 0.0761\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0754 - val_loss: 0.0758\n",
      "DONE\n",
      "Deep Learning MSE ([31, 31] nodes): 0.07522796848694081\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 40)                1280      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 2,313\n",
      "Trainable params: 2,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0830 - val_loss: 0.0803\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0777 - val_loss: 0.0772\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0770 - val_loss: 0.0768\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0766 - val_loss: 0.0772\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 36s 45us/step - loss: 0.0763 - val_loss: 0.0771\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 36s 46us/step - loss: 0.0761 - val_loss: 0.0768\n",
      "DONE\n",
      "Deep Learning MSE ([31, 40] nodes): 0.07634154077635309\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 50)                1600      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 2,643\n",
      "Trainable params: 2,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 37s 47us/step - loss: 0.0826 - val_loss: 0.0784\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 39s 48us/step - loss: 0.0779 - val_loss: 0.0777\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 0.0771 - val_loss: 0.0766\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0767 - val_loss: 0.0771\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0764 - val_loss: 0.0769\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0763 - val_loss: 0.0765\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0761 - val_loss: 0.0765\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0760 - val_loss: 0.0765\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0759 - val_loss: 0.0793\n",
      "DONE\n",
      "Deep Learning MSE ([31, 50] nodes): 0.07878534841673428\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 60)                1920      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 2,973\n",
      "Trainable params: 2,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 38s 47us/step - loss: 0.0822 - val_loss: 0.0784\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0777 - val_loss: 0.0786\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0770 - val_loss: 0.0767\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0765 - val_loss: 0.0789\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0763 - val_loss: 0.0775\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0761 - val_loss: 0.0764\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0759 - val_loss: 0.0765\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0758 - val_loss: 0.0759\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0757 - val_loss: 0.0763\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0756 - val_loss: 0.0759\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0755 - val_loss: 0.0762\n",
      "DONE\n",
      "Deep Learning MSE ([31, 60] nodes): 0.07551553507196271\n"
     ]
    }
   ],
   "source": [
    "# Loop over size of second layer\n",
    "n_nodes = [[31,5],[31,10],[31,20],[31,31],[31,40],[31,50],[31,60]]\n",
    "mse_DL = []\n",
    "for n in n_nodes:\n",
    "    mse_DL0 = train_model_DL(X_train2,Y_train,n)\n",
    "    print(\"Deep Learning MSE ({} nodes): {}\".format(n,mse_DL0))\n",
    "    mse_DL = np.append(mse_DL, mse_DL0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07559711, 0.07630887, 0.07611122, 0.07522797, 0.07634154,\n",
       "       0.07878535, 0.07551554])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1527a5a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.array(n_nodes)[:,1],mse_DL)\n",
    "#plt.title('MSE vs second layer nodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 5)                 160       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,150\n",
      "Trainable params: 2,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0854 - val_loss: 0.0785\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0776 - val_loss: 0.0772\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0769 - val_loss: 0.0775\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0765 - val_loss: 0.0770\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0762 - val_loss: 0.0766\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 48s 59us/step - loss: 0.0760 - val_loss: 0.0766\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0759 - val_loss: 0.0763\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0758 - val_loss: 0.0763\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0756 - val_loss: 0.0760\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0756 - val_loss: 0.0763\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0755 - val_loss: 0.0764\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0754 - val_loss: 0.0762\n",
      "DONE\n",
      "Deep Learning MSE ([31 31  5] nodes): 0.07554570820697558\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                320       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,315\n",
      "Trainable params: 2,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 47s 58us/step - loss: 0.0833 - val_loss: 0.0784\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0775 - val_loss: 0.0777\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0768 - val_loss: 0.0768\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0765 - val_loss: 0.0777\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0762 - val_loss: 0.0763\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0760 - val_loss: 0.0774\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0759 - val_loss: 0.0761\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0758 - val_loss: 0.0786\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0757 - val_loss: 0.0759\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0757 - val_loss: 0.0765\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0756 - val_loss: 0.0764\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 44s 54us/step - loss: 0.0755 - val_loss: 0.0768\n",
      "DONE\n",
      "Deep Learning MSE ([31 31 10] nodes): 0.0762425389100125\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 20)                640       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 2,645\n",
      "Trainable params: 2,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0864 - val_loss: 0.0777\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0776 - val_loss: 0.0769\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0769 - val_loss: 0.0805\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0765 - val_loss: 0.0776\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0762 - val_loss: 0.0770\n",
      "DONE\n",
      "Deep Learning MSE ([31 31 20] nodes): 0.07636834678439182\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 3,008\n",
      "Trainable params: 3,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0826 - val_loss: 0.0782\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0777 - val_loss: 0.0771\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0770 - val_loss: 0.0764\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0766 - val_loss: 0.0768\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0763 - val_loss: 0.0762\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0761 - val_loss: 0.0765\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0760 - val_loss: 0.0769\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0759 - val_loss: 0.0769\n",
      "DONE\n",
      "Deep Learning MSE ([31 31 31] nodes): 0.07642744363991266\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 40)                1280      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 3,305\n",
      "Trainable params: 3,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0854 - val_loss: 0.0777\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0776 - val_loss: 0.0771\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0769 - val_loss: 0.0765\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0765 - val_loss: 0.0770\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0763 - val_loss: 0.0761\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0761 - val_loss: 0.0764\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0759 - val_loss: 0.0762\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0758 - val_loss: 0.0763\n",
      "DONE\n",
      "Deep Learning MSE ([31 31 40] nodes): 0.07568436502283815\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 50)                1600      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 3,635\n",
      "Trainable params: 3,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 53s 67us/step - loss: 0.0828 - val_loss: 0.0776\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0774 - val_loss: 0.0766\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0768 - val_loss: 0.0765\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0764 - val_loss: 0.0771\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0762 - val_loss: 0.0766\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0760 - val_loss: 0.0766\n",
      "DONE\n",
      "Deep Learning MSE ([31 31 50] nodes): 0.07604948231929393\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 60)                1920      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 3,965\n",
      "Trainable params: 3,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 65s 81us/step - loss: 0.0825 - val_loss: 0.0782\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 66s 82us/step - loss: 0.0777 - val_loss: 0.0777\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 64s 80us/step - loss: 0.0770 - val_loss: 0.0767\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 58s 72us/step - loss: 0.0767 - val_loss: 0.0763\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 56s 70us/step - loss: 0.0764 - val_loss: 0.0764\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 61s 76us/step - loss: 0.0762 - val_loss: 0.0765\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 54s 67us/step - loss: 0.0760 - val_loss: 0.0763\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0759 - val_loss: 0.0759\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0758 - val_loss: 0.0763\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0756 - val_loss: 0.0760\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0755 - val_loss: 0.0763\n",
      "DONE\n",
      "Deep Learning MSE ([31 31 60] nodes): 0.0757100585598136\n"
     ]
    }
   ],
   "source": [
    "# Loop over size of third layer\n",
    "n_nodes = np.array([[31,31,5],[31,31,10],[31,31,20],[31,31,31],[31,31,40],[31,31,50],[31,31,60]])\n",
    "mse_DL = []\n",
    "for n in n_nodes:\n",
    "    mse_DL0 = train_model_DL(X_train2,Y_train,n)\n",
    "    print(\"Deep Learning MSE ({} nodes): {}\".format(n,mse_DL0))\n",
    "    mse_DL = np.append(mse_DL, mse_DL0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x152818dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.array(n_nodes)[:,2],mse_DL)\n",
    "#plt.title('MSE vs third layer nodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 5)                 160       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 166\n",
      "Trainable params: 166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 44s 54us/step - loss: 0.0967 - val_loss: 0.0907\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0866 - val_loss: 0.0854\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0849 - val_loss: 0.0850\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0847 - val_loss: 0.0841\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0830 - val_loss: 0.0833\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0827 - val_loss: 0.0829\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0826 - val_loss: 0.0829\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0826 - val_loss: 0.0829\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0825 - val_loss: 0.0828\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0825 - val_loss: 0.0827\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0825 - val_loss: 0.0833\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 37s 46us/step - loss: 0.0824 - val_loss: 0.0827\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 0.0824 - val_loss: 0.0827\n",
      "DONE\n",
      "Deep Learning MSE ([5] nodes): 0.08228675848817224\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_73 (Dense)             (None, 10)                320       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0949 - val_loss: 0.0863\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0849 - val_loss: 0.0849\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0839 - val_loss: 0.0834\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0836 - val_loss: 0.0850\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 45s 57us/step - loss: 0.0825 - val_loss: 0.0822\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0821 - val_loss: 0.0822\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0817 - val_loss: 0.0816\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0814 - val_loss: 0.0823\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0808 - val_loss: 0.0817\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0803 - val_loss: 0.0805\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0801 - val_loss: 0.0801\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0800 - val_loss: 0.0802\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0800 - val_loss: 0.0807\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0799 - val_loss: 0.0799\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0799 - val_loss: 0.0801\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0798 - val_loss: 0.0806\n",
      "Epoch 17/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0797 - val_loss: 0.0799\n",
      "Epoch 18/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0796 - val_loss: 0.0796\n",
      "Epoch 19/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0795 - val_loss: 0.0802\n",
      "Epoch 20/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0795 - val_loss: 0.0795\n",
      "Epoch 21/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0795 - val_loss: 0.0807\n",
      "Epoch 22/30\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0795 - val_loss: 0.0795\n",
      "Epoch 23/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0795 - val_loss: 0.0810\n",
      "Epoch 24/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0794 - val_loss: 0.0799\n",
      "Epoch 25/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0794 - val_loss: 0.0794\n",
      "Epoch 26/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0794 - val_loss: 0.0797\n",
      "Epoch 27/30\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0794 - val_loss: 0.0794\n",
      "Epoch 28/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0794 - val_loss: 0.0797\n",
      "DONE\n",
      "Deep Learning MSE ([10] nodes): 0.07926403571975078\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 20)                640       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 661\n",
      "Trainable params: 661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0843 - val_loss: 0.0799\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0791 - val_loss: 0.0799\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0785 - val_loss: 0.0786\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0782 - val_loss: 0.0794\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0780 - val_loss: 0.0779\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0778 - val_loss: 0.0779\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0777 - val_loss: 0.0787\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0776 - val_loss: 0.0777\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0774 - val_loss: 0.0780\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0773 - val_loss: 0.0774\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0772 - val_loss: 0.0774\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0772 - val_loss: 0.0772\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0771 - val_loss: 0.0785\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0771 - val_loss: 0.0789\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0771 - val_loss: 0.0794\n",
      "DONE\n",
      "Deep Learning MSE ([20] nodes): 0.07884339780647566\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_77 (Dense)             (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 1,024\n",
      "Trainable params: 1,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0897 - val_loss: 0.0877\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0790 - val_loss: 0.0792\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0785 - val_loss: 0.0786\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0781 - val_loss: 0.0782\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0778 - val_loss: 0.0776\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0776 - val_loss: 0.0776\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0774 - val_loss: 0.0777\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0772 - val_loss: 0.0775\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0771 - val_loss: 0.0775\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0770 - val_loss: 0.0775\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0770 - val_loss: 0.0778\n",
      "DONE\n",
      "Deep Learning MSE ([31] nodes): 0.07720627382607662\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_79 (Dense)             (None, 40)                1280      \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 1,321\n",
      "Trainable params: 1,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 44s 55us/step - loss: 0.0886 - val_loss: 0.0797\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0785 - val_loss: 0.0778\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 45s 57us/step - loss: 0.0777 - val_loss: 0.0789\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0773 - val_loss: 0.0772\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0771 - val_loss: 0.0780\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0770 - val_loss: 0.0774\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0768 - val_loss: 0.0768\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0766 - val_loss: 0.0769\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 41s 52us/step - loss: 0.0765 - val_loss: 0.0794\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0765 - val_loss: 0.0776\n",
      "DONE\n",
      "Deep Learning MSE ([40] nodes): 0.07708178615654072\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 50)                1600      \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,651\n",
      "Trainable params: 1,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0824 - val_loss: 0.0789\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 43s 53us/step - loss: 0.0780 - val_loss: 0.0792\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0773 - val_loss: 0.0772\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 0.0769 - val_loss: 0.0769\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 39s 49us/step - loss: 0.0766 - val_loss: 0.0777\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 0.0765 - val_loss: 0.0764\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 42s 53us/step - loss: 0.0763 - val_loss: 0.0765\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 0.0762 - val_loss: 0.0766\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 43s 54us/step - loss: 0.0761 - val_loss: 0.0763\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0760 - val_loss: 0.0770\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 0.0759 - val_loss: 0.0763\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0759 - val_loss: 0.0768\n",
      "DONE\n",
      "Deep Learning MSE ([50] nodes): 0.076218319701957\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_83 (Dense)             (None, 60)                1920      \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 1,981\n",
      "Trainable params: 1,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 42s 52us/step - loss: 0.0846 - val_loss: 0.0776\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0779 - val_loss: 0.0771\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 39s 49us/step - loss: 0.0772 - val_loss: 0.0771\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 39s 49us/step - loss: 0.0767 - val_loss: 0.0775\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 39s 49us/step - loss: 0.0765 - val_loss: 0.0765\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 0.0763 - val_loss: 0.0772\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 0.0762 - val_loss: 0.0768\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 0.0761 - val_loss: 0.0765\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 39s 49us/step - loss: 0.0760 - val_loss: 0.0762\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 39s 49us/step - loss: 0.0760 - val_loss: 0.0767\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 41s 51us/step - loss: 0.0759 - val_loss: 0.0775\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 0.0759 - val_loss: 0.0762\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 40s 49us/step - loss: 0.0758 - val_loss: 0.0763\n",
      "Epoch 14/30\n",
      "800000/800000 [==============================] - 40s 51us/step - loss: 0.0758 - val_loss: 0.0760\n",
      "Epoch 15/30\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 0.0758 - val_loss: 0.0763\n",
      "Epoch 16/30\n",
      "800000/800000 [==============================] - 38s 48us/step - loss: 0.0757 - val_loss: 0.0763\n",
      "Epoch 17/30\n",
      "800000/800000 [==============================] - 40s 50us/step - loss: 0.0757 - val_loss: 0.0763\n",
      "DONE\n",
      "Deep Learning MSE ([60] nodes): 0.07579136552503565\n"
     ]
    }
   ],
   "source": [
    "# Loop over size of first layer\n",
    "n_nodes = np.array([[5],[10],[20],[31],[40],[50],[60]])\n",
    "mse_DL1 = []\n",
    "for n in n_nodes:\n",
    "    mse_DL0 = train_model_DL(X_train2,Y_train,n)\n",
    "    print(\"Deep Learning MSE ({} nodes): {}\".format(n,mse_DL0))\n",
    "    mse_DL1 = np.append(mse_DL1, mse_DL0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBIne0X1HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158f48a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(n_nodes,mse_DL1)\n",
    "#plt.title('MSE vs third layer nodes')\n",
    "plt.ylim((0.075,0.084))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_95 (Dense)             (None, 70)                2240      \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 1)                 71        \n",
      "=================================================================\n",
      "Total params: 2,311\n",
      "Trainable params: 2,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0850 - val_loss: 0.0780\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 56s 69us/step - loss: 0.0775 - val_loss: 0.0774\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0769 - val_loss: 0.0775\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0766 - val_loss: 0.0765\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0764 - val_loss: 0.0765\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0762 - val_loss: 0.0769\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 48s 61us/step - loss: 0.0761 - val_loss: 0.0771\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0760 - val_loss: 0.0761\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0759 - val_loss: 0.0775\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 45s 57us/step - loss: 0.0759 - val_loss: 0.0763\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 45s 56us/step - loss: 0.0758 - val_loss: 0.0763\n",
      "DONE\n",
      "Deep Learning MSE ([70] nodes): 0.07574447538835406\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_97 (Dense)             (None, 80)                2560      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 2,641\n",
      "Trainable params: 2,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0821 - val_loss: 0.0789\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 46s 57us/step - loss: 0.0777 - val_loss: 0.0772\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 47s 58us/step - loss: 0.0770 - val_loss: 0.0772\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 47s 58us/step - loss: 0.0766 - val_loss: 0.0768\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 47s 58us/step - loss: 0.0763 - val_loss: 0.0762\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0761 - val_loss: 0.0771\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0760 - val_loss: 0.0764\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0759 - val_loss: 0.0764\n",
      "DONE\n",
      "Deep Learning MSE ([80] nodes): 0.0757371601437698\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 100)               3200      \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,301\n",
      "Trainable params: 3,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0809 - val_loss: 0.0793\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0775 - val_loss: 0.0769\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0768 - val_loss: 0.0773\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0765 - val_loss: 0.0767\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0763 - val_loss: 0.0782\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 46s 58us/step - loss: 0.0761 - val_loss: 0.0762\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 47s 59us/step - loss: 0.0760 - val_loss: 0.0763\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0759 - val_loss: 0.0760\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0758 - val_loss: 0.0758\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0757 - val_loss: 0.0762\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0757 - val_loss: 0.0764\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 54s 68us/step - loss: 0.0756 - val_loss: 0.0762\n",
      "DONE\n",
      "Deep Learning MSE ([100] nodes): 0.07563453557468831\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_101 (Dense)            (None, 150)               4800      \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 4,951\n",
      "Trainable params: 4,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0825 - val_loss: 0.0796\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0771 - val_loss: 0.0769\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0766 - val_loss: 0.0765\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 55s 69us/step - loss: 0.0763 - val_loss: 0.0768\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0761 - val_loss: 0.0764\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 54s 68us/step - loss: 0.0760 - val_loss: 0.0761\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 56s 70us/step - loss: 0.0759 - val_loss: 0.0770\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 55s 69us/step - loss: 0.0757 - val_loss: 0.0768\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 49s 62us/step - loss: 0.0757 - val_loss: 0.0761\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 0.0757 - val_loss: 0.0757\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0756 - val_loss: 0.0772\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 51s 64us/step - loss: 0.0755 - val_loss: 0.0766\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0755 - val_loss: 0.0775\n",
      "DONE\n",
      "Deep Learning MSE ([150] nodes): 0.07687493719710617\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_103 (Dense)            (None, 200)               6400      \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 6,601\n",
      "Trainable params: 6,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 52s 65us/step - loss: 0.0814 - val_loss: 0.0776\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0772 - val_loss: 0.0782\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0766 - val_loss: 0.0764\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0762 - val_loss: 0.0763\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0760 - val_loss: 0.0762\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0759 - val_loss: 0.0762\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 57s 71us/step - loss: 0.0757 - val_loss: 0.0759\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 50s 63us/step - loss: 0.0757 - val_loss: 0.0761\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 56s 70us/step - loss: 0.0756 - val_loss: 0.0760\n",
      "Epoch 10/30\n",
      "800000/800000 [==============================] - 54s 68us/step - loss: 0.0755 - val_loss: 0.0758\n",
      "Epoch 11/30\n",
      "800000/800000 [==============================] - 51s 63us/step - loss: 0.0754 - val_loss: 0.0759\n",
      "Epoch 12/30\n",
      "800000/800000 [==============================] - 53s 66us/step - loss: 0.0754 - val_loss: 0.0759\n",
      "Epoch 13/30\n",
      "800000/800000 [==============================] - 50s 62us/step - loss: 0.0754 - val_loss: 0.0761\n",
      "DONE\n",
      "Deep Learning MSE ([200] nodes): 0.07546136449064511\n"
     ]
    }
   ],
   "source": [
    "# Loop over size of first layer (pt 2)\n",
    "n_nodes_DL1_1 = np.array([[70],[80],[100],[150],[200]])\n",
    "mse_DL1_1 = []\n",
    "for n in n_nodes_DL1_1:\n",
    "    mse_DL0 = train_model_DL(X_train2,Y_train,n)\n",
    "    print(\"Deep Learning MSE ({} nodes): {}\".format(n,mse_DL0))\n",
    "    mse_DL1_1 = np.append(mse_DL1_1, mse_DL0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1594aad30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.append(n_nodes,n_nodes_DL1_1),np.append(mse_DL1,mse_DL1_1))\n",
    "#plt.title('MSE vs third layer nodes')\n",
    "plt.ylim((0.075,0.084))\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08228676, 0.07926404, 0.0788434 , 0.07720627, 0.07708179,\n",
       "       0.07621832, 0.07579137, 0.07547574])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_DL1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 150)               4800      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 4,951\n",
      "Trainable params: 4,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 67s 83us/step - loss: 0.0821 - val_loss: 0.0775\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 67s 84us/step - loss: 0.0773 - val_loss: 0.0775\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 65s 82us/step - loss: 0.0766 - val_loss: 0.0765\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 67s 84us/step - loss: 0.0763 - val_loss: 0.0760\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 77s 96us/step - loss: 0.0761 - val_loss: 0.0765\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 75s 94us/step - loss: 0.0759 - val_loss: 0.0762\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 69s 86us/step - loss: 0.0758 - val_loss: 0.0761\n",
      "DONE\n",
      "Deep Learning MSE ([150] nodes): 0.0754697843517679\n"
     ]
    }
   ],
   "source": [
    "# Redo run with 1 layer, 150 nodes (outlier)\n",
    "n_nodes_DL1_2 = np.array([[150]])\n",
    "mse_DL1_2 = []\n",
    "for n in n_nodes_DL1_2:\n",
    "    mse_DL0 = train_model_DL(X_train2,Y_train,n)\n",
    "    print(\"Deep Learning MSE ({} nodes): {}\".format(n,mse_DL0))\n",
    "    mse_DL1_2 = np.append(mse_DL1_2, mse_DL0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning on full set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to train multi-layered neural network of a given number of nodes\n",
    "def train_model_DL(X_train,Y_train,n_nodes):\n",
    "    \"\"\" n_nodes is 1-D numpy array with number of nodes on each layer\n",
    "        e.g. [10,20,30] is a model with 3 (hidden) layers,\n",
    "        with 10/20/30 nodes on the first/second/third layers\n",
    "        Returns trained DL model \"\"\"\n",
    "    input_shape = (X_train.shape[1],) # Shape of input data\n",
    "    # Initialize model\n",
    "    model_DL = Sequential()\n",
    "    for i in range(len(n_nodes)):\n",
    "        if i == 0:\n",
    "            # First layer\n",
    "            model_DL.add(Dense(n_nodes[i],activation='relu',input_shape=input_shape))\n",
    "        else:\n",
    "            # Subsequent layers\n",
    "            model_DL.add(Dense(n_nodes[i],activation='relu'))\n",
    "    # Output layer\n",
    "    model_DL.add(Dense(1))\n",
    "    # Compile model\n",
    "    model_DL.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    # Print model summary\n",
    "    model_DL.summary()\n",
    "    # Early stopping monitor w/ patience=3 (stop after 3 runs without improvements)\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    # Fit model using 20% of data for validation\n",
    "    model_DL.fit(X_train, Y_train, validation_split=0.2, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    Y_train_DLpred = model_DL.predict(X_train)\n",
    "    mse_DL = mean_squared_error(Y_train, Y_train_DLpred)\n",
    "    print('DONE. Mean Squared Error: ', mse_DL)\n",
    "    return model_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Fangli/XY_train_sub/X_fp_train_sub0.csv',\n",
       " '../Fangli/XY_train_sub/X_fp_train_sub1.csv',\n",
       " '../Fangli/XY_train_sub/X_fp_train_sub2.csv',\n",
       " '../Fangli/XY_train_sub/X_fp_train_sub3.csv',\n",
       " '../Fangli/XY_train_sub/X_fp_train_sub4.csv',\n",
       " '../Fangli/XY_train_sub/X_fp_train_sub5.csv',\n",
       " '../Fangli/XY_train_sub/X_fp_train_sub6.csv',\n",
       " '../Fangli/XY_train_sub/X_fp_train_sub7.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data files\n",
    "datafiles_X_train = ['../Fangli/XY_train_sub/X_fp_train_sub'+s+'.csv' for s in np.char.mod('%d', range(8))]\n",
    "datafiles_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub0.csv...\n",
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub1.csv...\n",
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub2.csv...\n",
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub3.csv...\n",
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub4.csv...\n",
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub5.csv...\n",
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub6.csv...\n",
      "Reading file ../Fangli/XY_train_sub/X_fp_train_sub7.csv...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 2048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "X_train_fp = np.array([])\n",
    "for file in datafiles_X_train:\n",
    "    print('Reading file '+file+'...')\n",
    "    while not Path(file).is_file(): time.sleep(10)\n",
    "    if X_train_fp.shape[0] == 0: \n",
    "        X_train_fp = pd.read_csv(file).values\n",
    "    else: X_train_fp = np.vstack((X_train_fp, pd.read_csv(file).values))\n",
    "X_train_fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save training data to compressed .npz file\n",
    "train_fp_file = 'XY_fp_train.npz'\n",
    "np.savez_compressed(train_fp_file,X_train_fp=X_train_fp,Y_train=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total expressed / unexpressed molecular features: 1751 / 297\n"
     ]
    }
   ],
   "source": [
    "# Features with all 0 values\n",
    "i0 = (np.sum(X_train_fp,axis=0) == 0)\n",
    "i1 = np.logical_not(i0)\n",
    "print('Total expressed / unexpressed molecular features: {} / {}'.format(np.sum(i1),np.sum(i0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unexpressed features\n",
    "X_train_fp1 = X_train_fp[:,i1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save/Load file with training data\n",
    "train_fp1_file = 'XY_fp1_train.npz'\n",
    "np.savez_compressed(train_fp1_file,X_train_fp1=X_train_fp1,Y_train=Y_train)\n",
    "#np.load(train_fp1_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1751)              3067752   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1752      \n",
      "=================================================================\n",
      "Total params: 3,069,504\n",
      "Trainable params: 3,069,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 2620s 3ms/step - loss: 0.0070 - val_loss: 0.0033\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 2152s 3ms/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 1706s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 1732s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 1652s 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 1718s 2ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 1655s 2ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 1695s 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 1661s 2ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "DONE. Mean Squared Error:  0.0016409728577717055\n"
     ]
    }
   ],
   "source": [
    "# Train with 1 layer, same nodes as input (1752)\n",
    "n_nodes_fp1_DL = np.array([[X_train_fp1.shape[1]]])\n",
    "model_fp1_DL = []\n",
    "for n in n_nodes_fp1_DL:\n",
    "    model_fp1_DL0 = train_model_DL(X_train_fp1,Y_train,n)\n",
    "    model_fp1_DL = [model_fp1_DL, model_fp1_DL0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_fp1_DL_1751 = model_fp1_DL0\n",
    "model_fp1_DL_1751.save('model_fp1_DL_1751.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Fangli/X_fp_test_sub/X_fp_test_sub0.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub1.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub2.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub3.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub4.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub5.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub6.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub7.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub8.csv',\n",
       " '../Fangli/X_fp_test_sub/X_fp_test_sub9.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data files\n",
    "datafiles_X_test = ['../Fangli/X_fp_test_sub/X_fp_test_sub'+s+'.csv' for s in np.char.mod('%d', range(10))]\n",
    "datafiles_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub0.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub1.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub2.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub3.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub4.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub5.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub6.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub7.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub8.csv...\n",
      "Reading file ../Fangli/X_fp_test_sub/X_fp_test_sub9.csv...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(824230, 2048)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fp = np.array([])\n",
    "for file in datafiles_X_test:\n",
    "    print('Reading file '+file+'...')\n",
    "    while not Path(file).is_file(): time.sleep(10)\n",
    "    if X_test_fp.shape[0] == 0: \n",
    "        X_test_fp = pd.read_csv(file).values\n",
    "    else: X_test_fp = np.vstack((X_test_fp, pd.read_csv(file).values))\n",
    "X_test_fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(824230, 1751)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unexpressed features\n",
    "X_test_fp1 = X_test_fp[:,i1]\n",
    "X_test_fp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save/Load file with test data\n",
    "test_fp1_file = 'X_fp1_test.npz'\n",
    "np.savez_compressed(test_fp1_file,X_test_fp1=X_test_fp1)\n",
    "#np.load(test_fp1_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on test data and save results to CSV file\n",
    "Y_test_fp1_pred  = model_fp1_DL_1751.predict(X_test_fp1)\n",
    "def write_to_file(filename, predictions):\n",
    "    # Function to write predictions to CSV file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(predictions):\n",
    "            f.write(str(i+1) + \",\" + str(p) + \"\\n\")\n",
    "write_to_file(\"P1_Copete_DL_v2.csv\", Y_test_fp1_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1000)              1752000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 1,753,001\n",
      "Trainable params: 1,753,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/30\n",
      "800000/800000 [==============================] - 1649s 2ms/step - loss: 0.0069 - val_loss: 0.0029\n",
      "Epoch 2/30\n",
      "800000/800000 [==============================] - 1186s 1ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 3/30\n",
      "800000/800000 [==============================] - 1105s 1ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 4/30\n",
      "800000/800000 [==============================] - 1454s 2ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 5/30\n",
      "800000/800000 [==============================] - 2048s 3ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 6/30\n",
      "800000/800000 [==============================] - 2124s 3ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 7/30\n",
      "800000/800000 [==============================] - 2063s 3ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 8/30\n",
      "800000/800000 [==============================] - 2242s 3ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 9/30\n",
      "800000/800000 [==============================] - 2213s 3ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 10/30\n",
      "661056/800000 [=======================>......] - ETA: 6:16 - loss: 0.0012"
     ]
    }
   ],
   "source": [
    "# Train with 1 layer, 1000 nodes\n",
    "n_nodes_fp1_DL = np.array([[1000]])\n",
    "#model_fp1_DL = []\n",
    "for n in n_nodes_fp1_DL:\n",
    "    model_fp1_DL_1000 = train_model_DL(X_train_fp1,Y_train,n)\n",
    "#    model_fp1_DL = [model_fp1_DL, model_fp1_DL0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (full training set): R^2 = 0.9953100029714237, MSE = 0.0007775660193620835\n"
     ]
    }
   ],
   "source": [
    "# Run RF regression on full training set\n",
    "RF_fp1 = RandomForestRegressor()\n",
    "RF_fp1.fit(X_train_fp1, Y_train)\n",
    "R2_RF_fp1 = RF_fp1.score(X_train_fp1, Y_train)\n",
    "Y_train_fp_RFpred = RF_fp1.predict(X_train_fp1)\n",
    "mse_RF_fp1 = mean_squared_error(Y_train, Y_train_fp_RFpred)\n",
    "print(\"Random Forest (full training set): R^2 = {}, MSE = {}\".format(R2_RF_fp1,mse_RF_fp1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2167933a747a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot RF feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mRF_fp1_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRF_fp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimp_fp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF_fp1_imp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF_fp1_imp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot RF feature importances\n",
    "plt.figure(figsize=(17,5))\n",
    "RF_fp1_imp = RF_fp1.feature_importances_\n",
    "imp_fp1 = np.flip(np.argsort(RF_fp1_imp),0)\n",
    "plt.hist(RF_fp1_imp,bins=np.logspace(-5,0,200))\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0100452046409156 0.4126032565282579\n",
      "1.6859270726661675 0.2836737332765812\n",
      "-0.32411813197474815\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+AAAAEyCAYAAAB6TrpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAD34/5mJC3s704vXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130704be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of most important feature\n",
    "h0 = np.logical_not(X_train_fp1[:,imp_fp1[0]])\n",
    "h1 = np.logical_and(X_train_fp1[:,imp_fp1[0]],1)\n",
    "print(Y_train[h0].mean(), Y_train[h0].std())\n",
    "print(Y_train[h1].mean(), Y_train[h1].std())\n",
    "print(Y_train[h1].mean()-Y_train[h0].mean())\n",
    "plt.figure(figsize=(17,5))\n",
    "plt.hist(Y_train[h0],1000)\n",
    "plt.hist(Y_train[h1],1000)\n",
    "plt.yscale('symlog')\n",
    "plt.legend(['0','1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Simple Linear Regression with 5-fold Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "LR_cv5_fp1 = LinearRegression()\n",
    "LR_cv5_fp1_results = cross_val_score(LR_cv5_fp1, X_train_fp1, Y_train, cv=5)\n",
    "print(LR_cv5_fp1_results)\n",
    "print(np.mean(LR_cv5_fp1_results), np.std(LR_cv5_fp1_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create hold-out set (i.e. a test set from given data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_fp2, X_holdout_fp2, Y_train_fp2, Y_holdout_fp2 = train_test_split(X_train_fp1, Y_train, test_size=0.2, random_state=42) #How to set random_state?\n",
    "print(X_train_fp2.shape, X_holdout_fp2.shape, Y_train_fp2.shape, Y_holdout_fp2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try ElasticNet regression (combination of Ridge and Lasso)\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Create the hyperparameter grid (L1=1 for Lasso, <1 for Lasso/Ridge combination)\n",
    "l1_space = np.linspace(0, 1, 5)\n",
    "l1_grid = {'l1_ratio': l1_space}\n",
    "# Instantiate the ElasticNet regressor: EN\n",
    "ENhol = ElasticNet()\n",
    "# Setup the GridSearchCV object: EN_cv\n",
    "EN_cv = GridSearchCV(EN, l1_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit it to the new training data\n",
    "EN_cv.fit(X_train3, Y_train3)\n",
    "# Predict on the test set and compute metrics\n",
    "Y_pred3 = EN_cv.predict(X_holdout3)\n",
    "r2_EN_cv = EN_cv.score(X_holdout3, Y_holdout3)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_EN_cv = mean_squared_error(Y_holdout3, Y_pred3)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(EN_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2_EN_cv))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse_EN_cv))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
